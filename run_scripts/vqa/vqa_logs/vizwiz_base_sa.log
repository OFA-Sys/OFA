2022-07-12 12:56:52 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2022-07-12 12:56:52 - utils.py[line:261] - INFO: Start init
2022-07-12 12:56:52 - distributed_c10d.py[line:217] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-12 12:56:52 - distributed_c10d.py[line:252] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
2022-07-12 12:56:52 - utils.py[line:274] - INFO: initialized host cvpc18 as rank 0
single-machine distributed training is initialized.
2022-07-12 12:56:54 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 4, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 4, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 12, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/12_0.04_5e-5_480', 'restore_file': '../../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 15, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'vqa_score', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='../../dataset/vizwiz_data/trainval_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=4, batch_size_valid=4, best_checkpoint_metric='vqa_score', bf16=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../../dataset/vizwiz_data/vizwiz_train.tsv,../../dataset/vizwiz_data/vizwiz_val.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=15, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=12, max_object_length=30, max_source_positions=1024, max_src_length=80, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_bins=1000, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='../../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/12_0.04_5e-5_480', save_interval=1, save_interval_updates=0, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir=None, threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[4], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='beamsearch', valid_batch_size=20, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '../../dataset/vizwiz_data/vizwiz_train.tsv,../../dataset/vizwiz_data/vizwiz_val.tsv', 'selected_cols': '0,5,2,3,4', 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 80, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '../../dataset/vizwiz_data/trainval_ans2label.pkl', 'add_object': True, 'valid_batch_size': 20, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'beamsearch', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}, 'simul_type': None}
2022-07-12 12:56:54 - ofa_task.py[line:102] - INFO: source dictionary: 59457 types
2022-07-12 12:56:54 - ofa_task.py[line:103] - INFO: target dictionary: 59457 types
/data1/6willrut/OFA/venv/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2022-07-12 12:56:59 - train.py[line:101] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-07-12 12:56:59 - train.py[line:102] - INFO: task: VqaGenTask
2022-07-12 12:56:59 - train.py[line:103] - INFO: model: OFAModel
2022-07-12 12:56:59 - train.py[line:104] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2022-07-12 12:56:59 - train.py[line:108] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2022-07-12 12:56:59 - train.py[line:115] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../dataset/vizwiz_data/vizwiz_val.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_val.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_val.tsv slice_id 0 row count 4319 total row count 4319
/data1/6willrut/OFA/venv/lib/python3.6/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-07-12 12:57:00 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-07-12 12:57:00 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2022-07-12 12:57:00 - utils.py[line:765] - INFO: rank   0: capabilities =  7.5  ; total memory = 10.728 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-07-12 12:57:00 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2022-07-12 12:57:00 - train.py[line:145] - INFO: training on 1 devices (GPUs/TPUs)
2022-07-12 12:57:00 - train.py[line:151] - INFO: max tokens per device = None and max sentences per device = 4
2022-07-12 12:57:00 - trainer.py[line:458] - INFO: Preparing to load checkpoints ../../checkpoints/ofa_base.pt
2022-07-12 12:57:01 - trainer.py[line:590] - WARNING: EMA not found in checkpoints. But store_ema is True. EMA is re-initialized from checkpoints.
2022-07-12 12:57:01 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2022-07-12 12:57:01 - trainer.py[line:273] - INFO: Exponential Moving Average Shadow Model is initialized.
2022-07-12 12:57:01 - trainer.py[line:619] - INFO: Loaded checkpoints ../../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2022-07-12 12:57:02 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
/data1/6willrut/OFA/data/mm_data/vqa_gen_dataset.py:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  decoder_prompts = np.array([s['decoder_prompt'].tolist() for s in samples])
slice_id 0 seek offset 0
Total steps 15396, warmup steps 615, warmup_factor 0.0016260162601626016
2022-07-12 12:57:04 - trainer.py[line:703] - INFO: begin training epoch 1
2022-07-12 12:57:04 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 12:57:17 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 1283 loss=3.318, loss_v1=0, loss_v2=0, nll_loss=3.028, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=8.16, wps=41.6, ups=0.81, wpb=50.9, bsz=16, num_updates=10, lr=8.13008e-07, gnorm=33.167, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=17
2022-07-12 12:57:29 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 12:57:30 - progress_bar.py[line:274] - INFO: epoch 001:     21 / 1283 loss=3.33, loss_v1=0, loss_v2=0, nll_loss=3.083, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=8.48, wps=38.2, ups=0.74, wpb=51.5, bsz=16, num_updates=20, lr=1.62602e-06, gnorm=32.519, clip=100, loss_scale=64, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=30
2022-07-12 12:57:42 - progress_bar.py[line:274] - INFO: epoch 001:     31 / 1283 loss=3.134, loss_v1=0, loss_v2=0, nll_loss=2.88, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=7.36, wps=42.3, ups=0.82, wpb=51.6, bsz=16, num_updates=30, lr=2.43902e-06, gnorm=29.658, clip=100, loss_scale=64, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=43
2022-07-12 12:57:55 - progress_bar.py[line:274] - INFO: epoch 001:     41 / 1283 loss=2.79, loss_v1=0, loss_v2=0, nll_loss=2.545, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=5.84, wps=44.8, ups=0.82, wpb=54.6, bsz=16, num_updates=40, lr=3.25203e-06, gnorm=23.615, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=55
2022-07-12 12:58:07 - progress_bar.py[line:274] - INFO: epoch 001:     51 / 1283 loss=2.518, loss_v1=0, loss_v2=0, nll_loss=2.235, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=4.71, wps=43.1, ups=0.82, wpb=52.6, bsz=16, num_updates=50, lr=4.06504e-06, gnorm=23.073, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=67
2022-07-12 12:58:19 - progress_bar.py[line:274] - INFO: epoch 001:     61 / 1283 loss=2.306, loss_v1=0, loss_v2=0, nll_loss=1.994, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=3.98, wps=40.1, ups=0.8, wpb=49.8, bsz=16, num_updates=60, lr=4.87805e-06, gnorm=22.062, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=80
2022-07-12 12:58:32 - progress_bar.py[line:274] - INFO: epoch 001:     71 / 1283 loss=2.213, loss_v1=0, loss_v2=0, nll_loss=1.925, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=3.8, wps=42.9, ups=0.81, wpb=52.7, bsz=16, num_updates=70, lr=5.69106e-06, gnorm=21.274, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=92
2022-07-12 12:58:44 - progress_bar.py[line:274] - INFO: epoch 001:     81 / 1283 loss=1.925, loss_v1=0, loss_v2=0, nll_loss=1.611, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=3.05, wps=42.3, ups=0.81, wpb=52.4, bsz=16, num_updates=80, lr=6.50407e-06, gnorm=23.071, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=104
2022-07-12 12:58:56 - progress_bar.py[line:274] - INFO: epoch 001:     91 / 1283 loss=1.882, loss_v1=0, loss_v2=0, nll_loss=1.574, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=2.98, wps=42.8, ups=0.81, wpb=52.8, bsz=16, num_updates=90, lr=7.31707e-06, gnorm=20.84, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=117
2022-07-12 12:59:09 - progress_bar.py[line:274] - INFO: epoch 001:    101 / 1283 loss=1.804, loss_v1=0, loss_v2=0, nll_loss=1.494, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=2.82, wps=41.6, ups=0.81, wpb=51.4, bsz=16, num_updates=100, lr=8.13008e-06, gnorm=21.689, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=129
2022-07-12 12:59:21 - progress_bar.py[line:274] - INFO: epoch 001:    111 / 1283 loss=1.824, loss_v1=0, loss_v2=0, nll_loss=1.482, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=2.79, wps=39.7, ups=0.8, wpb=49.5, bsz=16, num_updates=110, lr=8.94309e-06, gnorm=20.367, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=141
2022-07-12 12:59:34 - progress_bar.py[line:274] - INFO: epoch 001:    121 / 1283 loss=1.788, loss_v1=0, loss_v2=0, nll_loss=1.498, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=2.82, wps=42.8, ups=0.81, wpb=53, bsz=16, num_updates=120, lr=9.7561e-06, gnorm=21.062, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=154
2022-07-12 12:59:46 - progress_bar.py[line:274] - INFO: epoch 001:    131 / 1283 loss=1.77, loss_v1=0, loss_v2=0, nll_loss=1.421, ntokens=49.3, nsentences=16, sample_size=49.3, sample_size_v1=0, sample_size_v2=0, ppl=2.68, wps=40, ups=0.81, wpb=49.3, bsz=16, num_updates=130, lr=1.05691e-05, gnorm=20.234, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=166
2022-07-12 12:59:58 - progress_bar.py[line:274] - INFO: epoch 001:    141 / 1283 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=1.411, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=2.66, wps=44.9, ups=0.81, wpb=55.1, bsz=16, num_updates=140, lr=1.13821e-05, gnorm=17.918, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=178
2022-07-12 13:00:10 - progress_bar.py[line:274] - INFO: epoch 001:    151 / 1283 loss=1.834, loss_v1=0, loss_v2=0, nll_loss=1.56, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=2.95, wps=43.7, ups=0.82, wpb=53.5, bsz=16, num_updates=150, lr=1.21951e-05, gnorm=17.658, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=191
2022-07-12 13:00:23 - progress_bar.py[line:274] - INFO: epoch 001:    161 / 1283 loss=1.595, loss_v1=0, loss_v2=0, nll_loss=1.308, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=2.48, wps=42.9, ups=0.8, wpb=53.5, bsz=16, num_updates=160, lr=1.30081e-05, gnorm=19.346, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=203
2022-07-12 13:00:35 - progress_bar.py[line:274] - INFO: epoch 001:    171 / 1283 loss=1.59, loss_v1=0, loss_v2=0, nll_loss=1.28, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=2.43, wps=42, ups=0.81, wpb=51.6, bsz=16, num_updates=170, lr=1.38211e-05, gnorm=17.64, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=215
2022-07-12 13:00:48 - progress_bar.py[line:274] - INFO: epoch 001:    181 / 1283 loss=1.629, loss_v1=0, loss_v2=0, nll_loss=1.302, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=2.47, wps=40.7, ups=0.81, wpb=50.3, bsz=16, num_updates=180, lr=1.46341e-05, gnorm=18.153, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=228
2022-07-12 13:01:00 - progress_bar.py[line:274] - INFO: epoch 001:    191 / 1283 loss=1.552, loss_v1=0, loss_v2=0, nll_loss=1.22, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=2.33, wps=41.9, ups=0.81, wpb=51.8, bsz=16, num_updates=190, lr=1.54472e-05, gnorm=16.393, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=240
2022-07-12 13:01:12 - progress_bar.py[line:274] - INFO: epoch 001:    201 / 1283 loss=1.571, loss_v1=0, loss_v2=0, nll_loss=1.284, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=2.44, wps=41.9, ups=0.81, wpb=51.9, bsz=16, num_updates=200, lr=1.62602e-05, gnorm=17.773, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=252
2022-07-12 13:01:24 - progress_bar.py[line:274] - INFO: epoch 001:    211 / 1283 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=1.366, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=2.58, wps=41.9, ups=0.82, wpb=51.1, bsz=16, num_updates=210, lr=1.70732e-05, gnorm=16.695, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=265
2022-07-12 13:01:37 - progress_bar.py[line:274] - INFO: epoch 001:    221 / 1283 loss=1.58, loss_v1=0, loss_v2=0, nll_loss=1.297, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=2.46, wps=41.8, ups=0.81, wpb=51.7, bsz=16, num_updates=220, lr=1.78862e-05, gnorm=18.053, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=277
2022-07-12 13:01:49 - progress_bar.py[line:274] - INFO: epoch 001:    231 / 1283 loss=1.524, loss_v1=0, loss_v2=0, nll_loss=1.223, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=2.33, wps=43, ups=0.81, wpb=53.3, bsz=16, num_updates=230, lr=1.86992e-05, gnorm=13.688, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=289
2022-07-12 13:02:02 - progress_bar.py[line:274] - INFO: epoch 001:    241 / 1283 loss=1.601, loss_v1=0, loss_v2=0, nll_loss=1.296, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=2.46, wps=41, ups=0.8, wpb=51.2, bsz=16, num_updates=240, lr=1.95122e-05, gnorm=16.057, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=302
2022-07-12 13:02:14 - progress_bar.py[line:274] - INFO: epoch 001:    251 / 1283 loss=1.421, loss_v1=0, loss_v2=0, nll_loss=1.106, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=2.15, wps=42.2, ups=0.81, wpb=52.4, bsz=16, num_updates=250, lr=2.03252e-05, gnorm=15.337, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=314
2022-07-12 13:02:26 - progress_bar.py[line:274] - INFO: epoch 001:    261 / 1283 loss=1.536, loss_v1=0, loss_v2=0, nll_loss=1.239, ntokens=52.6, nsentences=15.9, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=2.36, wps=43.3, ups=0.82, wpb=52.6, bsz=15.9, num_updates=260, lr=2.11382e-05, gnorm=13.473, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=326
2022-07-12 13:02:39 - progress_bar.py[line:274] - INFO: epoch 001:    271 / 1283 loss=1.56, loss_v1=0, loss_v2=0, nll_loss=1.218, ntokens=48.1, nsentences=16, sample_size=48.1, sample_size_v1=0, sample_size_v2=0, ppl=2.33, wps=38.9, ups=0.81, wpb=48.1, bsz=16, num_updates=270, lr=2.19512e-05, gnorm=21.269, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=339
2022-07-12 13:02:51 - progress_bar.py[line:274] - INFO: epoch 001:    281 / 1283 loss=1.571, loss_v1=0, loss_v2=0, nll_loss=1.28, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=2.43, wps=42.6, ups=0.8, wpb=53.4, bsz=16, num_updates=280, lr=2.27642e-05, gnorm=15.854, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=351
2022-07-12 13:03:04 - progress_bar.py[line:274] - INFO: epoch 001:    291 / 1283 loss=1.433, loss_v1=0, loss_v2=0, nll_loss=1.132, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=2.19, wps=42.1, ups=0.81, wpb=52.3, bsz=16, num_updates=290, lr=2.35772e-05, gnorm=15.798, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=364
2022-07-12 13:03:16 - progress_bar.py[line:274] - INFO: epoch 001:    301 / 1283 loss=1.584, loss_v1=0, loss_v2=0, nll_loss=1.283, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=2.43, wps=42, ups=0.81, wpb=51.6, bsz=16, num_updates=300, lr=2.43902e-05, gnorm=13.337, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=376
2022-07-12 13:03:28 - progress_bar.py[line:274] - INFO: epoch 001:    311 / 1283 loss=1.54, loss_v1=0, loss_v2=0, nll_loss=1.271, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=2.41, wps=42.5, ups=0.8, wpb=53, bsz=16, num_updates=310, lr=2.52033e-05, gnorm=15.074, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=389
2022-07-12 13:03:41 - progress_bar.py[line:274] - INFO: epoch 001:    321 / 1283 loss=1.473, loss_v1=0, loss_v2=0, nll_loss=1.187, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=42.8, ups=0.8, wpb=53.5, bsz=16, num_updates=320, lr=2.60163e-05, gnorm=14.442, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=401
2022-07-12 13:03:53 - progress_bar.py[line:274] - INFO: epoch 001:    331 / 1283 loss=1.633, loss_v1=0, loss_v2=0, nll_loss=1.354, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=2.56, wps=42.1, ups=0.8, wpb=52.5, bsz=16, num_updates=330, lr=2.68293e-05, gnorm=15.305, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=414
2022-07-12 13:04:06 - progress_bar.py[line:274] - INFO: epoch 001:    341 / 1283 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=1.383, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=2.61, wps=41.2, ups=0.8, wpb=51.6, bsz=16, num_updates=340, lr=2.76423e-05, gnorm=15.92, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=426
2022-07-12 13:04:18 - progress_bar.py[line:274] - INFO: epoch 001:    351 / 1283 loss=1.603, loss_v1=0, loss_v2=0, nll_loss=1.278, ntokens=48.4, nsentences=16, sample_size=48.4, sample_size_v1=0, sample_size_v2=0, ppl=2.42, wps=38.8, ups=0.8, wpb=48.4, bsz=16, num_updates=350, lr=2.84553e-05, gnorm=16.956, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=439
2022-07-12 13:04:31 - progress_bar.py[line:274] - INFO: epoch 001:    361 / 1283 loss=1.465, loss_v1=0, loss_v2=0, nll_loss=1.187, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=42.8, ups=0.81, wpb=52.7, bsz=16, num_updates=360, lr=2.92683e-05, gnorm=12.013, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=451
2022-07-12 13:04:43 - progress_bar.py[line:274] - INFO: epoch 001:    371 / 1283 loss=1.513, loss_v1=0, loss_v2=0, nll_loss=1.187, ntokens=49.1, nsentences=16, sample_size=49.1, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=39.5, ups=0.8, wpb=49.1, bsz=16, num_updates=370, lr=3.00813e-05, gnorm=12.88, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=463
2022-07-12 13:04:56 - progress_bar.py[line:274] - INFO: epoch 001:    381 / 1283 loss=1.482, loss_v1=0, loss_v2=0, nll_loss=1.182, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=2.27, wps=42, ups=0.8, wpb=52.2, bsz=16, num_updates=380, lr=3.08943e-05, gnorm=12.025, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=476
2022-07-12 13:05:08 - progress_bar.py[line:274] - INFO: epoch 001:    391 / 1283 loss=1.479, loss_v1=0, loss_v2=0, nll_loss=1.228, ntokens=55.3, nsentences=16, sample_size=55.3, sample_size_v1=0, sample_size_v2=0, ppl=2.34, wps=44.5, ups=0.81, wpb=55.3, bsz=16, num_updates=390, lr=3.17073e-05, gnorm=11.349, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=488
2022-07-12 13:05:20 - progress_bar.py[line:274] - INFO: epoch 001:    401 / 1283 loss=1.439, loss_v1=0, loss_v2=0, nll_loss=1.164, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=2.24, wps=43.4, ups=0.81, wpb=53.7, bsz=16, num_updates=400, lr=3.25203e-05, gnorm=10.35, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=501
2022-07-12 13:05:33 - progress_bar.py[line:274] - INFO: epoch 001:    411 / 1283 loss=1.559, loss_v1=0, loss_v2=0, nll_loss=1.267, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=2.41, wps=41, ups=0.8, wpb=51.2, bsz=16, num_updates=410, lr=3.33333e-05, gnorm=14.719, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=513
2022-07-12 13:05:45 - progress_bar.py[line:274] - INFO: epoch 001:    421 / 1283 loss=1.427, loss_v1=0, loss_v2=0, nll_loss=1.114, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=40.7, ups=0.81, wpb=50.5, bsz=16, num_updates=420, lr=3.41463e-05, gnorm=12.575, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=525
2022-07-12 13:05:58 - progress_bar.py[line:274] - INFO: epoch 001:    431 / 1283 loss=1.515, loss_v1=0, loss_v2=0, nll_loss=1.21, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=2.31, wps=41.2, ups=0.8, wpb=51.5, bsz=16, num_updates=430, lr=3.49593e-05, gnorm=15.449, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=538
2022-07-12 13:06:10 - progress_bar.py[line:274] - INFO: epoch 001:    441 / 1283 loss=1.506, loss_v1=0, loss_v2=0, nll_loss=1.196, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=2.29, wps=41.1, ups=0.8, wpb=51.4, bsz=16, num_updates=440, lr=3.57724e-05, gnorm=11.502, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=550
2022-07-12 13:06:23 - progress_bar.py[line:274] - INFO: epoch 001:    451 / 1283 loss=1.561, loss_v1=0, loss_v2=0, nll_loss=1.275, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=2.42, wps=40.6, ups=0.8, wpb=50.5, bsz=16, num_updates=450, lr=3.65854e-05, gnorm=10.398, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=563
2022-07-12 13:06:35 - progress_bar.py[line:274] - INFO: epoch 001:    461 / 1283 loss=1.555, loss_v1=0, loss_v2=0, nll_loss=1.273, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=2.42, wps=41.9, ups=0.8, wpb=52.5, bsz=16, num_updates=460, lr=3.73984e-05, gnorm=9.33, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=575
2022-07-12 13:06:48 - progress_bar.py[line:274] - INFO: epoch 001:    471 / 1283 loss=1.532, loss_v1=0, loss_v2=0, nll_loss=1.245, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=2.37, wps=41.8, ups=0.81, wpb=51.8, bsz=16, num_updates=470, lr=3.82114e-05, gnorm=9.83, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=588
2022-07-12 13:07:00 - progress_bar.py[line:274] - INFO: epoch 001:    481 / 1283 loss=1.588, loss_v1=0, loss_v2=0, nll_loss=1.328, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=2.51, wps=44, ups=0.82, wpb=53.8, bsz=16, num_updates=480, lr=3.90244e-05, gnorm=10.367, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=600
2022-07-12 13:07:12 - progress_bar.py[line:274] - INFO: epoch 001:    491 / 1283 loss=1.488, loss_v1=0, loss_v2=0, nll_loss=1.219, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=2.33, wps=42.2, ups=0.8, wpb=52.8, bsz=16, num_updates=490, lr=3.98374e-05, gnorm=10.01, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=613
2022-07-12 13:07:25 - progress_bar.py[line:274] - INFO: epoch 001:    501 / 1283 loss=1.529, loss_v1=0, loss_v2=0, nll_loss=1.237, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=2.36, wps=42.6, ups=0.81, wpb=52.5, bsz=16, num_updates=500, lr=4.06504e-05, gnorm=10.29, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=625
2022-07-12 13:07:37 - progress_bar.py[line:274] - INFO: epoch 001:    511 / 1283 loss=1.476, loss_v1=0, loss_v2=0, nll_loss=1.209, ntokens=55.2, nsentences=16, sample_size=55.2, sample_size_v1=0, sample_size_v2=0, ppl=2.31, wps=45, ups=0.82, wpb=55.2, bsz=16, num_updates=510, lr=4.14634e-05, gnorm=12.308, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=637
2022-07-12 13:07:49 - progress_bar.py[line:274] - INFO: epoch 001:    521 / 1283 loss=1.515, loss_v1=0, loss_v2=0, nll_loss=1.198, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=2.29, wps=39.7, ups=0.8, wpb=49.8, bsz=16, num_updates=520, lr=4.22764e-05, gnorm=9.896, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=650
2022-07-12 13:08:02 - progress_bar.py[line:274] - INFO: epoch 001:    531 / 1283 loss=1.404, loss_v1=0, loss_v2=0, nll_loss=1.142, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=2.21, wps=43.7, ups=0.8, wpb=54.6, bsz=16, num_updates=530, lr=4.30894e-05, gnorm=11.266, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=662
2022-07-12 13:08:14 - progress_bar.py[line:274] - INFO: epoch 001:    541 / 1283 loss=1.509, loss_v1=0, loss_v2=0, nll_loss=1.228, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=2.34, wps=42.4, ups=0.8, wpb=53.2, bsz=16, num_updates=540, lr=4.39024e-05, gnorm=9.482, clip=100, loss_scale=128, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=675
2022-07-12 13:08:27 - progress_bar.py[line:274] - INFO: epoch 001:    551 / 1283 loss=1.531, loss_v1=0, loss_v2=0, nll_loss=1.269, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=2.41, wps=43.5, ups=0.81, wpb=53.8, bsz=16, num_updates=550, lr=4.47154e-05, gnorm=11.802, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=687
2022-07-12 13:08:39 - progress_bar.py[line:274] - INFO: epoch 001:    561 / 1283 loss=1.465, loss_v1=0, loss_v2=0, nll_loss=1.199, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=2.3, wps=42.5, ups=0.8, wpb=52.9, bsz=16, num_updates=560, lr=4.55285e-05, gnorm=10.644, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=700
2022-07-12 13:08:52 - progress_bar.py[line:274] - INFO: epoch 001:    571 / 1283 loss=1.498, loss_v1=0, loss_v2=0, nll_loss=1.175, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=2.26, wps=40, ups=0.8, wpb=49.9, bsz=16, num_updates=570, lr=4.63415e-05, gnorm=12.408, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=712
2022-07-12 13:09:04 - progress_bar.py[line:274] - INFO: epoch 001:    581 / 1283 loss=1.472, loss_v1=0, loss_v2=0, nll_loss=1.166, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=2.24, wps=42.8, ups=0.81, wpb=52.9, bsz=16, num_updates=580, lr=4.71545e-05, gnorm=9.747, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=724
2022-07-12 13:09:17 - progress_bar.py[line:274] - INFO: epoch 001:    591 / 1283 loss=1.512, loss_v1=0, loss_v2=0, nll_loss=1.251, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=2.38, wps=42.6, ups=0.8, wpb=53.1, bsz=16, num_updates=590, lr=4.79675e-05, gnorm=8.795, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=737
2022-07-12 13:09:29 - progress_bar.py[line:274] - INFO: epoch 001:    601 / 1283 loss=1.534, loss_v1=0, loss_v2=0, nll_loss=1.268, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=2.41, wps=41, ups=0.81, wpb=50.8, bsz=16, num_updates=600, lr=4.87805e-05, gnorm=9.905, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=749
2022-07-12 13:09:41 - progress_bar.py[line:274] - INFO: epoch 001:    611 / 1283 loss=1.523, loss_v1=0, loss_v2=0, nll_loss=1.254, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=2.39, wps=42.2, ups=0.8, wpb=52.5, bsz=16, num_updates=610, lr=4.95935e-05, gnorm=10.315, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=762
2022-07-12 13:09:54 - progress_bar.py[line:274] - INFO: epoch 001:    621 / 1283 loss=1.548, loss_v1=0, loss_v2=0, nll_loss=1.288, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=2.44, wps=42.5, ups=0.8, wpb=53, bsz=16, num_updates=620, lr=4.99831e-05, gnorm=10.831, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=774
2022-07-12 13:10:06 - progress_bar.py[line:274] - INFO: epoch 001:    631 / 1283 loss=1.522, loss_v1=0, loss_v2=0, nll_loss=1.242, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=2.36, wps=41.7, ups=0.81, wpb=51.5, bsz=16, num_updates=630, lr=4.99493e-05, gnorm=10.353, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=786
2022-07-12 13:10:19 - progress_bar.py[line:274] - INFO: epoch 001:    641 / 1283 loss=1.461, loss_v1=0, loss_v2=0, nll_loss=1.191, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=42.9, ups=0.8, wpb=53.8, bsz=16, num_updates=640, lr=4.99154e-05, gnorm=9.438, clip=100, loss_scale=128, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=799
2022-07-12 13:10:31 - progress_bar.py[line:274] - INFO: epoch 001:    651 / 1283 loss=1.414, loss_v1=0, loss_v2=0, nll_loss=1.162, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=2.24, wps=43.3, ups=0.81, wpb=53.7, bsz=16, num_updates=650, lr=4.98816e-05, gnorm=7.853, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=811
2022-07-12 13:10:44 - progress_bar.py[line:274] - INFO: epoch 001:    661 / 1283 loss=1.57, loss_v1=0, loss_v2=0, nll_loss=1.296, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=2.46, wps=40.3, ups=0.81, wpb=50, bsz=16, num_updates=660, lr=4.98478e-05, gnorm=8.915, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=824
2022-07-12 13:10:49 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 10.73 GiB total capacity; 8.44 GiB already allocated; 181.19 MiB free; 9.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 13:10:49 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8367 MB |    9220 MB |   70853 GB |   70845 GB |
|       from large pool |    8221 MB |    9016 MB |   70477 GB |   70469 GB |
|       from small pool |     145 MB |     207 MB |     375 GB |     375 GB |
|---------------------------------------------------------------------------|
| Active memory         |    8367 MB |    9220 MB |   70853 GB |   70845 GB |
|       from large pool |    8221 MB |    9016 MB |   70477 GB |   70469 GB |
|       from small pool |     145 MB |     207 MB |     375 GB |     375 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9436 MB |    9500 MB |   25394 MB |   15958 MB |
|       from large pool |    9286 MB |    9286 MB |   25034 MB |   15748 MB |
|       from small pool |     150 MB |     214 MB |     360 MB |     210 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1068 MB |    3578 MB |   68360 GB |   68359 GB |
|       from large pool |    1064 MB |    3576 MB |   67926 GB |   67925 GB |
|       from small pool |       4 MB |      18 MB |     434 GB |     434 GB |
|---------------------------------------------------------------------------|
| Allocations           |    4049    |    4372    |   11034 K  |   11030 K  |
|       from large pool |     835    |     910    |    4706 K  |    4705 K  |
|       from small pool |    3214    |    3466    |    6328 K  |    6325 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4049    |    4372    |   11034 K  |   11030 K  |
|       from large pool |     835    |     910    |    4706 K  |    4705 K  |
|       from small pool |    3214    |    3466    |    6328 K  |    6325 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     179    |     211    |     483    |     304    |
|       from large pool |     104    |     104    |     303    |     199    |
|       from small pool |      75    |     107    |     180    |     105    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      48    |     185    |    5665 K  |    5665 K  |
|       from large pool |      36    |      81    |    2361 K  |    2361 K  |
|       from small pool |      12    |     120    |    3304 K  |    3304 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 13:10:49 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 13:10:54 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 13:10:58 - progress_bar.py[line:274] - INFO: epoch 001:    673 / 1283 loss=1.438, loss_v1=0, loss_v2=0, nll_loss=1.176, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=2.26, wps=38.1, ups=0.71, wpb=53.8, bsz=16, num_updates=670, lr=4.9814e-05, gnorm=8.245, clip=100, loss_scale=64, train_wall=14, gb_free=2.3, ema_decay=0.9999, wall=838
2022-07-12 13:11:10 - progress_bar.py[line:274] - INFO: epoch 001:    683 / 1283 loss=1.464, loss_v1=0, loss_v2=0, nll_loss=1.196, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=2.29, wps=44, ups=0.82, wpb=54, bsz=16, num_updates=680, lr=4.97801e-05, gnorm=11.133, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=850
2022-07-12 13:11:23 - progress_bar.py[line:274] - INFO: epoch 001:    693 / 1283 loss=1.521, loss_v1=0, loss_v2=0, nll_loss=1.214, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=2.32, wps=40.4, ups=0.79, wpb=50.8, bsz=16, num_updates=690, lr=4.97463e-05, gnorm=10.794, clip=100, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=863
2022-07-12 13:11:35 - progress_bar.py[line:274] - INFO: epoch 001:    703 / 1283 loss=1.548, loss_v1=0, loss_v2=0, nll_loss=1.264, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=2.4, wps=40.6, ups=0.82, wpb=49.7, bsz=16, num_updates=700, lr=4.97125e-05, gnorm=8.99, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=875
2022-07-12 13:11:47 - progress_bar.py[line:274] - INFO: epoch 001:    713 / 1283 loss=1.434, loss_v1=0, loss_v2=0, nll_loss=1.163, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=2.24, wps=42.1, ups=0.81, wpb=51.9, bsz=16, num_updates=710, lr=4.96786e-05, gnorm=8.392, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=887
2022-07-12 13:11:59 - progress_bar.py[line:274] - INFO: epoch 001:    723 / 1283 loss=1.415, loss_v1=0, loss_v2=0, nll_loss=1.14, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=2.2, wps=44.3, ups=0.81, wpb=54.6, bsz=16, num_updates=720, lr=4.96448e-05, gnorm=9.368, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=900
2022-07-12 13:12:12 - progress_bar.py[line:274] - INFO: epoch 001:    733 / 1283 loss=1.522, loss_v1=0, loss_v2=0, nll_loss=1.252, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=2.38, wps=41.5, ups=0.81, wpb=51.5, bsz=16, num_updates=730, lr=4.9611e-05, gnorm=10.545, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=912
2022-07-12 13:12:20 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-12 13:12:25 - progress_bar.py[line:274] - INFO: epoch 001:    744 / 1283 loss=1.503, loss_v1=0, loss_v2=0, nll_loss=1.235, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=2.35, wps=39.3, ups=0.74, wpb=52.9, bsz=16, num_updates=740, lr=4.95772e-05, gnorm=8.359, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=926
2022-07-12 13:12:38 - progress_bar.py[line:274] - INFO: epoch 001:    754 / 1283 loss=1.449, loss_v1=0, loss_v2=0, nll_loss=1.188, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=42.6, ups=0.81, wpb=52.4, bsz=16, num_updates=750, lr=4.95433e-05, gnorm=7.264, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=938
2022-07-12 13:12:50 - progress_bar.py[line:274] - INFO: epoch 001:    764 / 1283 loss=1.503, loss_v1=0, loss_v2=0, nll_loss=1.235, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=2.35, wps=41.9, ups=0.81, wpb=52, bsz=16, num_updates=760, lr=4.95095e-05, gnorm=9.8, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=950
2022-07-12 13:13:02 - progress_bar.py[line:274] - INFO: epoch 001:    774 / 1283 loss=1.635, loss_v1=0, loss_v2=0, nll_loss=1.376, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=2.6, wps=42, ups=0.81, wpb=51.9, bsz=16, num_updates=770, lr=4.94757e-05, gnorm=9.264, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=963
2022-07-12 13:13:15 - progress_bar.py[line:274] - INFO: epoch 001:    784 / 1283 loss=1.441, loss_v1=0, loss_v2=0, nll_loss=1.195, ntokens=54.8, nsentences=16, sample_size=54.8, sample_size_v1=0, sample_size_v2=0, ppl=2.29, wps=44.2, ups=0.81, wpb=54.8, bsz=16, num_updates=780, lr=4.94419e-05, gnorm=7.434, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=975
2022-07-12 13:13:16 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 178.00 MiB (GPU 0; 10.73 GiB total capacity; 8.48 GiB already allocated; 179.19 MiB free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 13:13:16 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8397 MB |    9293 MB |   83557 GB |   83549 GB |
|       from large pool |    8251 MB |    9083 MB |   83114 GB |   83106 GB |
|       from small pool |     145 MB |     212 MB |     442 GB |     442 GB |
|---------------------------------------------------------------------------|
| Active memory         |    8397 MB |    9293 MB |   83557 GB |   83549 GB |
|       from large pool |    8251 MB |    9083 MB |   83114 GB |   83106 GB |
|       from small pool |     145 MB |     212 MB |     442 GB |     442 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9438 MB |    9504 MB |   25474 MB |   16036 MB |
|       from large pool |    9286 MB |    9286 MB |   25034 MB |   15748 MB |
|       from small pool |     152 MB |     218 MB |     440 MB |     288 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1040 MB |    3578 MB |   80737 GB |   80736 GB |
|       from large pool |    1034 MB |    3576 MB |   80224 GB |   80223 GB |
|       from small pool |       6 MB |      22 MB |     512 GB |     512 GB |
|---------------------------------------------------------------------------|
| Allocations           |    4049    |    4372    |   13012 K  |   13008 K  |
|       from large pool |     835    |     910    |    5550 K  |    5549 K  |
|       from small pool |    3214    |    3466    |    7461 K  |    7458 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4049    |    4372    |   13012 K  |   13008 K  |
|       from large pool |     835    |     910    |    5550 K  |    5549 K  |
|       from small pool |    3214    |    3466    |    7461 K  |    7458 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     180    |     213    |     523    |     343    |
|       from large pool |     104    |     104    |     303    |     199    |
|       from small pool |      76    |     109    |     220    |     144    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      54    |     189    |    6679 K  |    6679 K  |
|       from large pool |      39    |      76    |    2786 K  |    2786 K  |
|       from small pool |      15    |     135    |    3893 K  |    3893 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 13:13:16 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 13:13:28 - progress_bar.py[line:274] - INFO: epoch 001:    795 / 1283 loss=1.378, loss_v1=0, loss_v2=0, nll_loss=1.106, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=2.15, wps=41.3, ups=0.78, wpb=53, bsz=16, num_updates=790, lr=4.9408e-05, gnorm=6.636, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=988
2022-07-12 13:13:40 - progress_bar.py[line:274] - INFO: epoch 001:    805 / 1283 loss=1.448, loss_v1=0, loss_v2=0, nll_loss=1.139, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=2.2, wps=40.1, ups=0.81, wpb=49.7, bsz=16, num_updates=800, lr=4.93742e-05, gnorm=7.903, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=1000
2022-07-12 13:13:53 - progress_bar.py[line:274] - INFO: epoch 001:    815 / 1283 loss=1.448, loss_v1=0, loss_v2=0, nll_loss=1.159, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=2.23, wps=41.6, ups=0.8, wpb=52.1, bsz=16, num_updates=810, lr=4.93404e-05, gnorm=9.781, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1013
2022-07-12 13:14:05 - progress_bar.py[line:274] - INFO: epoch 001:    825 / 1283 loss=1.558, loss_v1=0, loss_v2=0, nll_loss=1.29, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=2.45, wps=41.4, ups=0.8, wpb=51.7, bsz=16, num_updates=820, lr=4.93065e-05, gnorm=9.006, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=1025
2022-07-12 13:14:18 - progress_bar.py[line:274] - INFO: epoch 001:    835 / 1283 loss=1.437, loss_v1=0, loss_v2=0, nll_loss=1.14, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=2.2, wps=40, ups=0.8, wpb=50.3, bsz=16, num_updates=830, lr=4.92727e-05, gnorm=8.046, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=1038
2022-07-12 13:14:30 - progress_bar.py[line:274] - INFO: epoch 001:    845 / 1283 loss=1.474, loss_v1=0, loss_v2=0, nll_loss=1.185, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=2.27, wps=41.1, ups=0.8, wpb=51.2, bsz=16, num_updates=840, lr=4.92389e-05, gnorm=7.444, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=1050
2022-07-12 13:14:42 - progress_bar.py[line:274] - INFO: epoch 001:    855 / 1283 loss=1.485, loss_v1=0, loss_v2=0, nll_loss=1.212, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=2.32, wps=43.2, ups=0.81, wpb=53.3, bsz=16, num_updates=850, lr=4.92051e-05, gnorm=8.951, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=1063
2022-07-12 13:14:55 - progress_bar.py[line:274] - INFO: epoch 001:    865 / 1283 loss=1.457, loss_v1=0, loss_v2=0, nll_loss=1.133, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=2.19, wps=40, ups=0.8, wpb=50, bsz=16, num_updates=860, lr=4.91712e-05, gnorm=8.902, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=1075
2022-07-12 13:15:07 - progress_bar.py[line:274] - INFO: epoch 001:    875 / 1283 loss=1.443, loss_v1=0, loss_v2=0, nll_loss=1.162, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=2.24, wps=42, ups=0.81, wpb=51.8, bsz=16, num_updates=870, lr=4.91374e-05, gnorm=8.788, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1087
2022-07-12 13:15:20 - progress_bar.py[line:274] - INFO: epoch 001:    885 / 1283 loss=1.441, loss_v1=0, loss_v2=0, nll_loss=1.189, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=42.1, ups=0.8, wpb=52.4, bsz=16, num_updates=880, lr=4.91036e-05, gnorm=9.087, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1100
2022-07-12 13:15:32 - progress_bar.py[line:274] - INFO: epoch 001:    895 / 1283 loss=1.422, loss_v1=0, loss_v2=0, nll_loss=1.177, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=2.26, wps=43.2, ups=0.8, wpb=54, bsz=16, num_updates=890, lr=4.90698e-05, gnorm=9.395, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1112
2022-07-12 13:15:45 - progress_bar.py[line:274] - INFO: epoch 001:    905 / 1283 loss=1.437, loss_v1=0, loss_v2=0, nll_loss=1.182, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=2.27, wps=43.5, ups=0.81, wpb=53.9, bsz=16, num_updates=900, lr=4.90359e-05, gnorm=8.658, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=1125
2022-07-12 13:15:57 - progress_bar.py[line:274] - INFO: epoch 001:    915 / 1283 loss=1.521, loss_v1=0, loss_v2=0, nll_loss=1.23, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=2.35, wps=39.9, ups=0.8, wpb=50.1, bsz=16, num_updates=910, lr=4.90021e-05, gnorm=8.507, clip=100, loss_scale=32, train_wall=13, gb_free=2.5, ema_decay=0.9999, wall=1137
2022-07-12 13:16:09 - progress_bar.py[line:274] - INFO: epoch 001:    925 / 1283 loss=1.421, loss_v1=0, loss_v2=0, nll_loss=1.127, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=2.18, wps=41.3, ups=0.81, wpb=50.8, bsz=16, num_updates=920, lr=4.89683e-05, gnorm=7.861, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1150
2022-07-12 13:16:22 - progress_bar.py[line:274] - INFO: epoch 001:    935 / 1283 loss=1.48, loss_v1=0, loss_v2=0, nll_loss=1.21, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=2.31, wps=41.8, ups=0.8, wpb=52.2, bsz=16, num_updates=930, lr=4.89344e-05, gnorm=8.018, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1162
2022-07-12 13:16:34 - progress_bar.py[line:274] - INFO: epoch 001:    945 / 1283 loss=1.497, loss_v1=0, loss_v2=0, nll_loss=1.232, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=2.35, wps=42.6, ups=0.8, wpb=52.9, bsz=16, num_updates=940, lr=4.89006e-05, gnorm=7.101, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=1175
2022-07-12 13:16:47 - progress_bar.py[line:274] - INFO: epoch 001:    955 / 1283 loss=1.439, loss_v1=0, loss_v2=0, nll_loss=1.158, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=2.23, wps=42, ups=0.81, wpb=52.1, bsz=16, num_updates=950, lr=4.88668e-05, gnorm=8.57, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=1187
2022-07-12 13:16:59 - progress_bar.py[line:274] - INFO: epoch 001:    965 / 1283 loss=1.408, loss_v1=0, loss_v2=0, nll_loss=1.15, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=2.22, wps=42, ups=0.81, wpb=52, bsz=16, num_updates=960, lr=4.8833e-05, gnorm=7.423, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1199
2022-07-12 13:17:11 - progress_bar.py[line:274] - INFO: epoch 001:    975 / 1283 loss=1.462, loss_v1=0, loss_v2=0, nll_loss=1.186, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=41.4, ups=0.81, wpb=50.9, bsz=16, num_updates=970, lr=4.87991e-05, gnorm=7.947, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1212
2022-07-12 13:17:24 - progress_bar.py[line:274] - INFO: epoch 001:    985 / 1283 loss=1.556, loss_v1=0, loss_v2=0, nll_loss=1.257, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=2.39, wps=39.7, ups=0.8, wpb=49.7, bsz=16, num_updates=980, lr=4.87653e-05, gnorm=8.026, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1224
2022-07-12 13:17:36 - progress_bar.py[line:274] - INFO: epoch 001:    995 / 1283 loss=1.451, loss_v1=0, loss_v2=0, nll_loss=1.195, ntokens=54.5, nsentences=16, sample_size=54.5, sample_size_v1=0, sample_size_v2=0, ppl=2.29, wps=44.2, ups=0.81, wpb=54.5, bsz=16, num_updates=990, lr=4.87315e-05, gnorm=6.87, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1237
2022-07-12 13:17:49 - progress_bar.py[line:274] - INFO: epoch 001:   1005 / 1283 loss=1.435, loss_v1=0, loss_v2=0, nll_loss=1.133, ntokens=48.7, nsentences=16, sample_size=48.7, sample_size_v1=0, sample_size_v2=0, ppl=2.19, wps=38.6, ups=0.79, wpb=48.7, bsz=16, num_updates=1000, lr=4.86977e-05, gnorm=8.728, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=1249
2022-07-12 13:18:01 - progress_bar.py[line:274] - INFO: epoch 001:   1015 / 1283 loss=1.484, loss_v1=0, loss_v2=0, nll_loss=1.186, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=40.8, ups=0.81, wpb=50.4, bsz=16, num_updates=1010, lr=4.86638e-05, gnorm=8.557, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=1261
2022-07-12 13:18:14 - progress_bar.py[line:274] - INFO: epoch 001:   1025 / 1283 loss=1.55, loss_v1=0, loss_v2=0, nll_loss=1.259, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=2.39, wps=40.3, ups=0.81, wpb=49.6, bsz=16, num_updates=1020, lr=4.863e-05, gnorm=7.866, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1274
2022-07-12 13:18:26 - progress_bar.py[line:274] - INFO: epoch 001:   1035 / 1283 loss=1.375, loss_v1=0, loss_v2=0, nll_loss=1.089, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=2.13, wps=43, ups=0.8, wpb=53.5, bsz=16, num_updates=1030, lr=4.85962e-05, gnorm=7.291, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1286
2022-07-12 13:18:38 - progress_bar.py[line:274] - INFO: epoch 001:   1045 / 1283 loss=1.447, loss_v1=0, loss_v2=0, nll_loss=1.181, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=2.27, wps=41.8, ups=0.81, wpb=51.7, bsz=16, num_updates=1040, lr=4.85623e-05, gnorm=8.205, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=1299
2022-07-12 13:18:51 - progress_bar.py[line:274] - INFO: epoch 001:   1055 / 1283 loss=1.441, loss_v1=0, loss_v2=0, nll_loss=1.176, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=2.26, wps=42.5, ups=0.8, wpb=53, bsz=16, num_updates=1050, lr=4.85285e-05, gnorm=7.595, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1311
2022-07-12 13:19:03 - progress_bar.py[line:274] - INFO: epoch 001:   1065 / 1283 loss=1.46, loss_v1=0, loss_v2=0, nll_loss=1.177, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=2.26, wps=42.2, ups=0.81, wpb=52.2, bsz=16, num_updates=1060, lr=4.84947e-05, gnorm=8.476, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=1323
2022-07-12 13:19:16 - progress_bar.py[line:274] - INFO: epoch 001:   1075 / 1283 loss=1.528, loss_v1=0, loss_v2=0, nll_loss=1.249, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=2.38, wps=41.9, ups=0.81, wpb=52, bsz=16, num_updates=1070, lr=4.84609e-05, gnorm=8.927, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=1336
2022-07-12 13:19:28 - progress_bar.py[line:274] - INFO: epoch 001:   1085 / 1283 loss=1.424, loss_v1=0, loss_v2=0, nll_loss=1.165, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=2.24, wps=44, ups=0.81, wpb=54.2, bsz=16, num_updates=1080, lr=4.8427e-05, gnorm=7.565, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1348
2022-07-12 13:19:40 - progress_bar.py[line:274] - INFO: epoch 001:   1095 / 1283 loss=1.425, loss_v1=0, loss_v2=0, nll_loss=1.128, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=2.19, wps=40, ups=0.81, wpb=49.6, bsz=16, num_updates=1090, lr=4.83932e-05, gnorm=7.818, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1361
2022-07-12 13:19:53 - progress_bar.py[line:274] - INFO: epoch 001:   1105 / 1283 loss=1.497, loss_v1=0, loss_v2=0, nll_loss=1.19, ntokens=49.1, nsentences=16, sample_size=49.1, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=39.5, ups=0.8, wpb=49.1, bsz=16, num_updates=1100, lr=4.83594e-05, gnorm=9.918, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1373
2022-07-12 13:20:05 - progress_bar.py[line:274] - INFO: epoch 001:   1115 / 1283 loss=1.29, loss_v1=0, loss_v2=0, nll_loss=0.999, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=2, wps=43, ups=0.8, wpb=53.8, bsz=16, num_updates=1110, lr=4.83256e-05, gnorm=8.032, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1386
2022-07-12 13:20:18 - progress_bar.py[line:274] - INFO: epoch 001:   1125 / 1283 loss=1.516, loss_v1=0, loss_v2=0, nll_loss=1.267, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=2.41, wps=41.8, ups=0.81, wpb=51.8, bsz=16, num_updates=1120, lr=4.82917e-05, gnorm=7.431, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=1398
2022-07-12 13:20:30 - progress_bar.py[line:274] - INFO: epoch 001:   1135 / 1283 loss=1.371, loss_v1=0, loss_v2=0, nll_loss=1.093, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=2.13, wps=41, ups=0.81, wpb=50.6, bsz=16, num_updates=1130, lr=4.82579e-05, gnorm=7.778, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1410
2022-07-12 13:20:42 - progress_bar.py[line:274] - INFO: epoch 001:   1145 / 1283 loss=1.28, loss_v1=0, loss_v2=0, nll_loss=1.013, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=2.02, wps=43, ups=0.8, wpb=53.5, bsz=16, num_updates=1140, lr=4.82241e-05, gnorm=7.199, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1423
2022-07-12 13:20:55 - progress_bar.py[line:274] - INFO: epoch 001:   1155 / 1283 loss=1.501, loss_v1=0, loss_v2=0, nll_loss=1.24, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=2.36, wps=41.2, ups=0.8, wpb=51.4, bsz=16, num_updates=1150, lr=4.81902e-05, gnorm=8.639, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1435
2022-07-12 13:21:07 - progress_bar.py[line:274] - INFO: epoch 001:   1165 / 1283 loss=1.393, loss_v1=0, loss_v2=0, nll_loss=1.119, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=42.1, ups=0.8, wpb=52.4, bsz=16, num_updates=1160, lr=4.81564e-05, gnorm=8.279, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1448
2022-07-12 13:21:20 - progress_bar.py[line:274] - INFO: epoch 001:   1175 / 1283 loss=1.294, loss_v1=0, loss_v2=0, nll_loss=0.987, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.98, wps=42.2, ups=0.81, wpb=52.3, bsz=16, num_updates=1170, lr=4.81226e-05, gnorm=8.933, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=1460
2022-07-12 13:21:32 - progress_bar.py[line:274] - INFO: epoch 001:   1185 / 1283 loss=1.393, loss_v1=0, loss_v2=0, nll_loss=1.096, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=2.14, wps=42.7, ups=0.8, wpb=53.3, bsz=16, num_updates=1180, lr=4.80888e-05, gnorm=9.398, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1472
2022-07-12 13:21:45 - progress_bar.py[line:274] - INFO: epoch 001:   1195 / 1283 loss=1.492, loss_v1=0, loss_v2=0, nll_loss=1.199, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=2.3, wps=40.8, ups=0.8, wpb=51, bsz=16, num_updates=1190, lr=4.80549e-05, gnorm=9.701, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=1485
2022-07-12 13:21:57 - progress_bar.py[line:274] - INFO: epoch 001:   1205 / 1283 loss=1.393, loss_v1=0, loss_v2=0, nll_loss=1.118, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=41.5, ups=0.81, wpb=51.5, bsz=16, num_updates=1200, lr=4.80211e-05, gnorm=9.157, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=1497
2022-07-12 13:22:10 - progress_bar.py[line:274] - INFO: epoch 001:   1215 / 1283 loss=1.37, loss_v1=0, loss_v2=0, nll_loss=1.065, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=2.09, wps=39.6, ups=0.8, wpb=49.4, bsz=16, num_updates=1210, lr=4.79873e-05, gnorm=8.569, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=1510
2022-07-12 13:22:22 - progress_bar.py[line:274] - INFO: epoch 001:   1225 / 1283 loss=1.418, loss_v1=0, loss_v2=0, nll_loss=1.131, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=2.19, wps=42.1, ups=0.81, wpb=51.9, bsz=16, num_updates=1220, lr=4.79535e-05, gnorm=7.524, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1522
2022-07-12 13:22:34 - progress_bar.py[line:274] - INFO: epoch 001:   1235 / 1283 loss=1.347, loss_v1=0, loss_v2=0, nll_loss=1.028, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=2.04, wps=40.6, ups=0.81, wpb=50.2, bsz=16, num_updates=1230, lr=4.79196e-05, gnorm=7.332, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=1535
2022-07-12 13:22:47 - progress_bar.py[line:274] - INFO: epoch 001:   1245 / 1283 loss=1.445, loss_v1=0, loss_v2=0, nll_loss=1.147, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=2.21, wps=40.8, ups=0.81, wpb=50.6, bsz=16, num_updates=1240, lr=4.78858e-05, gnorm=7.355, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=1547
2022-07-12 13:22:59 - progress_bar.py[line:274] - INFO: epoch 001:   1255 / 1283 loss=1.333, loss_v1=0, loss_v2=0, nll_loss=1.056, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=2.08, wps=43.1, ups=0.81, wpb=53.5, bsz=16, num_updates=1250, lr=4.7852e-05, gnorm=7.35, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=1559
2022-07-12 13:23:12 - progress_bar.py[line:274] - INFO: epoch 001:   1265 / 1283 loss=1.389, loss_v1=0, loss_v2=0, nll_loss=1.084, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=2.12, wps=41.5, ups=0.8, wpb=51.9, bsz=16, num_updates=1260, lr=4.78181e-05, gnorm=6.918, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=1572
2022-07-12 13:23:24 - progress_bar.py[line:274] - INFO: epoch 001:   1275 / 1283 loss=1.606, loss_v1=0, loss_v2=0, nll_loss=1.322, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=2.5, wps=39.7, ups=0.8, wpb=49.8, bsz=16, num_updates=1270, lr=4.77843e-05, gnorm=7.187, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=1584
2022-07-12 13:23:34 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
/data1/6willrut/OFA/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  beams_buf = indices_buf // vocab_size
/data1/6willrut/OFA/models/sequence_generator.py:699: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  unfin_idx = bbsz_idx // beam_size
2022-07-12 13:31:40 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 1.295 | loss_v1 0 | loss_v2 0 | nll_loss 1.03 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.04 | vqa_score 0.0882 | wps 30.2 | wpb 13.6 | bsz 4 | num_updates 1278
2022-07-12 13:31:40 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 1 @ 1278 updates
2022-07-12 13:31:40 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints1.pt
2022-07-12 13:32:12 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints1.pt
2022-07-12 13:33:08 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints1.pt (epoch 1 @ 1278 updates, score 0.0882) (writing took 88.14399039698765 seconds)
2022-07-12 13:33:08 - train.py[line:323] - INFO: end of epoch 1 (average epoch stats below)
2022-07-12 13:33:08 - progress_bar.py[line:282] - INFO: epoch 001 | loss 1.578 | loss_v1 0 | loss_v2 0 | nll_loss 1.293 | ntokens 51.944 | nsentences 15.996 | sample_size 51.944 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.45 | wps 30.7 | ups 0.59 | wpb 51.9 | bsz 16 | num_updates 1278 | lr 4.77573e-05 | gnorm 11.958 | clip 100 | loss_scale 64 | train_wall 1584 | gb_free 2.3 | ema_decay 0.9999 | wall 2168
2022-07-12 13:33:08 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 13:33:10 - trainer.py[line:703] - INFO: begin training epoch 2
2022-07-12 13:33:10 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 13:33:13 - progress_bar.py[line:274] - INFO: epoch 002:      2 / 1283 loss=1.522, loss_v1=0, loss_v2=0, nll_loss=1.229, ntokens=47.5, nsentences=15.6, sample_size=47.5, sample_size_v1=0, sample_size_v2=0, ppl=2.34, wps=0.8, ups=0.02, wpb=47.5, bsz=15.6, num_updates=1280, lr=4.77505e-05, gnorm=7.375, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2173
2022-07-12 13:33:25 - progress_bar.py[line:274] - INFO: epoch 002:     12 / 1283 loss=1.421, loss_v1=0, loss_v2=0, nll_loss=1.145, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=2.21, wps=42.2, ups=0.81, wpb=51.9, bsz=16, num_updates=1290, lr=4.77167e-05, gnorm=7.484, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=2186
2022-07-12 13:33:38 - progress_bar.py[line:274] - INFO: epoch 002:     22 / 1283 loss=1.44, loss_v1=0, loss_v2=0, nll_loss=1.16, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=2.23, wps=41.8, ups=0.81, wpb=51.3, bsz=16, num_updates=1300, lr=4.76828e-05, gnorm=6.561, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2198
2022-07-12 13:33:50 - progress_bar.py[line:274] - INFO: epoch 002:     32 / 1283 loss=1.296, loss_v1=0, loss_v2=0, nll_loss=1.014, ntokens=54.3, nsentences=16, sample_size=54.3, sample_size_v1=0, sample_size_v2=0, ppl=2.02, wps=44.6, ups=0.82, wpb=54.3, bsz=16, num_updates=1310, lr=4.7649e-05, gnorm=7.631, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2210
2022-07-12 13:34:02 - progress_bar.py[line:274] - INFO: epoch 002:     42 / 1283 loss=1.393, loss_v1=0, loss_v2=0, nll_loss=1.112, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=2.16, wps=42.8, ups=0.82, wpb=52.3, bsz=16, num_updates=1320, lr=4.76152e-05, gnorm=6.872, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=2222
2022-07-12 13:34:14 - progress_bar.py[line:274] - INFO: epoch 002:     52 / 1283 loss=1.413, loss_v1=0, loss_v2=0, nll_loss=1.136, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=2.2, wps=42.8, ups=0.82, wpb=52.2, bsz=16, num_updates=1330, lr=4.75814e-05, gnorm=6.398, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2234
2022-07-12 13:34:27 - progress_bar.py[line:274] - INFO: epoch 002:     62 / 1283 loss=1.319, loss_v1=0, loss_v2=0, nll_loss=1.003, ntokens=49, nsentences=16, sample_size=49, sample_size_v1=0, sample_size_v2=0, ppl=2, wps=39, ups=0.8, wpb=49, bsz=16, num_updates=1340, lr=4.75475e-05, gnorm=6.755, clip=100, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=2247
2022-07-12 13:34:39 - progress_bar.py[line:274] - INFO: epoch 002:     72 / 1283 loss=1.483, loss_v1=0, loss_v2=0, nll_loss=1.217, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=2.32, wps=43.7, ups=0.82, wpb=53.6, bsz=16, num_updates=1350, lr=4.75137e-05, gnorm=6.147, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2259
2022-07-12 13:34:51 - progress_bar.py[line:274] - INFO: epoch 002:     82 / 1283 loss=1.28, loss_v1=0, loss_v2=0, nll_loss=0.987, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.98, wps=43.3, ups=0.81, wpb=53.3, bsz=16, num_updates=1360, lr=4.74799e-05, gnorm=7.03, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=2271
2022-07-12 13:35:04 - progress_bar.py[line:274] - INFO: epoch 002:     92 / 1283 loss=1.428, loss_v1=0, loss_v2=0, nll_loss=1.147, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=2.21, wps=42.9, ups=0.81, wpb=52.7, bsz=16, num_updates=1370, lr=4.7446e-05, gnorm=7.091, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2284
2022-07-12 13:35:16 - progress_bar.py[line:274] - INFO: epoch 002:    102 / 1283 loss=1.445, loss_v1=0, loss_v2=0, nll_loss=1.148, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=2.22, wps=40.7, ups=0.81, wpb=50, bsz=16, num_updates=1380, lr=4.74122e-05, gnorm=8.182, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2296
2022-07-12 13:35:28 - progress_bar.py[line:274] - INFO: epoch 002:    112 / 1283 loss=1.505, loss_v1=0, loss_v2=0, nll_loss=1.206, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=2.31, wps=40.4, ups=0.8, wpb=50.2, bsz=16, num_updates=1390, lr=4.73784e-05, gnorm=7.666, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2308
2022-07-12 13:35:41 - progress_bar.py[line:274] - INFO: epoch 002:    122 / 1283 loss=1.394, loss_v1=0, loss_v2=0, nll_loss=1.118, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=42.3, ups=0.81, wpb=52.3, bsz=16, num_updates=1400, lr=4.73446e-05, gnorm=5.918, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2321
2022-07-12 13:35:53 - progress_bar.py[line:274] - INFO: epoch 002:    132 / 1283 loss=1.415, loss_v1=0, loss_v2=0, nll_loss=1.117, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=41.1, ups=0.81, wpb=50.5, bsz=16, num_updates=1410, lr=4.73107e-05, gnorm=6.522, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2333
2022-07-12 13:36:05 - progress_bar.py[line:274] - INFO: epoch 002:    142 / 1283 loss=1.488, loss_v1=0, loss_v2=0, nll_loss=1.235, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=2.35, wps=44.4, ups=0.82, wpb=54.1, bsz=16, num_updates=1420, lr=4.72769e-05, gnorm=6.734, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2345
2022-07-12 13:36:17 - progress_bar.py[line:274] - INFO: epoch 002:    152 / 1283 loss=1.506, loss_v1=0, loss_v2=0, nll_loss=1.243, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=2.37, wps=43.5, ups=0.81, wpb=53.4, bsz=16, num_updates=1430, lr=4.72431e-05, gnorm=6.601, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2358
2022-07-12 13:36:30 - progress_bar.py[line:274] - INFO: epoch 002:    162 / 1283 loss=1.34, loss_v1=0, loss_v2=0, nll_loss=1.061, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=2.09, wps=42.9, ups=0.81, wpb=52.9, bsz=16, num_updates=1440, lr=4.72093e-05, gnorm=6.8, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2370
2022-07-12 13:36:42 - progress_bar.py[line:274] - INFO: epoch 002:    172 / 1283 loss=1.299, loss_v1=0, loss_v2=0, nll_loss=1.006, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=2.01, wps=42.7, ups=0.82, wpb=52.2, bsz=16, num_updates=1450, lr=4.71754e-05, gnorm=6.352, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2382
2022-07-12 13:36:54 - progress_bar.py[line:274] - INFO: epoch 002:    182 / 1283 loss=1.398, loss_v1=0, loss_v2=0, nll_loss=1.099, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=2.14, wps=41.3, ups=0.82, wpb=50.7, bsz=16, num_updates=1460, lr=4.71416e-05, gnorm=10.66, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2394
2022-07-12 13:37:06 - progress_bar.py[line:274] - INFO: epoch 002:    192 / 1283 loss=1.322, loss_v1=0, loss_v2=0, nll_loss=1.029, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=2.04, wps=42.6, ups=0.81, wpb=52.3, bsz=16, num_updates=1470, lr=4.71078e-05, gnorm=5.947, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2407
2022-07-12 13:37:19 - progress_bar.py[line:274] - INFO: epoch 002:    202 / 1283 loss=1.251, loss_v1=0, loss_v2=0, nll_loss=0.96, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.95, wps=41.5, ups=0.81, wpb=50.9, bsz=16, num_updates=1480, lr=4.70739e-05, gnorm=6.75, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2419
2022-07-12 13:37:31 - progress_bar.py[line:274] - INFO: epoch 002:    212 / 1283 loss=1.402, loss_v1=0, loss_v2=0, nll_loss=1.108, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=2.16, wps=41.8, ups=0.81, wpb=51.3, bsz=16, num_updates=1490, lr=4.70401e-05, gnorm=6.509, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=2431
2022-07-12 13:37:43 - progress_bar.py[line:274] - INFO: epoch 002:    222 / 1283 loss=1.414, loss_v1=0, loss_v2=0, nll_loss=1.143, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=2.21, wps=41.9, ups=0.81, wpb=51.5, bsz=16, num_updates=1500, lr=4.70063e-05, gnorm=7.736, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2444
2022-07-12 13:37:56 - progress_bar.py[line:274] - INFO: epoch 002:    232 / 1283 loss=1.386, loss_v1=0, loss_v2=0, nll_loss=1.11, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=2.16, wps=43.1, ups=0.81, wpb=53.3, bsz=16, num_updates=1510, lr=4.69725e-05, gnorm=6.561, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2456
2022-07-12 13:38:08 - progress_bar.py[line:274] - INFO: epoch 002:    242 / 1283 loss=1.236, loss_v1=0, loss_v2=0, nll_loss=0.933, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.91, wps=41.3, ups=0.8, wpb=51.8, bsz=16, num_updates=1520, lr=4.69386e-05, gnorm=6.721, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=2468
2022-07-12 13:38:20 - progress_bar.py[line:274] - INFO: epoch 002:    252 / 1283 loss=1.229, loss_v1=0, loss_v2=0, nll_loss=0.911, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.88, wps=42.8, ups=0.82, wpb=52.2, bsz=16, num_updates=1530, lr=4.69048e-05, gnorm=8.864, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2481
2022-07-12 13:38:33 - progress_bar.py[line:274] - INFO: epoch 002:    262 / 1283 loss=1.325, loss_v1=0, loss_v2=0, nll_loss=1.024, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=2.03, wps=43.3, ups=0.82, wpb=53, bsz=16, num_updates=1540, lr=4.6871e-05, gnorm=7.525, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=2493
2022-07-12 13:38:45 - progress_bar.py[line:274] - INFO: epoch 002:    272 / 1283 loss=1.336, loss_v1=0, loss_v2=0, nll_loss=0.986, ntokens=47.8, nsentences=16, sample_size=47.8, sample_size_v1=0, sample_size_v2=0, ppl=1.98, wps=38.8, ups=0.81, wpb=47.8, bsz=16, num_updates=1550, lr=4.68372e-05, gnorm=7.395, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2505
2022-07-12 13:38:58 - progress_bar.py[line:274] - INFO: epoch 002:    282 / 1283 loss=1.39, loss_v1=0, loss_v2=0, nll_loss=1.109, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=2.16, wps=43.1, ups=0.8, wpb=54.1, bsz=16, num_updates=1560, lr=4.68033e-05, gnorm=7.037, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=2518
2022-07-12 13:39:10 - progress_bar.py[line:274] - INFO: epoch 002:    292 / 1283 loss=1.157, loss_v1=0, loss_v2=0, nll_loss=0.848, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=1570, lr=4.67695e-05, gnorm=7.661, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2530
2022-07-12 13:39:22 - progress_bar.py[line:274] - INFO: epoch 002:    302 / 1283 loss=1.327, loss_v1=0, loss_v2=0, nll_loss=1.019, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=2.03, wps=41.4, ups=0.81, wpb=51.3, bsz=16, num_updates=1580, lr=4.67357e-05, gnorm=7.406, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2543
2022-07-12 13:39:35 - progress_bar.py[line:274] - INFO: epoch 002:    312 / 1283 loss=1.285, loss_v1=0, loss_v2=0, nll_loss=0.988, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.98, wps=43.1, ups=0.81, wpb=53.1, bsz=16, num_updates=1590, lr=4.67018e-05, gnorm=7.365, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2555
2022-07-12 13:39:47 - progress_bar.py[line:274] - INFO: epoch 002:    322 / 1283 loss=1.227, loss_v1=0, loss_v2=0, nll_loss=0.948, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.93, wps=43.6, ups=0.8, wpb=54.4, bsz=16, num_updates=1600, lr=4.6668e-05, gnorm=6.912, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2567
2022-07-12 13:40:00 - progress_bar.py[line:274] - INFO: epoch 002:    332 / 1283 loss=1.365, loss_v1=0, loss_v2=0, nll_loss=1.052, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=2.07, wps=41.2, ups=0.8, wpb=51.6, bsz=16, num_updates=1610, lr=4.66342e-05, gnorm=9.546, clip=100, loss_scale=64, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=2580
2022-07-12 13:40:12 - progress_bar.py[line:274] - INFO: epoch 002:    342 / 1283 loss=1.44, loss_v1=0, loss_v2=0, nll_loss=1.137, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=2.2, wps=40.8, ups=0.81, wpb=50.6, bsz=16, num_updates=1620, lr=4.66004e-05, gnorm=8.137, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2592
2022-07-12 13:40:24 - progress_bar.py[line:274] - INFO: epoch 002:    352 / 1283 loss=1.343, loss_v1=0, loss_v2=0, nll_loss=1.012, ntokens=49.2, nsentences=16, sample_size=49.2, sample_size_v1=0, sample_size_v2=0, ppl=2.02, wps=40, ups=0.81, wpb=49.2, bsz=16, num_updates=1630, lr=4.65665e-05, gnorm=8.882, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=2605
2022-07-12 13:40:37 - progress_bar.py[line:274] - INFO: epoch 002:    362 / 1283 loss=1.217, loss_v1=0, loss_v2=0, nll_loss=0.904, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.87, wps=42.9, ups=0.82, wpb=52.4, bsz=16, num_updates=1640, lr=4.65327e-05, gnorm=7.698, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2617
2022-07-12 13:40:49 - progress_bar.py[line:274] - INFO: epoch 002:    372 / 1283 loss=1.256, loss_v1=0, loss_v2=0, nll_loss=0.93, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.9, wps=41, ups=0.81, wpb=50.4, bsz=16, num_updates=1650, lr=4.64989e-05, gnorm=7.507, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2629
2022-07-12 13:41:01 - progress_bar.py[line:274] - INFO: epoch 002:    382 / 1283 loss=1.239, loss_v1=0, loss_v2=0, nll_loss=0.914, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.88, wps=41.6, ups=0.8, wpb=51.7, bsz=16, num_updates=1660, lr=4.64651e-05, gnorm=8.454, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=2642
2022-07-12 13:41:14 - progress_bar.py[line:274] - INFO: epoch 002:    392 / 1283 loss=1.204, loss_v1=0, loss_v2=0, nll_loss=0.928, ntokens=54.9, nsentences=16, sample_size=54.9, sample_size_v1=0, sample_size_v2=0, ppl=1.9, wps=44.6, ups=0.81, wpb=54.9, bsz=16, num_updates=1670, lr=4.64312e-05, gnorm=7.267, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2654
2022-07-12 13:41:26 - progress_bar.py[line:274] - INFO: epoch 002:    402 / 1283 loss=1.228, loss_v1=0, loss_v2=0, nll_loss=0.93, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.9, wps=43.2, ups=0.81, wpb=53.3, bsz=16, num_updates=1680, lr=4.63974e-05, gnorm=8.943, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2666
2022-07-12 13:41:38 - progress_bar.py[line:274] - INFO: epoch 002:    412 / 1283 loss=1.279, loss_v1=0, loss_v2=0, nll_loss=0.967, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.95, wps=41.1, ups=0.8, wpb=51.4, bsz=16, num_updates=1690, lr=4.63636e-05, gnorm=7.885, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2679
2022-07-12 13:41:51 - progress_bar.py[line:274] - INFO: epoch 002:    422 / 1283 loss=1.235, loss_v1=0, loss_v2=0, nll_loss=0.896, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=40.2, ups=0.81, wpb=49.9, bsz=16, num_updates=1700, lr=4.63297e-05, gnorm=7.479, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2691
2022-07-12 13:42:03 - progress_bar.py[line:274] - INFO: epoch 002:    432 / 1283 loss=1.28, loss_v1=0, loss_v2=0, nll_loss=0.96, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.94, wps=42.2, ups=0.81, wpb=52, bsz=16, num_updates=1710, lr=4.62959e-05, gnorm=9.915, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=2703
2022-07-12 13:42:16 - progress_bar.py[line:274] - INFO: epoch 002:    442 / 1283 loss=1.204, loss_v1=0, loss_v2=0, nll_loss=0.877, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.84, wps=42, ups=0.81, wpb=52.1, bsz=16, num_updates=1720, lr=4.62621e-05, gnorm=7.715, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=2716
2022-07-12 13:42:28 - progress_bar.py[line:274] - INFO: epoch 002:    452 / 1283 loss=1.305, loss_v1=0, loss_v2=0, nll_loss=0.98, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.97, wps=40.5, ups=0.81, wpb=49.9, bsz=16, num_updates=1730, lr=4.62283e-05, gnorm=8.84, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=2728
2022-07-12 13:42:40 - progress_bar.py[line:274] - INFO: epoch 002:    462 / 1283 loss=1.243, loss_v1=0, loss_v2=0, nll_loss=0.921, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.89, wps=42, ups=0.8, wpb=52.3, bsz=16, num_updates=1740, lr=4.61944e-05, gnorm=6.759, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2741
2022-07-12 13:42:53 - progress_bar.py[line:274] - INFO: epoch 002:    472 / 1283 loss=1.188, loss_v1=0, loss_v2=0, nll_loss=0.856, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.81, wps=42.4, ups=0.81, wpb=52.2, bsz=16, num_updates=1750, lr=4.61606e-05, gnorm=9.055, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2753
2022-07-12 13:43:05 - progress_bar.py[line:274] - INFO: epoch 002:    482 / 1283 loss=1.285, loss_v1=0, loss_v2=0, nll_loss=0.977, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.97, wps=43.9, ups=0.82, wpb=53.7, bsz=16, num_updates=1760, lr=4.61268e-05, gnorm=8.088, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2765
2022-07-12 13:43:17 - progress_bar.py[line:274] - INFO: epoch 002:    492 / 1283 loss=1.227, loss_v1=0, loss_v2=0, nll_loss=0.915, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.89, wps=42.4, ups=0.8, wpb=52.8, bsz=16, num_updates=1770, lr=4.6093e-05, gnorm=7.246, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2778
2022-07-12 13:43:30 - progress_bar.py[line:274] - INFO: epoch 002:    502 / 1283 loss=1.303, loss_v1=0, loss_v2=0, nll_loss=0.986, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.98, wps=42, ups=0.81, wpb=52.1, bsz=16, num_updates=1780, lr=4.60591e-05, gnorm=10.224, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2790
2022-07-12 13:43:42 - progress_bar.py[line:274] - INFO: epoch 002:    512 / 1283 loss=1.236, loss_v1=0, loss_v2=0, nll_loss=0.941, ntokens=55.5, nsentences=16, sample_size=55.5, sample_size_v1=0, sample_size_v2=0, ppl=1.92, wps=45.4, ups=0.82, wpb=55.5, bsz=16, num_updates=1790, lr=4.60253e-05, gnorm=8.115, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2802
2022-07-12 13:43:54 - progress_bar.py[line:274] - INFO: epoch 002:    522 / 1283 loss=1.239, loss_v1=0, loss_v2=0, nll_loss=0.896, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=40, ups=0.8, wpb=50, bsz=16, num_updates=1800, lr=4.59915e-05, gnorm=8.644, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2815
2022-07-12 13:44:07 - progress_bar.py[line:274] - INFO: epoch 002:    532 / 1283 loss=1.191, loss_v1=0, loss_v2=0, nll_loss=0.882, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.84, wps=44.2, ups=0.81, wpb=54.4, bsz=16, num_updates=1810, lr=4.59576e-05, gnorm=8.457, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2827
2022-07-12 13:44:19 - progress_bar.py[line:274] - INFO: epoch 002:    542 / 1283 loss=1.251, loss_v1=0, loss_v2=0, nll_loss=0.948, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=1.93, wps=43.8, ups=0.81, wpb=54.2, bsz=16, num_updates=1820, lr=4.59238e-05, gnorm=9.045, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2839
2022-07-12 13:44:32 - progress_bar.py[line:274] - INFO: epoch 002:    552 / 1283 loss=1.246, loss_v1=0, loss_v2=0, nll_loss=0.933, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.91, wps=42.1, ups=0.8, wpb=52.7, bsz=16, num_updates=1830, lr=4.589e-05, gnorm=7.203, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2852
2022-07-12 13:44:44 - progress_bar.py[line:274] - INFO: epoch 002:    562 / 1283 loss=1.209, loss_v1=0, loss_v2=0, nll_loss=0.903, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.87, wps=41.9, ups=0.8, wpb=52.1, bsz=16, num_updates=1840, lr=4.58562e-05, gnorm=6.611, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2864
2022-07-12 13:44:57 - progress_bar.py[line:274] - INFO: epoch 002:    572 / 1283 loss=1.162, loss_v1=0, loss_v2=0, nll_loss=0.823, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=41.4, ups=0.8, wpb=51.6, bsz=16, num_updates=1850, lr=4.58223e-05, gnorm=6.493, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2877
2022-07-12 13:45:09 - progress_bar.py[line:274] - INFO: epoch 002:    582 / 1283 loss=1.215, loss_v1=0, loss_v2=0, nll_loss=0.861, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=41.9, ups=0.81, wpb=51.9, bsz=16, num_updates=1860, lr=4.57885e-05, gnorm=7.933, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2889
2022-07-12 13:45:22 - progress_bar.py[line:274] - INFO: epoch 002:    592 / 1283 loss=1.135, loss_v1=0, loss_v2=0, nll_loss=0.803, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=42.3, ups=0.79, wpb=53.5, bsz=16, num_updates=1870, lr=4.57547e-05, gnorm=7.143, clip=100, loss_scale=128, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=2902
2022-07-12 13:45:34 - progress_bar.py[line:274] - INFO: epoch 002:    602 / 1283 loss=1.228, loss_v1=0, loss_v2=0, nll_loss=0.911, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.88, wps=41.8, ups=0.82, wpb=50.9, bsz=16, num_updates=1880, lr=4.57209e-05, gnorm=10.458, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2914
2022-07-12 13:45:46 - progress_bar.py[line:274] - INFO: epoch 002:    612 / 1283 loss=1.277, loss_v1=0, loss_v2=0, nll_loss=0.971, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.96, wps=42.4, ups=0.81, wpb=52.4, bsz=16, num_updates=1890, lr=4.5687e-05, gnorm=7.854, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2926
2022-07-12 13:45:59 - progress_bar.py[line:274] - INFO: epoch 002:    622 / 1283 loss=1.169, loss_v1=0, loss_v2=0, nll_loss=0.851, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=42.4, ups=0.8, wpb=52.8, bsz=16, num_updates=1900, lr=4.56532e-05, gnorm=8.138, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2939
2022-07-12 13:46:11 - progress_bar.py[line:274] - INFO: epoch 002:    632 / 1283 loss=1.229, loss_v1=0, loss_v2=0, nll_loss=0.899, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=41.7, ups=0.81, wpb=51.4, bsz=16, num_updates=1910, lr=4.56194e-05, gnorm=9.029, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=2951
2022-07-12 13:46:23 - progress_bar.py[line:274] - INFO: epoch 002:    642 / 1283 loss=1.12, loss_v1=0, loss_v2=0, nll_loss=0.804, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=43.8, ups=0.81, wpb=54.4, bsz=16, num_updates=1920, lr=4.55855e-05, gnorm=7.496, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2964
2022-07-12 13:46:36 - progress_bar.py[line:274] - INFO: epoch 002:    652 / 1283 loss=1.132, loss_v1=0, loss_v2=0, nll_loss=0.81, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=43.8, ups=0.82, wpb=53.7, bsz=16, num_updates=1930, lr=4.55517e-05, gnorm=7.221, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2976
2022-07-12 13:46:48 - progress_bar.py[line:274] - INFO: epoch 002:    662 / 1283 loss=1.194, loss_v1=0, loss_v2=0, nll_loss=0.851, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=40.8, ups=0.81, wpb=50.3, bsz=16, num_updates=1940, lr=4.55179e-05, gnorm=8.676, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=2988
2022-07-12 13:47:00 - progress_bar.py[line:274] - INFO: epoch 002:    672 / 1283 loss=1.238, loss_v1=0, loss_v2=0, nll_loss=0.949, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.93, wps=42.2, ups=0.8, wpb=52.5, bsz=16, num_updates=1950, lr=4.54841e-05, gnorm=8.614, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3001
2022-07-12 13:47:13 - progress_bar.py[line:274] - INFO: epoch 002:    682 / 1283 loss=1.131, loss_v1=0, loss_v2=0, nll_loss=0.82, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=43.9, ups=0.81, wpb=54.4, bsz=16, num_updates=1960, lr=4.54502e-05, gnorm=8.253, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=3013
2022-07-12 13:47:25 - progress_bar.py[line:274] - INFO: epoch 002:    692 / 1283 loss=1.217, loss_v1=0, loss_v2=0, nll_loss=0.86, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=41, ups=0.81, wpb=50.6, bsz=16, num_updates=1970, lr=4.54164e-05, gnorm=9.244, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3025
2022-07-12 13:47:37 - progress_bar.py[line:274] - INFO: epoch 002:    702 / 1283 loss=1.262, loss_v1=0, loss_v2=0, nll_loss=0.917, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.89, wps=40.7, ups=0.81, wpb=50.1, bsz=16, num_updates=1980, lr=4.53826e-05, gnorm=7.247, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3038
2022-07-12 13:47:50 - progress_bar.py[line:274] - INFO: epoch 002:    712 / 1283 loss=1.199, loss_v1=0, loss_v2=0, nll_loss=0.864, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=41.5, ups=0.81, wpb=51.2, bsz=16, num_updates=1990, lr=4.53488e-05, gnorm=7.585, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3050
2022-07-12 13:48:02 - progress_bar.py[line:274] - INFO: epoch 002:    722 / 1283 loss=1.112, loss_v1=0, loss_v2=0, nll_loss=0.793, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=44.2, ups=0.82, wpb=54.1, bsz=16, num_updates=2000, lr=4.53149e-05, gnorm=7.523, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3062
2022-07-12 13:48:14 - progress_bar.py[line:274] - INFO: epoch 002:    732 / 1283 loss=1.188, loss_v1=0, loss_v2=0, nll_loss=0.868, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.83, wps=42.4, ups=0.81, wpb=52.5, bsz=16, num_updates=2010, lr=4.52811e-05, gnorm=6.875, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3075
2022-07-12 13:48:27 - progress_bar.py[line:274] - INFO: epoch 002:    742 / 1283 loss=1.214, loss_v1=0, loss_v2=0, nll_loss=0.898, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=42.6, ups=0.82, wpb=52.2, bsz=16, num_updates=2020, lr=4.52473e-05, gnorm=8.548, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=3087
2022-07-12 13:48:39 - progress_bar.py[line:274] - INFO: epoch 002:    752 / 1283 loss=1.151, loss_v1=0, loss_v2=0, nll_loss=0.835, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=43.5, ups=0.81, wpb=53.5, bsz=16, num_updates=2030, lr=4.52134e-05, gnorm=7.27, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3099
2022-07-12 13:48:51 - progress_bar.py[line:274] - INFO: epoch 002:    762 / 1283 loss=1.188, loss_v1=0, loss_v2=0, nll_loss=0.867, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=42.3, ups=0.81, wpb=51.9, bsz=16, num_updates=2040, lr=4.51796e-05, gnorm=6.386, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3111
2022-07-12 13:49:04 - progress_bar.py[line:274] - INFO: epoch 002:    772 / 1283 loss=1.32, loss_v1=0, loss_v2=0, nll_loss=1.001, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=2, wps=41.3, ups=0.8, wpb=51.5, bsz=16, num_updates=2050, lr=4.51458e-05, gnorm=8.069, clip=100, loss_scale=128, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=3124
2022-07-12 13:49:16 - progress_bar.py[line:274] - INFO: epoch 002:    782 / 1283 loss=1.16, loss_v1=0, loss_v2=0, nll_loss=0.851, ntokens=53.1, nsentences=15.9, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=42.9, ups=0.81, wpb=53.1, bsz=15.9, num_updates=2060, lr=4.5112e-05, gnorm=7.51, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3136
2022-07-12 13:49:28 - progress_bar.py[line:274] - INFO: epoch 002:    792 / 1283 loss=1.127, loss_v1=0, loss_v2=0, nll_loss=0.828, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=43.8, ups=0.81, wpb=54.1, bsz=16, num_updates=2070, lr=4.50781e-05, gnorm=6.523, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3149
2022-07-12 13:49:41 - progress_bar.py[line:274] - INFO: epoch 002:    802 / 1283 loss=1.143, loss_v1=0, loss_v2=0, nll_loss=0.784, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=40.6, ups=0.82, wpb=49.7, bsz=16, num_updates=2080, lr=4.50443e-05, gnorm=7.662, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3161
2022-07-12 13:49:53 - progress_bar.py[line:274] - INFO: epoch 002:    812 / 1283 loss=1.156, loss_v1=0, loss_v2=0, nll_loss=0.817, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=41.9, ups=0.8, wpb=52.5, bsz=16, num_updates=2090, lr=4.50105e-05, gnorm=7.523, clip=100, loss_scale=128, train_wall=12, gb_free=2, ema_decay=0.9999, wall=3173
2022-07-12 13:50:05 - progress_bar.py[line:274] - INFO: epoch 002:    822 / 1283 loss=1.216, loss_v1=0, loss_v2=0, nll_loss=0.876, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.84, wps=42.1, ups=0.81, wpb=52.1, bsz=16, num_updates=2100, lr=4.49767e-05, gnorm=9.457, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3186
2022-07-12 13:50:18 - progress_bar.py[line:274] - INFO: epoch 002:    832 / 1283 loss=1.196, loss_v1=0, loss_v2=0, nll_loss=0.852, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.81, wps=41.1, ups=0.81, wpb=51, bsz=16, num_updates=2110, lr=4.49428e-05, gnorm=7.57, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3198
2022-07-12 13:50:30 - progress_bar.py[line:274] - INFO: epoch 002:    842 / 1283 loss=1.136, loss_v1=0, loss_v2=0, nll_loss=0.797, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=41.2, ups=0.8, wpb=51.3, bsz=16, num_updates=2120, lr=4.4909e-05, gnorm=7.794, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3211
2022-07-12 13:50:43 - progress_bar.py[line:274] - INFO: epoch 002:    852 / 1283 loss=1.157, loss_v1=0, loss_v2=0, nll_loss=0.82, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=41.6, ups=0.81, wpb=51.6, bsz=16, num_updates=2130, lr=4.48752e-05, gnorm=7.228, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3223
2022-07-12 13:50:55 - progress_bar.py[line:274] - INFO: epoch 002:    862 / 1283 loss=1.24, loss_v1=0, loss_v2=0, nll_loss=0.884, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.85, wps=41.1, ups=0.8, wpb=51.1, bsz=16, num_updates=2140, lr=4.48414e-05, gnorm=7.135, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3235
2022-07-12 13:51:08 - progress_bar.py[line:274] - INFO: epoch 002:    872 / 1283 loss=1.203, loss_v1=0, loss_v2=0, nll_loss=0.867, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=41.2, ups=0.81, wpb=51.2, bsz=16, num_updates=2150, lr=4.48075e-05, gnorm=7.406, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=3248
2022-07-12 13:51:20 - progress_bar.py[line:274] - INFO: epoch 002:    882 / 1283 loss=1.182, loss_v1=0, loss_v2=0, nll_loss=0.867, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=42, ups=0.82, wpb=51.5, bsz=16, num_updates=2160, lr=4.47737e-05, gnorm=8.002, clip=100, loss_scale=128, train_wall=12, gb_free=2, ema_decay=0.9999, wall=3260
2022-07-12 13:51:32 - progress_bar.py[line:274] - INFO: epoch 002:    892 / 1283 loss=1.214, loss_v1=0, loss_v2=0, nll_loss=0.91, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.88, wps=42.6, ups=0.81, wpb=52.7, bsz=16, num_updates=2170, lr=4.47399e-05, gnorm=8.317, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3272
2022-07-12 13:51:45 - progress_bar.py[line:274] - INFO: epoch 002:    902 / 1283 loss=1.065, loss_v1=0, loss_v2=0, nll_loss=0.77, ntokens=56.4, nsentences=16, sample_size=56.4, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=45.4, ups=0.81, wpb=56.4, bsz=16, num_updates=2180, lr=4.4706e-05, gnorm=8.24, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=3285
2022-07-12 13:51:57 - progress_bar.py[line:274] - INFO: epoch 002:    912 / 1283 loss=1.253, loss_v1=0, loss_v2=0, nll_loss=0.918, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.89, wps=39.4, ups=0.79, wpb=49.6, bsz=16, num_updates=2190, lr=4.46722e-05, gnorm=8.171, clip=100, loss_scale=128, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=3297
2022-07-12 13:52:10 - progress_bar.py[line:274] - INFO: epoch 002:    922 / 1283 loss=1.129, loss_v1=0, loss_v2=0, nll_loss=0.767, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=41.4, ups=0.81, wpb=51.1, bsz=16, num_updates=2200, lr=4.46384e-05, gnorm=7.441, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=3310
2022-07-12 13:52:22 - progress_bar.py[line:274] - INFO: epoch 002:    932 / 1283 loss=1.218, loss_v1=0, loss_v2=0, nll_loss=0.893, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=41.6, ups=0.81, wpb=51.6, bsz=16, num_updates=2210, lr=4.46046e-05, gnorm=7.225, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3322
2022-07-12 13:52:34 - progress_bar.py[line:274] - INFO: epoch 002:    942 / 1283 loss=1.137, loss_v1=0, loss_v2=0, nll_loss=0.8, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=42, ups=0.81, wpb=52.1, bsz=16, num_updates=2220, lr=4.45707e-05, gnorm=6.532, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3335
2022-07-12 13:52:47 - progress_bar.py[line:274] - INFO: epoch 002:    952 / 1283 loss=1.165, loss_v1=0, loss_v2=0, nll_loss=0.839, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=43.3, ups=0.81, wpb=53.5, bsz=16, num_updates=2230, lr=4.45369e-05, gnorm=6.832, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3347
2022-07-12 13:52:59 - progress_bar.py[line:274] - INFO: epoch 002:    962 / 1283 loss=1.179, loss_v1=0, loss_v2=0, nll_loss=0.861, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=41.8, ups=0.8, wpb=52.1, bsz=16, num_updates=2240, lr=4.45031e-05, gnorm=8.008, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3359
2022-07-12 13:53:12 - progress_bar.py[line:274] - INFO: epoch 002:    972 / 1283 loss=1.156, loss_v1=0, loss_v2=0, nll_loss=0.821, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=41.1, ups=0.81, wpb=50.8, bsz=16, num_updates=2250, lr=4.44693e-05, gnorm=9.7, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3372
2022-07-12 13:53:24 - progress_bar.py[line:274] - INFO: epoch 002:    982 / 1283 loss=1.157, loss_v1=0, loss_v2=0, nll_loss=0.793, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=39.8, ups=0.8, wpb=49.5, bsz=16, num_updates=2260, lr=4.44354e-05, gnorm=7.012, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3384
2022-07-12 13:53:36 - progress_bar.py[line:274] - INFO: epoch 002:    992 / 1283 loss=1.269, loss_v1=0, loss_v2=0, nll_loss=0.937, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.91, wps=41.7, ups=0.81, wpb=51.7, bsz=16, num_updates=2270, lr=4.44016e-05, gnorm=7.757, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=3397
2022-07-12 13:53:49 - progress_bar.py[line:274] - INFO: epoch 002:   1002 / 1283 loss=1.121, loss_v1=0, loss_v2=0, nll_loss=0.797, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=41.9, ups=0.81, wpb=52, bsz=16, num_updates=2280, lr=4.43678e-05, gnorm=8.937, clip=100, loss_scale=256, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=3409
2022-07-12 13:54:01 - progress_bar.py[line:274] - INFO: epoch 002:   1012 / 1283 loss=1.253, loss_v1=0, loss_v2=0, nll_loss=0.893, ntokens=48.9, nsentences=16, sample_size=48.9, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=38.9, ups=0.79, wpb=48.9, bsz=16, num_updates=2290, lr=4.43339e-05, gnorm=8.863, clip=100, loss_scale=256, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=3422
2022-07-12 13:54:14 - progress_bar.py[line:274] - INFO: epoch 002:   1022 / 1283 loss=1.261, loss_v1=0, loss_v2=0, nll_loss=0.93, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.91, wps=41.7, ups=0.82, wpb=50.8, bsz=16, num_updates=2300, lr=4.43001e-05, gnorm=7.651, clip=100, loss_scale=256, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3434
2022-07-12 13:54:26 - progress_bar.py[line:274] - INFO: epoch 002:   1032 / 1283 loss=1.119, loss_v1=0, loss_v2=0, nll_loss=0.781, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=42.6, ups=0.8, wpb=53, bsz=16, num_updates=2310, lr=4.42663e-05, gnorm=6.877, clip=100, loss_scale=256, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3446
2022-07-12 13:54:38 - progress_bar.py[line:274] - INFO: epoch 002:   1042 / 1283 loss=1.126, loss_v1=0, loss_v2=0, nll_loss=0.794, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=42.2, ups=0.81, wpb=52.1, bsz=16, num_updates=2320, lr=4.42325e-05, gnorm=8.122, clip=100, loss_scale=256, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3459
2022-07-12 13:54:51 - progress_bar.py[line:274] - INFO: epoch 002:   1052 / 1283 loss=1.168, loss_v1=0, loss_v2=0, nll_loss=0.837, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=41.5, ups=0.8, wpb=51.7, bsz=16, num_updates=2330, lr=4.41986e-05, gnorm=7.087, clip=100, loss_scale=256, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3471
2022-07-12 13:55:03 - progress_bar.py[line:274] - INFO: epoch 002:   1062 / 1283 loss=1.171, loss_v1=0, loss_v2=0, nll_loss=0.86, ntokens=54.7, nsentences=16, sample_size=54.7, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=44.1, ups=0.81, wpb=54.7, bsz=16, num_updates=2340, lr=4.41648e-05, gnorm=8.38, clip=100, loss_scale=256, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3483
2022-07-12 13:55:16 - progress_bar.py[line:274] - INFO: epoch 002:   1072 / 1283 loss=1.232, loss_v1=0, loss_v2=0, nll_loss=0.89, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.85, wps=40.8, ups=0.81, wpb=50.3, bsz=16, num_updates=2350, lr=4.4131e-05, gnorm=9.479, clip=100, loss_scale=256, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3496
2022-07-12 13:55:28 - progress_bar.py[line:274] - INFO: epoch 002:   1082 / 1283 loss=1.183, loss_v1=0, loss_v2=0, nll_loss=0.859, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.81, wps=43.1, ups=0.81, wpb=53.4, bsz=16, num_updates=2360, lr=4.40972e-05, gnorm=8.07, clip=100, loss_scale=256, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3508
2022-07-12 13:55:40 - progress_bar.py[line:274] - INFO: epoch 002:   1092 / 1283 loss=1.123, loss_v1=0, loss_v2=0, nll_loss=0.776, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=41.8, ups=0.82, wpb=51, bsz=16, num_updates=2370, lr=4.40633e-05, gnorm=7.803, clip=100, loss_scale=256, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=3520
2022-07-12 13:55:53 - progress_bar.py[line:274] - INFO: epoch 002:   1102 / 1283 loss=1.141, loss_v1=0, loss_v2=0, nll_loss=0.793, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=40.6, ups=0.8, wpb=50.6, bsz=16, num_updates=2380, lr=4.40295e-05, gnorm=8.819, clip=100, loss_scale=256, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3533
2022-07-12 13:56:05 - progress_bar.py[line:274] - INFO: epoch 002:   1112 / 1283 loss=1.158, loss_v1=0, loss_v2=0, nll_loss=0.793, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=41.3, ups=0.8, wpb=51.3, bsz=16, num_updates=2390, lr=4.39957e-05, gnorm=8.129, clip=100, loss_scale=256, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3545
2022-07-12 13:56:18 - progress_bar.py[line:274] - INFO: epoch 002:   1122 / 1283 loss=1.097, loss_v1=0, loss_v2=0, nll_loss=0.779, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=42.1, ups=0.8, wpb=52.7, bsz=16, num_updates=2400, lr=4.39618e-05, gnorm=7.717, clip=100, loss_scale=256, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3558
2022-07-12 13:56:21 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-07-12 13:56:31 - progress_bar.py[line:274] - INFO: epoch 002:   1133 / 1283 loss=1.11, loss_v1=0, loss_v2=0, nll_loss=0.78, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=38.1, ups=0.75, wpb=51, bsz=16, num_updates=2410, lr=4.3928e-05, gnorm=7.906, clip=100, loss_scale=128, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=3571
2022-07-12 13:56:43 - progress_bar.py[line:274] - INFO: epoch 002:   1143 / 1283 loss=1.046, loss_v1=0, loss_v2=0, nll_loss=0.718, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=42.7, ups=0.8, wpb=53, bsz=16, num_updates=2420, lr=4.38942e-05, gnorm=10.27, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=3584
2022-07-12 13:56:56 - progress_bar.py[line:274] - INFO: epoch 002:   1153 / 1283 loss=1.231, loss_v1=0, loss_v2=0, nll_loss=0.917, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.89, wps=41, ups=0.8, wpb=51.1, bsz=16, num_updates=2430, lr=4.38604e-05, gnorm=8.748, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3596
2022-07-12 13:57:08 - progress_bar.py[line:274] - INFO: epoch 002:   1163 / 1283 loss=1.211, loss_v1=0, loss_v2=0, nll_loss=0.898, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=42.1, ups=0.8, wpb=52.3, bsz=16, num_updates=2440, lr=4.38265e-05, gnorm=8.569, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=3609
2022-07-12 13:57:21 - progress_bar.py[line:274] - INFO: epoch 002:   1173 / 1283 loss=1.001, loss_v1=0, loss_v2=0, nll_loss=0.655, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=42.7, ups=0.81, wpb=52.9, bsz=16, num_updates=2450, lr=4.37927e-05, gnorm=8.539, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3621
2022-07-12 13:57:33 - progress_bar.py[line:274] - INFO: epoch 002:   1183 / 1283 loss=1.15, loss_v1=0, loss_v2=0, nll_loss=0.79, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=41.6, ups=0.8, wpb=51.7, bsz=16, num_updates=2460, lr=4.37589e-05, gnorm=8.567, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=3633
2022-07-12 13:57:46 - progress_bar.py[line:274] - INFO: epoch 002:   1193 / 1283 loss=1.162, loss_v1=0, loss_v2=0, nll_loss=0.806, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=41.5, ups=0.8, wpb=52, bsz=16, num_updates=2470, lr=4.37251e-05, gnorm=9.474, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=3646
2022-07-12 13:57:58 - progress_bar.py[line:274] - INFO: epoch 002:   1203 / 1283 loss=1.095, loss_v1=0, loss_v2=0, nll_loss=0.762, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=41.8, ups=0.8, wpb=52, bsz=16, num_updates=2480, lr=4.36912e-05, gnorm=10.86, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3658
2022-07-12 13:58:10 - progress_bar.py[line:274] - INFO: epoch 002:   1213 / 1283 loss=1.144, loss_v1=0, loss_v2=0, nll_loss=0.795, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=40.7, ups=0.82, wpb=49.9, bsz=16, num_updates=2490, lr=4.36574e-05, gnorm=11.034, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3671
2022-07-12 13:58:23 - progress_bar.py[line:274] - INFO: epoch 002:   1223 / 1283 loss=1.218, loss_v1=0, loss_v2=0, nll_loss=0.9, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.87, wps=41.7, ups=0.81, wpb=51.8, bsz=16, num_updates=2500, lr=4.36236e-05, gnorm=8.766, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3683
2022-07-12 13:58:35 - progress_bar.py[line:274] - INFO: epoch 002:   1233 / 1283 loss=0.99, loss_v1=0, loss_v2=0, nll_loss=0.611, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=41.1, ups=0.81, wpb=50.9, bsz=16, num_updates=2510, lr=4.35897e-05, gnorm=6.754, clip=100, loss_scale=128, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=3695
2022-07-12 13:58:47 - progress_bar.py[line:274] - INFO: epoch 002:   1243 / 1283 loss=1.23, loss_v1=0, loss_v2=0, nll_loss=0.87, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.83, wps=40.9, ups=0.81, wpb=50.3, bsz=16, num_updates=2520, lr=4.35559e-05, gnorm=7.753, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3708
2022-07-12 13:59:00 - progress_bar.py[line:274] - INFO: epoch 002:   1253 / 1283 loss=1.036, loss_v1=0, loss_v2=0, nll_loss=0.695, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=42.3, ups=0.81, wpb=52.3, bsz=16, num_updates=2530, lr=4.35221e-05, gnorm=6.819, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=3720
2022-07-12 13:59:12 - progress_bar.py[line:274] - INFO: epoch 002:   1263 / 1283 loss=1.07, loss_v1=0, loss_v2=0, nll_loss=0.725, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=41.7, ups=0.79, wpb=52.8, bsz=16, num_updates=2540, lr=4.34883e-05, gnorm=7.72, clip=100, loss_scale=128, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=3733
2022-07-12 13:59:25 - progress_bar.py[line:274] - INFO: epoch 002:   1273 / 1283 loss=1.252, loss_v1=0, loss_v2=0, nll_loss=0.897, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=40, ups=0.8, wpb=50, bsz=16, num_updates=2550, lr=4.34544e-05, gnorm=7.675, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3745
2022-07-12 13:59:37 - progress_bar.py[line:274] - INFO: epoch 002:   1283 / 1283 loss=1.191, loss_v1=0, loss_v2=0, nll_loss=0.815, ntokens=47.5, nsentences=15.6, sample_size=47.5, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=39.3, ups=0.83, wpb=47.5, bsz=15.6, num_updates=2560, lr=4.34206e-05, gnorm=8.497, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=3757
2022-07-12 13:59:37 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 14:07:35 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 1.303 | loss_v1 0 | loss_v2 0 | nll_loss 1.008 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.01 | vqa_score 0.1396 | wps 30.7 | wpb 13.6 | bsz 4 | num_updates 2560 | best_vqa_score 0.1396
2022-07-12 14:07:35 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 2 @ 2560 updates
2022-07-12 14:07:35 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints2.pt
2022-07-12 14:08:06 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints2.pt
2022-07-12 14:09:02 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints2.pt (epoch 2 @ 2560 updates, score 0.1396) (writing took 86.903831247997 seconds)
2022-07-12 14:09:02 - train.py[line:323] - INFO: end of epoch 2 (average epoch stats below)
2022-07-12 14:09:02 - progress_bar.py[line:282] - INFO: epoch 002 | loss 1.234 | loss_v1 0 | loss_v2 0 | nll_loss 0.916 | ntokens 51.942 | nsentences 15.996 | sample_size 51.942 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.89 | wps 30.9 | ups 0.6 | wpb 51.9 | bsz 16 | num_updates 2560 | lr 4.34206e-05 | gnorm 7.845 | clip 100 | loss_scale 128 | train_wall 1582 | gb_free 2.3 | ema_decay 0.9999 | wall 4322
2022-07-12 14:09:02 - trainer.py[line:639] - INFO: loading train data for epoch 3
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 14:09:04 - trainer.py[line:703] - INFO: begin training epoch 3
2022-07-12 14:09:04 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 14:09:17 - progress_bar.py[line:274] - INFO: epoch 003:     10 / 1283 loss=1.175, loss_v1=0, loss_v2=0, nll_loss=0.833, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=0.9, ups=0.02, wpb=50.9, bsz=16, num_updates=2570, lr=4.33868e-05, gnorm=8.029, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=4337
2022-07-12 14:09:29 - progress_bar.py[line:274] - INFO: epoch 003:     20 / 1283 loss=1.058, loss_v1=0, loss_v2=0, nll_loss=0.705, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=41.1, ups=0.81, wpb=50.9, bsz=16, num_updates=2580, lr=4.3353e-05, gnorm=8.439, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=4349
2022-07-12 14:09:41 - progress_bar.py[line:274] - INFO: epoch 003:     30 / 1283 loss=1.096, loss_v1=0, loss_v2=0, nll_loss=0.751, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=42.6, ups=0.81, wpb=52.3, bsz=16, num_updates=2590, lr=4.33191e-05, gnorm=8.367, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4362
2022-07-12 14:09:54 - progress_bar.py[line:274] - INFO: epoch 003:     40 / 1283 loss=1.073, loss_v1=0, loss_v2=0, nll_loss=0.737, ntokens=54.5, nsentences=16, sample_size=54.5, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=44.7, ups=0.82, wpb=54.5, bsz=16, num_updates=2600, lr=4.32853e-05, gnorm=7.077, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4374
2022-07-12 14:10:06 - progress_bar.py[line:274] - INFO: epoch 003:     50 / 1283 loss=1.089, loss_v1=0, loss_v2=0, nll_loss=0.747, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=43.2, ups=0.81, wpb=53.1, bsz=16, num_updates=2610, lr=4.32515e-05, gnorm=8.639, clip=100, loss_scale=128, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=4386
2022-07-12 14:10:18 - progress_bar.py[line:274] - INFO: epoch 003:     60 / 1283 loss=1.056, loss_v1=0, loss_v2=0, nll_loss=0.686, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=40, ups=0.81, wpb=49.4, bsz=16, num_updates=2620, lr=4.32176e-05, gnorm=6.705, clip=100, loss_scale=128, train_wall=12, gb_free=2, ema_decay=0.9999, wall=4398
2022-07-12 14:10:31 - progress_bar.py[line:274] - INFO: epoch 003:     70 / 1283 loss=1.189, loss_v1=0, loss_v2=0, nll_loss=0.847, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=41.8, ups=0.81, wpb=51.5, bsz=16, num_updates=2630, lr=4.31838e-05, gnorm=7.28, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4411
2022-07-12 14:10:43 - progress_bar.py[line:274] - INFO: epoch 003:     80 / 1283 loss=1.015, loss_v1=0, loss_v2=0, nll_loss=0.66, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=43.5, ups=0.81, wpb=53.7, bsz=16, num_updates=2640, lr=4.315e-05, gnorm=6.492, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4423
2022-07-12 14:10:55 - progress_bar.py[line:274] - INFO: epoch 003:     90 / 1283 loss=1.218, loss_v1=0, loss_v2=0, nll_loss=0.893, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=42.8, ups=0.82, wpb=52.3, bsz=16, num_updates=2650, lr=4.31162e-05, gnorm=7.109, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4435
2022-07-12 14:11:07 - progress_bar.py[line:274] - INFO: epoch 003:    100 / 1283 loss=1.022, loss_v1=0, loss_v2=0, nll_loss=0.669, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=42, ups=0.81, wpb=51.8, bsz=16, num_updates=2660, lr=4.30823e-05, gnorm=6.785, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4448
2022-07-12 14:11:20 - progress_bar.py[line:274] - INFO: epoch 003:    110 / 1283 loss=1.213, loss_v1=0, loss_v2=0, nll_loss=0.858, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.81, wps=40.3, ups=0.8, wpb=50.3, bsz=16, num_updates=2670, lr=4.30485e-05, gnorm=8.215, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=4460
2022-07-12 14:11:32 - progress_bar.py[line:274] - INFO: epoch 003:    120 / 1283 loss=1.157, loss_v1=0, loss_v2=0, nll_loss=0.823, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=41.9, ups=0.81, wpb=51.8, bsz=16, num_updates=2680, lr=4.30147e-05, gnorm=6.481, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4473
2022-07-12 14:11:45 - progress_bar.py[line:274] - INFO: epoch 003:    130 / 1283 loss=1.185, loss_v1=0, loss_v2=0, nll_loss=0.834, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=41, ups=0.81, wpb=50.4, bsz=16, num_updates=2690, lr=4.29809e-05, gnorm=8.24, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4485
2022-07-12 14:11:57 - progress_bar.py[line:274] - INFO: epoch 003:    140 / 1283 loss=1.014, loss_v1=0, loss_v2=0, nll_loss=0.68, ntokens=54.9, nsentences=16, sample_size=54.9, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=44.8, ups=0.82, wpb=54.9, bsz=16, num_updates=2700, lr=4.2947e-05, gnorm=6.134, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4497
2022-07-12 14:12:09 - progress_bar.py[line:274] - INFO: epoch 003:    150 / 1283 loss=1.272, loss_v1=0, loss_v2=0, nll_loss=0.95, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.93, wps=43, ups=0.81, wpb=52.8, bsz=16, num_updates=2710, lr=4.29132e-05, gnorm=7.853, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4509
2022-07-12 14:12:18 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 14:12:23 - progress_bar.py[line:274] - INFO: epoch 003:    161 / 1283 loss=1.116, loss_v1=0, loss_v2=0, nll_loss=0.793, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=39.2, ups=0.74, wpb=52.9, bsz=16, num_updates=2720, lr=4.28794e-05, gnorm=6.509, clip=100, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=4523
2022-07-12 14:12:35 - progress_bar.py[line:274] - INFO: epoch 003:    171 / 1283 loss=1.062, loss_v1=0, loss_v2=0, nll_loss=0.707, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=42, ups=0.81, wpb=51.6, bsz=16, num_updates=2730, lr=4.28455e-05, gnorm=6.743, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4535
2022-07-12 14:12:47 - progress_bar.py[line:274] - INFO: epoch 003:    181 / 1283 loss=1.119, loss_v1=0, loss_v2=0, nll_loss=0.767, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=40.4, ups=0.8, wpb=50.3, bsz=16, num_updates=2740, lr=4.28117e-05, gnorm=7.259, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4548
2022-07-12 14:13:00 - progress_bar.py[line:274] - INFO: epoch 003:    191 / 1283 loss=1.056, loss_v1=0, loss_v2=0, nll_loss=0.71, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=42.1, ups=0.81, wpb=51.8, bsz=16, num_updates=2750, lr=4.27779e-05, gnorm=7.857, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4560
2022-07-12 14:13:12 - progress_bar.py[line:274] - INFO: epoch 003:    201 / 1283 loss=1.099, loss_v1=0, loss_v2=0, nll_loss=0.763, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=41.9, ups=0.81, wpb=51.9, bsz=16, num_updates=2760, lr=4.27441e-05, gnorm=6.907, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4572
2022-07-12 14:13:24 - progress_bar.py[line:274] - INFO: epoch 003:    211 / 1283 loss=1.099, loss_v1=0, loss_v2=0, nll_loss=0.746, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=41.7, ups=0.82, wpb=51.1, bsz=16, num_updates=2770, lr=4.27102e-05, gnorm=6.861, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4585
2022-07-12 14:13:37 - progress_bar.py[line:274] - INFO: epoch 003:    221 / 1283 loss=1.136, loss_v1=0, loss_v2=0, nll_loss=0.812, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=41.7, ups=0.81, wpb=51.7, bsz=16, num_updates=2780, lr=4.26764e-05, gnorm=7.21, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4597
2022-07-12 14:13:49 - progress_bar.py[line:274] - INFO: epoch 003:    231 / 1283 loss=1.063, loss_v1=0, loss_v2=0, nll_loss=0.724, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=43.2, ups=0.81, wpb=53.3, bsz=16, num_updates=2790, lr=4.26426e-05, gnorm=7.027, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=4609
2022-07-12 14:14:02 - progress_bar.py[line:274] - INFO: epoch 003:    241 / 1283 loss=1.025, loss_v1=0, loss_v2=0, nll_loss=0.664, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=40.9, ups=0.8, wpb=51.2, bsz=16, num_updates=2800, lr=4.26088e-05, gnorm=8.195, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4622
2022-07-12 14:14:14 - progress_bar.py[line:274] - INFO: epoch 003:    251 / 1283 loss=0.973, loss_v1=0, loss_v2=0, nll_loss=0.614, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=42.7, ups=0.81, wpb=52.4, bsz=16, num_updates=2810, lr=4.25749e-05, gnorm=7.935, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4634
2022-07-12 14:14:26 - progress_bar.py[line:274] - INFO: epoch 003:    261 / 1283 loss=1.056, loss_v1=0, loss_v2=0, nll_loss=0.693, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=43.4, ups=0.82, wpb=52.8, bsz=16, num_updates=2820, lr=4.25411e-05, gnorm=6.584, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4646
2022-07-12 14:14:38 - progress_bar.py[line:274] - INFO: epoch 003:    271 / 1283 loss=1.058, loss_v1=0, loss_v2=0, nll_loss=0.656, ntokens=48.2, nsentences=16, sample_size=48.2, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=39.2, ups=0.81, wpb=48.2, bsz=16, num_updates=2830, lr=4.25073e-05, gnorm=7.384, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4659
2022-07-12 14:14:51 - progress_bar.py[line:274] - INFO: epoch 003:    281 / 1283 loss=1.109, loss_v1=0, loss_v2=0, nll_loss=0.768, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=42.5, ups=0.79, wpb=53.6, bsz=16, num_updates=2840, lr=4.24734e-05, gnorm=7.643, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=4671
2022-07-12 14:15:03 - progress_bar.py[line:274] - INFO: epoch 003:    291 / 1283 loss=0.977, loss_v1=0, loss_v2=0, nll_loss=0.619, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=42, ups=0.81, wpb=52, bsz=16, num_updates=2850, lr=4.24396e-05, gnorm=8.535, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4684
2022-07-12 14:15:16 - progress_bar.py[line:274] - INFO: epoch 003:    301 / 1283 loss=1.073, loss_v1=0, loss_v2=0, nll_loss=0.72, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=42.2, ups=0.81, wpb=52.1, bsz=16, num_updates=2860, lr=4.24058e-05, gnorm=7.421, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4696
2022-07-12 14:15:28 - progress_bar.py[line:274] - INFO: epoch 003:    311 / 1283 loss=1.015, loss_v1=0, loss_v2=0, nll_loss=0.657, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=42.3, ups=0.8, wpb=52.7, bsz=16, num_updates=2870, lr=4.2372e-05, gnorm=6.586, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4708
2022-07-12 14:15:41 - progress_bar.py[line:274] - INFO: epoch 003:    321 / 1283 loss=1.055, loss_v1=0, loss_v2=0, nll_loss=0.728, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=42.8, ups=0.8, wpb=53.3, bsz=16, num_updates=2880, lr=4.23381e-05, gnorm=9.006, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4721
2022-07-12 14:15:53 - progress_bar.py[line:274] - INFO: epoch 003:    331 / 1283 loss=1.076, loss_v1=0, loss_v2=0, nll_loss=0.721, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=42.6, ups=0.81, wpb=52.5, bsz=16, num_updates=2890, lr=4.23043e-05, gnorm=7.993, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4733
2022-07-12 14:16:05 - progress_bar.py[line:274] - INFO: epoch 003:    341 / 1283 loss=1.125, loss_v1=0, loss_v2=0, nll_loss=0.774, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=41.6, ups=0.8, wpb=51.8, bsz=16, num_updates=2900, lr=4.22705e-05, gnorm=8.378, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4746
2022-07-12 14:16:18 - progress_bar.py[line:274] - INFO: epoch 003:    351 / 1283 loss=1.051, loss_v1=0, loss_v2=0, nll_loss=0.644, ntokens=48.2, nsentences=16, sample_size=48.2, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=39.2, ups=0.81, wpb=48.2, bsz=16, num_updates=2910, lr=4.22367e-05, gnorm=8.232, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4758
2022-07-12 14:16:30 - progress_bar.py[line:274] - INFO: epoch 003:    361 / 1283 loss=1.014, loss_v1=0, loss_v2=0, nll_loss=0.667, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=43.5, ups=0.82, wpb=53, bsz=16, num_updates=2920, lr=4.22028e-05, gnorm=6.68, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4770
2022-07-12 14:16:42 - progress_bar.py[line:274] - INFO: epoch 003:    371 / 1283 loss=1.024, loss_v1=0, loss_v2=0, nll_loss=0.652, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=39.9, ups=0.81, wpb=49.4, bsz=16, num_updates=2930, lr=4.2169e-05, gnorm=10.053, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4782
2022-07-12 14:16:55 - progress_bar.py[line:274] - INFO: epoch 003:    381 / 1283 loss=1.048, loss_v1=0, loss_v2=0, nll_loss=0.689, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=41.5, ups=0.8, wpb=51.8, bsz=16, num_updates=2940, lr=4.21352e-05, gnorm=8.651, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=4795
2022-07-12 14:17:07 - progress_bar.py[line:274] - INFO: epoch 003:    391 / 1283 loss=0.985, loss_v1=0, loss_v2=0, nll_loss=0.653, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=44.7, ups=0.81, wpb=55.1, bsz=16, num_updates=2950, lr=4.21013e-05, gnorm=7.763, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=4807
2022-07-12 14:17:19 - progress_bar.py[line:274] - INFO: epoch 003:    401 / 1283 loss=0.94, loss_v1=0, loss_v2=0, nll_loss=0.593, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=43.7, ups=0.81, wpb=53.7, bsz=16, num_updates=2960, lr=4.20675e-05, gnorm=7.853, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4820
2022-07-12 14:17:32 - progress_bar.py[line:274] - INFO: epoch 003:    411 / 1283 loss=1.012, loss_v1=0, loss_v2=0, nll_loss=0.64, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=41.5, ups=0.81, wpb=51.5, bsz=16, num_updates=2970, lr=4.20337e-05, gnorm=7.939, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4832
2022-07-12 14:17:44 - progress_bar.py[line:274] - INFO: epoch 003:    421 / 1283 loss=0.986, loss_v1=0, loss_v2=0, nll_loss=0.602, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=40.6, ups=0.81, wpb=50.2, bsz=16, num_updates=2980, lr=4.19999e-05, gnorm=7.413, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4844
2022-07-12 14:17:56 - progress_bar.py[line:274] - INFO: epoch 003:    431 / 1283 loss=1.06, loss_v1=0, loss_v2=0, nll_loss=0.693, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=41.8, ups=0.81, wpb=51.6, bsz=16, num_updates=2990, lr=4.1966e-05, gnorm=8.769, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4857
2022-07-12 14:18:09 - progress_bar.py[line:274] - INFO: epoch 003:    441 / 1283 loss=0.997, loss_v1=0, loss_v2=0, nll_loss=0.62, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=41.3, ups=0.8, wpb=51.5, bsz=16, num_updates=3000, lr=4.19322e-05, gnorm=7.365, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4869
2022-07-12 14:18:21 - progress_bar.py[line:274] - INFO: epoch 003:    451 / 1283 loss=1.067, loss_v1=0, loss_v2=0, nll_loss=0.709, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=40.8, ups=0.81, wpb=50.3, bsz=16, num_updates=3010, lr=4.18984e-05, gnorm=8.789, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=4881
2022-07-12 14:18:34 - progress_bar.py[line:274] - INFO: epoch 003:    461 / 1283 loss=1.005, loss_v1=0, loss_v2=0, nll_loss=0.633, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=42.2, ups=0.8, wpb=52.5, bsz=16, num_updates=3020, lr=4.18646e-05, gnorm=6.875, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4894
2022-07-12 14:18:46 - progress_bar.py[line:274] - INFO: epoch 003:    471 / 1283 loss=0.976, loss_v1=0, loss_v2=0, nll_loss=0.593, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=42.2, ups=0.81, wpb=51.9, bsz=16, num_updates=3030, lr=4.18307e-05, gnorm=7.909, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4906
2022-07-12 14:18:58 - progress_bar.py[line:274] - INFO: epoch 003:    481 / 1283 loss=1.08, loss_v1=0, loss_v2=0, nll_loss=0.733, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=44, ups=0.82, wpb=53.8, bsz=16, num_updates=3040, lr=4.17969e-05, gnorm=8.521, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=4918
2022-07-12 14:19:11 - progress_bar.py[line:274] - INFO: epoch 003:    491 / 1283 loss=1.011, loss_v1=0, loss_v2=0, nll_loss=0.659, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=42.4, ups=0.8, wpb=52.8, bsz=16, num_updates=3050, lr=4.17631e-05, gnorm=7.464, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4931
2022-07-12 14:19:23 - progress_bar.py[line:274] - INFO: epoch 003:    501 / 1283 loss=1.084, loss_v1=0, loss_v2=0, nll_loss=0.731, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=42.9, ups=0.82, wpb=52.5, bsz=16, num_updates=3060, lr=4.17292e-05, gnorm=8.874, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4943
2022-07-12 14:19:35 - progress_bar.py[line:274] - INFO: epoch 003:    511 / 1283 loss=1.04, loss_v1=0, loss_v2=0, nll_loss=0.693, ntokens=55.3, nsentences=16, sample_size=55.3, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=45, ups=0.81, wpb=55.3, bsz=16, num_updates=3070, lr=4.16954e-05, gnorm=8.529, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=4955
2022-07-12 14:19:48 - progress_bar.py[line:274] - INFO: epoch 003:    521 / 1283 loss=0.954, loss_v1=0, loss_v2=0, nll_loss=0.545, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=40, ups=0.8, wpb=49.8, bsz=16, num_updates=3080, lr=4.16616e-05, gnorm=8.016, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=4968
2022-07-12 14:20:00 - progress_bar.py[line:274] - INFO: epoch 003:    531 / 1283 loss=0.955, loss_v1=0, loss_v2=0, nll_loss=0.608, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=44.4, ups=0.82, wpb=54.4, bsz=16, num_updates=3090, lr=4.16278e-05, gnorm=9.87, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4980
2022-07-12 14:20:12 - progress_bar.py[line:274] - INFO: epoch 003:    541 / 1283 loss=1.04, loss_v1=0, loss_v2=0, nll_loss=0.69, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=43.1, ups=0.81, wpb=53.3, bsz=16, num_updates=3100, lr=4.15939e-05, gnorm=8.673, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=4992
2022-07-12 14:20:25 - progress_bar.py[line:274] - INFO: epoch 003:    551 / 1283 loss=1.019, loss_v1=0, loss_v2=0, nll_loss=0.666, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=43.3, ups=0.8, wpb=53.9, bsz=16, num_updates=3110, lr=4.15601e-05, gnorm=9.413, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5005
2022-07-12 14:20:37 - progress_bar.py[line:274] - INFO: epoch 003:    561 / 1283 loss=0.99, loss_v1=0, loss_v2=0, nll_loss=0.642, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=42.7, ups=0.81, wpb=52.8, bsz=16, num_updates=3120, lr=4.15263e-05, gnorm=7.41, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5017
2022-07-12 14:20:49 - progress_bar.py[line:274] - INFO: epoch 003:    571 / 1283 loss=0.944, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=40.3, ups=0.8, wpb=50.1, bsz=16, num_updates=3130, lr=4.14925e-05, gnorm=6.557, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=5030
2022-07-12 14:21:02 - progress_bar.py[line:274] - INFO: epoch 003:    581 / 1283 loss=0.935, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=42.8, ups=0.81, wpb=52.6, bsz=16, num_updates=3140, lr=4.14586e-05, gnorm=6.941, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5042
2022-07-12 14:21:14 - progress_bar.py[line:274] - INFO: epoch 003:    591 / 1283 loss=0.964, loss_v1=0, loss_v2=0, nll_loss=0.595, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=42.5, ups=0.79, wpb=53.5, bsz=16, num_updates=3150, lr=4.14248e-05, gnorm=6.816, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=5055
2022-07-12 14:21:27 - progress_bar.py[line:274] - INFO: epoch 003:    601 / 1283 loss=1.046, loss_v1=0, loss_v2=0, nll_loss=0.679, ntokens=50.2, nsentences=15.9, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=41, ups=0.82, wpb=50.2, bsz=15.9, num_updates=3160, lr=4.1391e-05, gnorm=9.693, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5067
2022-07-12 14:21:39 - progress_bar.py[line:274] - INFO: epoch 003:    611 / 1283 loss=1.001, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=42.7, ups=0.81, wpb=52.5, bsz=16, num_updates=3170, lr=4.13571e-05, gnorm=7.348, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=5079
2022-07-12 14:21:51 - progress_bar.py[line:274] - INFO: epoch 003:    621 / 1283 loss=0.96, loss_v1=0, loss_v2=0, nll_loss=0.606, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=42.7, ups=0.81, wpb=53, bsz=16, num_updates=3180, lr=4.13233e-05, gnorm=8.376, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5092
2022-07-12 14:22:04 - progress_bar.py[line:274] - INFO: epoch 003:    631 / 1283 loss=0.937, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=42, ups=0.82, wpb=51.5, bsz=16, num_updates=3190, lr=4.12895e-05, gnorm=12.248, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5104
2022-07-12 14:22:16 - progress_bar.py[line:274] - INFO: epoch 003:    641 / 1283 loss=0.988, loss_v1=0, loss_v2=0, nll_loss=0.629, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=43.3, ups=0.81, wpb=53.8, bsz=16, num_updates=3200, lr=4.12557e-05, gnorm=9.044, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5116
2022-07-12 14:22:28 - progress_bar.py[line:274] - INFO: epoch 003:    651 / 1283 loss=0.921, loss_v1=0, loss_v2=0, nll_loss=0.567, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=43.7, ups=0.81, wpb=53.7, bsz=16, num_updates=3210, lr=4.12218e-05, gnorm=7.648, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5128
2022-07-12 14:22:41 - progress_bar.py[line:274] - INFO: epoch 003:    661 / 1283 loss=1.043, loss_v1=0, loss_v2=0, nll_loss=0.654, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=40.6, ups=0.81, wpb=50, bsz=16, num_updates=3220, lr=4.1188e-05, gnorm=8.459, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5141
2022-07-12 14:22:53 - progress_bar.py[line:274] - INFO: epoch 003:    671 / 1283 loss=1.007, loss_v1=0, loss_v2=0, nll_loss=0.679, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=43.1, ups=0.81, wpb=53.4, bsz=16, num_updates=3230, lr=4.11542e-05, gnorm=7.875, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=5153
2022-07-12 14:23:05 - progress_bar.py[line:274] - INFO: epoch 003:    681 / 1283 loss=0.893, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=43.8, ups=0.81, wpb=53.9, bsz=16, num_updates=3240, lr=4.11204e-05, gnorm=6.518, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5165
2022-07-12 14:23:18 - progress_bar.py[line:274] - INFO: epoch 003:    691 / 1283 loss=1.034, loss_v1=0, loss_v2=0, nll_loss=0.656, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=41.4, ups=0.81, wpb=51.2, bsz=16, num_updates=3250, lr=4.10865e-05, gnorm=8.332, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5178
2022-07-12 14:23:30 - progress_bar.py[line:274] - INFO: epoch 003:    701 / 1283 loss=1.028, loss_v1=0, loss_v2=0, nll_loss=0.625, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=40.6, ups=0.82, wpb=49.7, bsz=16, num_updates=3260, lr=4.10527e-05, gnorm=7.963, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5190
2022-07-12 14:23:42 - progress_bar.py[line:274] - INFO: epoch 003:    711 / 1283 loss=0.937, loss_v1=0, loss_v2=0, nll_loss=0.558, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=41.9, ups=0.82, wpb=51.3, bsz=16, num_updates=3270, lr=4.10189e-05, gnorm=6.606, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=5202
2022-07-12 14:23:54 - progress_bar.py[line:274] - INFO: epoch 003:    721 / 1283 loss=0.958, loss_v1=0, loss_v2=0, nll_loss=0.601, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=44.2, ups=0.82, wpb=54.2, bsz=16, num_updates=3280, lr=4.0985e-05, gnorm=6.969, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5215
2022-07-12 14:24:07 - progress_bar.py[line:274] - INFO: epoch 003:    731 / 1283 loss=0.955, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=42.8, ups=0.81, wpb=52.7, bsz=16, num_updates=3290, lr=4.09512e-05, gnorm=7.137, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5227
2022-07-12 14:24:18 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 14:24:20 - progress_bar.py[line:274] - INFO: epoch 003:    742 / 1283 loss=1.023, loss_v1=0, loss_v2=0, nll_loss=0.658, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=38.8, ups=0.75, wpb=51.8, bsz=16, num_updates=3300, lr=4.09174e-05, gnorm=8.413, clip=100, loss_scale=64, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=5240
2022-07-12 14:24:32 - progress_bar.py[line:274] - INFO: epoch 003:    752 / 1283 loss=0.917, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=43.7, ups=0.81, wpb=53.7, bsz=16, num_updates=3310, lr=4.08836e-05, gnorm=6.319, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5253
2022-07-12 14:24:45 - progress_bar.py[line:274] - INFO: epoch 003:    762 / 1283 loss=1.005, loss_v1=0, loss_v2=0, nll_loss=0.645, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=42.1, ups=0.81, wpb=51.9, bsz=16, num_updates=3320, lr=4.08497e-05, gnorm=8.192, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5265
2022-07-12 14:24:57 - progress_bar.py[line:274] - INFO: epoch 003:    772 / 1283 loss=1.084, loss_v1=0, loss_v2=0, nll_loss=0.718, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=41.3, ups=0.8, wpb=51.4, bsz=16, num_updates=3330, lr=4.08159e-05, gnorm=8.551, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=5277
2022-07-12 14:25:09 - progress_bar.py[line:274] - INFO: epoch 003:    782 / 1283 loss=0.939, loss_v1=0, loss_v2=0, nll_loss=0.586, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=43.1, ups=0.81, wpb=53.4, bsz=16, num_updates=3340, lr=4.07821e-05, gnorm=7.218, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5290
2022-07-12 14:25:22 - progress_bar.py[line:274] - INFO: epoch 003:    792 / 1283 loss=0.889, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=43.8, ups=0.81, wpb=54.1, bsz=16, num_updates=3350, lr=4.07483e-05, gnorm=6.58, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5302
2022-07-12 14:25:34 - progress_bar.py[line:274] - INFO: epoch 003:    802 / 1283 loss=0.95, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=40.7, ups=0.82, wpb=49.7, bsz=16, num_updates=3360, lr=4.07144e-05, gnorm=6.441, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5314
2022-07-12 14:25:47 - progress_bar.py[line:274] - INFO: epoch 003:    812 / 1283 loss=0.926, loss_v1=0, loss_v2=0, nll_loss=0.56, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=42, ups=0.8, wpb=52.5, bsz=16, num_updates=3370, lr=4.06806e-05, gnorm=7.534, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=5327
2022-07-12 14:25:59 - progress_bar.py[line:274] - INFO: epoch 003:    822 / 1283 loss=1.043, loss_v1=0, loss_v2=0, nll_loss=0.673, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=42.2, ups=0.81, wpb=52.1, bsz=16, num_updates=3380, lr=4.06468e-05, gnorm=8.043, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5339
2022-07-12 14:26:11 - progress_bar.py[line:274] - INFO: epoch 003:    832 / 1283 loss=0.953, loss_v1=0, loss_v2=0, nll_loss=0.563, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=41.1, ups=0.81, wpb=51, bsz=16, num_updates=3390, lr=4.06129e-05, gnorm=6.401, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5352
2022-07-12 14:26:24 - progress_bar.py[line:274] - INFO: epoch 003:    842 / 1283 loss=0.958, loss_v1=0, loss_v2=0, nll_loss=0.566, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=40.9, ups=0.8, wpb=51.3, bsz=16, num_updates=3400, lr=4.05791e-05, gnorm=7.803, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5364
2022-07-12 14:26:36 - progress_bar.py[line:274] - INFO: epoch 003:    852 / 1283 loss=0.971, loss_v1=0, loss_v2=0, nll_loss=0.597, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=42.1, ups=0.82, wpb=51.6, bsz=16, num_updates=3410, lr=4.05453e-05, gnorm=7.469, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5376
2022-07-12 14:26:48 - progress_bar.py[line:274] - INFO: epoch 003:    862 / 1283 loss=1.027, loss_v1=0, loss_v2=0, nll_loss=0.641, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=41.2, ups=0.81, wpb=51.1, bsz=16, num_updates=3420, lr=4.05115e-05, gnorm=7.376, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5389
2022-07-12 14:27:01 - progress_bar.py[line:274] - INFO: epoch 003:    872 / 1283 loss=1.019, loss_v1=0, loss_v2=0, nll_loss=0.638, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=41.5, ups=0.81, wpb=51.2, bsz=16, num_updates=3430, lr=4.04776e-05, gnorm=7.65, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=5401
2022-07-12 14:27:13 - progress_bar.py[line:274] - INFO: epoch 003:    882 / 1283 loss=0.946, loss_v1=0, loss_v2=0, nll_loss=0.584, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=42, ups=0.81, wpb=51.5, bsz=16, num_updates=3440, lr=4.04438e-05, gnorm=7.143, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=5413
2022-07-12 14:27:25 - progress_bar.py[line:274] - INFO: epoch 003:    892 / 1283 loss=1.018, loss_v1=0, loss_v2=0, nll_loss=0.679, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=42.5, ups=0.81, wpb=52.7, bsz=16, num_updates=3450, lr=4.041e-05, gnorm=9.65, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5426
2022-07-12 14:27:38 - progress_bar.py[line:274] - INFO: epoch 003:    902 / 1283 loss=0.872, loss_v1=0, loss_v2=0, nll_loss=0.532, ntokens=56.4, nsentences=16, sample_size=56.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=45.7, ups=0.81, wpb=56.4, bsz=16, num_updates=3460, lr=4.03762e-05, gnorm=7.218, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=5438
2022-07-12 14:27:50 - progress_bar.py[line:274] - INFO: epoch 003:    912 / 1283 loss=1.047, loss_v1=0, loss_v2=0, nll_loss=0.659, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=39.3, ups=0.79, wpb=49.6, bsz=16, num_updates=3470, lr=4.03423e-05, gnorm=9.788, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=5451
2022-07-12 14:28:03 - progress_bar.py[line:274] - INFO: epoch 003:    922 / 1283 loss=0.903, loss_v1=0, loss_v2=0, nll_loss=0.505, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=41.6, ups=0.81, wpb=51.1, bsz=16, num_updates=3480, lr=4.03085e-05, gnorm=6.73, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=5463
2022-07-12 14:28:15 - progress_bar.py[line:274] - INFO: epoch 003:    932 / 1283 loss=1.018, loss_v1=0, loss_v2=0, nll_loss=0.652, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=41.8, ups=0.81, wpb=51.6, bsz=16, num_updates=3490, lr=4.02747e-05, gnorm=7.558, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5475
2022-07-12 14:28:27 - progress_bar.py[line:274] - INFO: epoch 003:    942 / 1283 loss=0.923, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=42.4, ups=0.81, wpb=52.1, bsz=16, num_updates=3500, lr=4.02408e-05, gnorm=6.322, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5488
2022-07-12 14:28:40 - progress_bar.py[line:274] - INFO: epoch 003:    952 / 1283 loss=0.922, loss_v1=0, loss_v2=0, nll_loss=0.55, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=43.3, ups=0.81, wpb=53.5, bsz=16, num_updates=3510, lr=4.0207e-05, gnorm=6.481, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5500
2022-07-12 14:28:52 - progress_bar.py[line:274] - INFO: epoch 003:    962 / 1283 loss=0.96, loss_v1=0, loss_v2=0, nll_loss=0.597, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=42, ups=0.81, wpb=52.1, bsz=16, num_updates=3520, lr=4.01732e-05, gnorm=7.447, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5512
2022-07-12 14:29:04 - progress_bar.py[line:274] - INFO: epoch 003:    972 / 1283 loss=0.949, loss_v1=0, loss_v2=0, nll_loss=0.564, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=41.5, ups=0.82, wpb=50.8, bsz=16, num_updates=3530, lr=4.01394e-05, gnorm=10.293, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5525
2022-07-12 14:29:17 - progress_bar.py[line:274] - INFO: epoch 003:    982 / 1283 loss=0.993, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=39.7, ups=0.8, wpb=49.5, bsz=16, num_updates=3540, lr=4.01055e-05, gnorm=9.392, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5537
2022-07-12 14:29:29 - progress_bar.py[line:274] - INFO: epoch 003:    992 / 1283 loss=1.039, loss_v1=0, loss_v2=0, nll_loss=0.672, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=41.9, ups=0.81, wpb=51.7, bsz=16, num_updates=3550, lr=4.00717e-05, gnorm=8.646, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=5549
2022-07-12 14:29:42 - progress_bar.py[line:274] - INFO: epoch 003:   1002 / 1283 loss=0.941, loss_v1=0, loss_v2=0, nll_loss=0.578, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=42.1, ups=0.81, wpb=52, bsz=16, num_updates=3560, lr=4.00379e-05, gnorm=8.181, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=5562
2022-07-12 14:29:54 - progress_bar.py[line:274] - INFO: epoch 003:   1012 / 1283 loss=1.007, loss_v1=0, loss_v2=0, nll_loss=0.594, ntokens=48.9, nsentences=16, sample_size=48.9, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=39, ups=0.8, wpb=48.9, bsz=16, num_updates=3570, lr=4.00041e-05, gnorm=8.958, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5574
2022-07-12 14:30:06 - progress_bar.py[line:274] - INFO: epoch 003:   1022 / 1283 loss=0.974, loss_v1=0, loss_v2=0, nll_loss=0.582, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=41.7, ups=0.82, wpb=50.8, bsz=16, num_updates=3580, lr=3.99702e-05, gnorm=7.059, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5586
2022-07-12 14:30:19 - progress_bar.py[line:274] - INFO: epoch 003:   1032 / 1283 loss=0.972, loss_v1=0, loss_v2=0, nll_loss=0.598, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=42.8, ups=0.81, wpb=53, bsz=16, num_updates=3590, lr=3.99364e-05, gnorm=8.166, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5599
2022-07-12 14:30:31 - progress_bar.py[line:274] - INFO: epoch 003:   1042 / 1283 loss=0.938, loss_v1=0, loss_v2=0, nll_loss=0.568, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=42.3, ups=0.81, wpb=52.1, bsz=16, num_updates=3600, lr=3.99026e-05, gnorm=7.292, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5611
2022-07-12 14:30:43 - progress_bar.py[line:274] - INFO: epoch 003:   1052 / 1283 loss=0.961, loss_v1=0, loss_v2=0, nll_loss=0.58, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=41.4, ups=0.8, wpb=51.7, bsz=16, num_updates=3610, lr=3.98688e-05, gnorm=6.494, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5624
2022-07-12 14:30:56 - progress_bar.py[line:274] - INFO: epoch 003:   1062 / 1283 loss=0.995, loss_v1=0, loss_v2=0, nll_loss=0.641, ntokens=54.7, nsentences=16, sample_size=54.7, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=44.2, ups=0.81, wpb=54.7, bsz=16, num_updates=3620, lr=3.98349e-05, gnorm=6.979, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5636
2022-07-12 14:31:08 - progress_bar.py[line:274] - INFO: epoch 003:   1072 / 1283 loss=1.007, loss_v1=0, loss_v2=0, nll_loss=0.633, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=40.8, ups=0.81, wpb=50.3, bsz=16, num_updates=3630, lr=3.98011e-05, gnorm=11.121, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5648
2022-07-12 14:31:21 - progress_bar.py[line:274] - INFO: epoch 003:   1082 / 1283 loss=1.012, loss_v1=0, loss_v2=0, nll_loss=0.642, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=43.2, ups=0.81, wpb=53.4, bsz=16, num_updates=3640, lr=3.97673e-05, gnorm=8.1, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5661
2022-07-12 14:31:33 - progress_bar.py[line:274] - INFO: epoch 003:   1092 / 1283 loss=0.96, loss_v1=0, loss_v2=0, nll_loss=0.582, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=41.8, ups=0.82, wpb=51, bsz=16, num_updates=3650, lr=3.97334e-05, gnorm=8.828, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=5673
2022-07-12 14:31:45 - progress_bar.py[line:274] - INFO: epoch 003:   1102 / 1283 loss=0.953, loss_v1=0, loss_v2=0, nll_loss=0.572, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=40.8, ups=0.81, wpb=50.6, bsz=16, num_updates=3660, lr=3.96996e-05, gnorm=8.152, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5685
2022-07-12 14:31:57 - progress_bar.py[line:274] - INFO: epoch 003:   1112 / 1283 loss=0.939, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=41.5, ups=0.81, wpb=51.3, bsz=16, num_updates=3670, lr=3.96658e-05, gnorm=8.055, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5698
2022-07-12 14:32:10 - progress_bar.py[line:274] - INFO: epoch 003:   1122 / 1283 loss=0.897, loss_v1=0, loss_v2=0, nll_loss=0.534, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=42.6, ups=0.81, wpb=52.7, bsz=16, num_updates=3680, lr=3.9632e-05, gnorm=8.469, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5710
2022-07-12 14:32:22 - progress_bar.py[line:274] - INFO: epoch 003:   1132 / 1283 loss=1.001, loss_v1=0, loss_v2=0, nll_loss=0.637, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=40.9, ups=0.81, wpb=50.5, bsz=16, num_updates=3690, lr=3.95981e-05, gnorm=9.374, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=5722
2022-07-12 14:32:35 - progress_bar.py[line:274] - INFO: epoch 003:   1142 / 1283 loss=0.871, loss_v1=0, loss_v2=0, nll_loss=0.512, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=43.1, ups=0.81, wpb=53.2, bsz=16, num_updates=3700, lr=3.95643e-05, gnorm=7.589, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5735
2022-07-12 14:32:47 - progress_bar.py[line:274] - INFO: epoch 003:   1152 / 1283 loss=1.024, loss_v1=0, loss_v2=0, nll_loss=0.672, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=41.2, ups=0.81, wpb=51.2, bsz=16, num_updates=3710, lr=3.95305e-05, gnorm=7.52, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5747
2022-07-12 14:32:59 - progress_bar.py[line:274] - INFO: epoch 003:   1162 / 1283 loss=1.015, loss_v1=0, loss_v2=0, nll_loss=0.665, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=42.9, ups=0.81, wpb=52.9, bsz=16, num_updates=3720, lr=3.94967e-05, gnorm=9.023, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5760
2022-07-12 14:33:03 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-12 14:33:13 - progress_bar.py[line:274] - INFO: epoch 003:   1173 / 1283 loss=0.811, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=38.8, ups=0.74, wpb=52.3, bsz=16, num_updates=3730, lr=3.94628e-05, gnorm=7.05, clip=100, loss_scale=32, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=5773
2022-07-12 14:33:25 - progress_bar.py[line:274] - INFO: epoch 003:   1183 / 1283 loss=0.947, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=41.7, ups=0.81, wpb=51.7, bsz=16, num_updates=3740, lr=3.9429e-05, gnorm=7.869, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5785
2022-07-12 14:33:38 - progress_bar.py[line:274] - INFO: epoch 003:   1193 / 1283 loss=0.954, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=41.6, ups=0.8, wpb=52, bsz=16, num_updates=3750, lr=3.93952e-05, gnorm=7.897, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5798
2022-07-12 14:33:50 - progress_bar.py[line:274] - INFO: epoch 003:   1203 / 1283 loss=0.928, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=42, ups=0.81, wpb=52, bsz=16, num_updates=3760, lr=3.93613e-05, gnorm=8.596, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5810
2022-07-12 14:34:02 - progress_bar.py[line:274] - INFO: epoch 003:   1213 / 1283 loss=1.003, loss_v1=0, loss_v2=0, nll_loss=0.619, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=40.9, ups=0.82, wpb=49.9, bsz=16, num_updates=3770, lr=3.93275e-05, gnorm=8.322, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5822
2022-07-12 14:34:15 - progress_bar.py[line:274] - INFO: epoch 003:   1223 / 1283 loss=0.996, loss_v1=0, loss_v2=0, nll_loss=0.636, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=41.9, ups=0.81, wpb=51.8, bsz=16, num_updates=3780, lr=3.92937e-05, gnorm=7.868, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5835
2022-07-12 14:34:27 - progress_bar.py[line:274] - INFO: epoch 003:   1233 / 1283 loss=0.827, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=41.5, ups=0.82, wpb=50.9, bsz=16, num_updates=3790, lr=3.92599e-05, gnorm=6.597, clip=100, loss_scale=32, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=5847
2022-07-12 14:34:39 - progress_bar.py[line:274] - INFO: epoch 003:   1243 / 1283 loss=0.989, loss_v1=0, loss_v2=0, nll_loss=0.588, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=40.8, ups=0.81, wpb=50.3, bsz=16, num_updates=3800, lr=3.9226e-05, gnorm=7.288, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5859
2022-07-12 14:34:52 - progress_bar.py[line:274] - INFO: epoch 003:   1253 / 1283 loss=0.871, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=42.3, ups=0.81, wpb=52.3, bsz=16, num_updates=3810, lr=3.91922e-05, gnorm=8.082, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=5872
2022-07-12 14:35:04 - progress_bar.py[line:274] - INFO: epoch 003:   1263 / 1283 loss=0.874, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=41.9, ups=0.79, wpb=52.8, bsz=16, num_updates=3820, lr=3.91584e-05, gnorm=7.29, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=5884
2022-07-12 14:35:17 - progress_bar.py[line:274] - INFO: epoch 003:   1273 / 1283 loss=0.96, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=40, ups=0.8, wpb=50, bsz=16, num_updates=3830, lr=3.91246e-05, gnorm=7.906, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5897
2022-07-12 14:35:29 - progress_bar.py[line:274] - INFO: epoch 003:   1283 / 1283 loss=0.973, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=47.5, nsentences=15.6, sample_size=47.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=39.3, ups=0.83, wpb=47.5, bsz=15.6, num_updates=3840, lr=3.90907e-05, gnorm=8.882, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=5909
2022-07-12 14:35:29 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 14:43:27 - progress_bar.py[line:282] - INFO: epoch 003 | valid on 'valid' subset | loss 1.357 | loss_v1 0 | loss_v2 0 | nll_loss 1.056 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.08 | vqa_score 0.1954 | wps 30.7 | wpb 13.6 | bsz 4 | num_updates 3840 | best_vqa_score 0.1954
2022-07-12 14:43:27 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 3 @ 3840 updates
2022-07-12 14:43:27 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints3.pt
2022-07-12 14:43:58 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints3.pt
2022-07-12 14:44:54 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints3.pt (epoch 3 @ 3840 updates, score 0.1954) (writing took 87.42742522998014 seconds)
2022-07-12 14:44:54 - train.py[line:323] - INFO: end of epoch 3 (average epoch stats below)
2022-07-12 14:44:54 - progress_bar.py[line:282] - INFO: epoch 003 | loss 1.007 | loss_v1 0 | loss_v2 0 | nll_loss 0.641 | ntokens 51.934 | nsentences 15.996 | sample_size 51.934 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.56 | wps 30.9 | ups 0.59 | wpb 51.9 | bsz 16 | num_updates 3840 | lr 3.90907e-05 | gnorm 7.837 | clip 100 | loss_scale 32 | train_wall 1580 | gb_free 2.3 | ema_decay 0.9999 | wall 6474
2022-07-12 14:44:54 - trainer.py[line:639] - INFO: loading train data for epoch 4
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 14:44:56 - trainer.py[line:703] - INFO: begin training epoch 4
2022-07-12 14:44:56 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 14:45:09 - progress_bar.py[line:274] - INFO: epoch 004:     10 / 1283 loss=0.969, loss_v1=0, loss_v2=0, nll_loss=0.583, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=0.9, ups=0.02, wpb=50.9, bsz=16, num_updates=3850, lr=3.90569e-05, gnorm=9.349, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=6489
2022-07-12 14:45:22 - progress_bar.py[line:274] - INFO: epoch 004:     20 / 1283 loss=0.888, loss_v1=0, loss_v2=0, nll_loss=0.501, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=40.8, ups=0.8, wpb=50.9, bsz=16, num_updates=3860, lr=3.90231e-05, gnorm=6.849, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=6502
2022-07-12 14:45:34 - progress_bar.py[line:274] - INFO: epoch 004:     30 / 1283 loss=0.912, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=42.7, ups=0.82, wpb=52.3, bsz=16, num_updates=3870, lr=3.89892e-05, gnorm=7.539, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6514
2022-07-12 14:45:46 - progress_bar.py[line:274] - INFO: epoch 004:     40 / 1283 loss=0.924, loss_v1=0, loss_v2=0, nll_loss=0.567, ntokens=54.5, nsentences=16, sample_size=54.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=44.7, ups=0.82, wpb=54.5, bsz=16, num_updates=3880, lr=3.89554e-05, gnorm=7.148, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6526
2022-07-12 14:45:58 - progress_bar.py[line:274] - INFO: epoch 004:     50 / 1283 loss=0.905, loss_v1=0, loss_v2=0, nll_loss=0.525, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=43.2, ups=0.81, wpb=53.1, bsz=16, num_updates=3890, lr=3.89216e-05, gnorm=7.061, clip=100, loss_scale=32, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=6539
2022-07-12 14:46:11 - progress_bar.py[line:274] - INFO: epoch 004:     60 / 1283 loss=0.885, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=40.1, ups=0.81, wpb=49.4, bsz=16, num_updates=3900, lr=3.88878e-05, gnorm=7.06, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=6551
2022-07-12 14:46:23 - progress_bar.py[line:274] - INFO: epoch 004:     70 / 1283 loss=0.955, loss_v1=0, loss_v2=0, nll_loss=0.563, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=41.8, ups=0.81, wpb=51.5, bsz=16, num_updates=3910, lr=3.88539e-05, gnorm=7.567, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6563
2022-07-12 14:46:35 - progress_bar.py[line:274] - INFO: epoch 004:     80 / 1283 loss=0.821, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=43.4, ups=0.81, wpb=53.7, bsz=16, num_updates=3920, lr=3.88201e-05, gnorm=6.621, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6576
2022-07-12 14:46:48 - progress_bar.py[line:274] - INFO: epoch 004:     90 / 1283 loss=1, loss_v1=0, loss_v2=0, nll_loss=0.626, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=42.5, ups=0.81, wpb=52.3, bsz=16, num_updates=3930, lr=3.87863e-05, gnorm=8.192, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6588
2022-07-12 14:47:00 - progress_bar.py[line:274] - INFO: epoch 004:    100 / 1283 loss=0.846, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=41.9, ups=0.81, wpb=51.8, bsz=16, num_updates=3940, lr=3.87525e-05, gnorm=7.864, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6600
2022-07-12 14:47:12 - progress_bar.py[line:274] - INFO: epoch 004:    110 / 1283 loss=0.955, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=40.5, ups=0.8, wpb=50.3, bsz=16, num_updates=3950, lr=3.87186e-05, gnorm=7.282, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=6613
2022-07-12 14:47:25 - progress_bar.py[line:274] - INFO: epoch 004:    120 / 1283 loss=0.955, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=41.9, ups=0.81, wpb=51.8, bsz=16, num_updates=3960, lr=3.86848e-05, gnorm=8.097, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6625
2022-07-12 14:47:37 - progress_bar.py[line:274] - INFO: epoch 004:    130 / 1283 loss=0.957, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=41, ups=0.81, wpb=50.4, bsz=16, num_updates=3970, lr=3.8651e-05, gnorm=7.963, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6637
2022-07-12 14:47:49 - progress_bar.py[line:274] - INFO: epoch 004:    140 / 1283 loss=0.818, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=54.9, nsentences=16, sample_size=54.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=44.8, ups=0.82, wpb=54.9, bsz=16, num_updates=3980, lr=3.86171e-05, gnorm=7.278, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6650
2022-07-12 14:48:02 - progress_bar.py[line:274] - INFO: epoch 004:    150 / 1283 loss=1.04, loss_v1=0, loss_v2=0, nll_loss=0.678, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=43.3, ups=0.82, wpb=52.8, bsz=16, num_updates=3990, lr=3.85833e-05, gnorm=8.268, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6662
2022-07-12 14:48:14 - progress_bar.py[line:274] - INFO: epoch 004:    160 / 1283 loss=0.924, loss_v1=0, loss_v2=0, nll_loss=0.563, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=43.1, ups=0.81, wpb=53.4, bsz=16, num_updates=4000, lr=3.85495e-05, gnorm=7.967, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=6674
2022-07-12 14:48:26 - progress_bar.py[line:274] - INFO: epoch 004:    170 / 1283 loss=0.835, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=51.6, nsentences=15.9, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=42.1, ups=0.82, wpb=51.6, bsz=15.9, num_updates=4010, lr=3.85157e-05, gnorm=6.293, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=6686
2022-07-12 14:48:39 - progress_bar.py[line:274] - INFO: epoch 004:    180 / 1283 loss=0.984, loss_v1=0, loss_v2=0, nll_loss=0.587, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=40.9, ups=0.81, wpb=50.5, bsz=16, num_updates=4020, lr=3.84818e-05, gnorm=10.643, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6699
2022-07-12 14:48:51 - progress_bar.py[line:274] - INFO: epoch 004:    190 / 1283 loss=0.914, loss_v1=0, loss_v2=0, nll_loss=0.538, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=41.6, ups=0.81, wpb=51.4, bsz=16, num_updates=4030, lr=3.8448e-05, gnorm=9.533, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6711
2022-07-12 14:49:03 - progress_bar.py[line:274] - INFO: epoch 004:    200 / 1283 loss=0.953, loss_v1=0, loss_v2=0, nll_loss=0.598, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=41.9, ups=0.81, wpb=51.9, bsz=16, num_updates=4040, lr=3.84142e-05, gnorm=7.869, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=6723
2022-07-12 14:49:16 - progress_bar.py[line:274] - INFO: epoch 004:    210 / 1283 loss=0.946, loss_v1=0, loss_v2=0, nll_loss=0.571, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=41.7, ups=0.81, wpb=51.4, bsz=16, num_updates=4050, lr=3.83804e-05, gnorm=7.839, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6736
2022-07-12 14:49:28 - progress_bar.py[line:274] - INFO: epoch 004:    220 / 1283 loss=0.947, loss_v1=0, loss_v2=0, nll_loss=0.58, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=42, ups=0.81, wpb=52.1, bsz=16, num_updates=4060, lr=3.83465e-05, gnorm=8.121, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6748
2022-07-12 14:49:40 - progress_bar.py[line:274] - INFO: epoch 004:    230 / 1283 loss=0.927, loss_v1=0, loss_v2=0, nll_loss=0.55, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=43.2, ups=0.82, wpb=52.8, bsz=16, num_updates=4070, lr=3.83127e-05, gnorm=8.719, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6760
2022-07-12 14:49:53 - progress_bar.py[line:274] - INFO: epoch 004:    240 / 1283 loss=0.89, loss_v1=0, loss_v2=0, nll_loss=0.501, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=41, ups=0.8, wpb=51.2, bsz=16, num_updates=4080, lr=3.82789e-05, gnorm=8.081, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=6773
2022-07-12 14:50:05 - progress_bar.py[line:274] - INFO: epoch 004:    250 / 1283 loss=0.816, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=43.1, ups=0.81, wpb=53.2, bsz=16, num_updates=4090, lr=3.8245e-05, gnorm=7.222, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=6785
2022-07-12 14:50:17 - progress_bar.py[line:274] - INFO: epoch 004:    260 / 1283 loss=0.975, loss_v1=0, loss_v2=0, nll_loss=0.587, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=42.8, ups=0.82, wpb=51.9, bsz=16, num_updates=4100, lr=3.82112e-05, gnorm=7.078, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6797
2022-07-12 14:50:29 - progress_bar.py[line:274] - INFO: epoch 004:    270 / 1283 loss=0.832, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=40.6, ups=0.82, wpb=49.7, bsz=16, num_updates=4110, lr=3.81774e-05, gnorm=7.601, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6810
2022-07-12 14:50:42 - progress_bar.py[line:274] - INFO: epoch 004:    280 / 1283 loss=0.956, loss_v1=0, loss_v2=0, nll_loss=0.568, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=41.6, ups=0.8, wpb=52.1, bsz=16, num_updates=4120, lr=3.81436e-05, gnorm=8.084, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6822
2022-07-12 14:50:54 - progress_bar.py[line:274] - INFO: epoch 004:    290 / 1283 loss=0.864, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=41.8, ups=0.8, wpb=52.1, bsz=16, num_updates=4130, lr=3.81097e-05, gnorm=7.112, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6835
2022-07-12 14:51:07 - progress_bar.py[line:274] - INFO: epoch 004:    300 / 1283 loss=0.879, loss_v1=0, loss_v2=0, nll_loss=0.475, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=41.7, ups=0.82, wpb=50.9, bsz=16, num_updates=4140, lr=3.80759e-05, gnorm=7.707, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6847
2022-07-12 14:51:19 - progress_bar.py[line:274] - INFO: epoch 004:    310 / 1283 loss=0.938, loss_v1=0, loss_v2=0, nll_loss=0.556, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=42.9, ups=0.81, wpb=53.1, bsz=16, num_updates=4150, lr=3.80421e-05, gnorm=8.457, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6859
2022-07-12 14:51:31 - progress_bar.py[line:274] - INFO: epoch 004:    320 / 1283 loss=0.912, loss_v1=0, loss_v2=0, nll_loss=0.556, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=43.7, ups=0.81, wpb=54, bsz=16, num_updates=4160, lr=3.80083e-05, gnorm=8.077, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=6872
2022-07-12 14:51:44 - progress_bar.py[line:274] - INFO: epoch 004:    330 / 1283 loss=0.901, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=42.4, ups=0.81, wpb=52.5, bsz=16, num_updates=4170, lr=3.79744e-05, gnorm=8.627, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6884
2022-07-12 14:51:56 - progress_bar.py[line:274] - INFO: epoch 004:    340 / 1283 loss=0.942, loss_v1=0, loss_v2=0, nll_loss=0.561, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=42.9, ups=0.82, wpb=52.6, bsz=16, num_updates=4180, lr=3.79406e-05, gnorm=8.363, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6896
2022-07-12 14:52:09 - progress_bar.py[line:274] - INFO: epoch 004:    350 / 1283 loss=0.952, loss_v1=0, loss_v2=0, nll_loss=0.522, ntokens=47.7, nsentences=16, sample_size=47.7, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=38, ups=0.8, wpb=47.7, bsz=16, num_updates=4190, lr=3.79068e-05, gnorm=9.776, clip=100, loss_scale=32, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=6909
2022-07-12 14:52:21 - progress_bar.py[line:274] - INFO: epoch 004:    360 / 1283 loss=0.877, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=42, ups=0.82, wpb=51.2, bsz=16, num_updates=4200, lr=3.78729e-05, gnorm=8.576, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6921
2022-07-12 14:52:33 - progress_bar.py[line:274] - INFO: epoch 004:    370 / 1283 loss=0.892, loss_v1=0, loss_v2=0, nll_loss=0.498, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=40.6, ups=0.81, wpb=50.4, bsz=16, num_updates=4210, lr=3.78391e-05, gnorm=9.47, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=6933
2022-07-12 14:52:45 - progress_bar.py[line:274] - INFO: epoch 004:    380 / 1283 loss=0.862, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=4220, lr=3.78053e-05, gnorm=7.81, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=6946
2022-07-12 14:52:58 - progress_bar.py[line:274] - INFO: epoch 004:    390 / 1283 loss=0.86, loss_v1=0, loss_v2=0, nll_loss=0.489, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=43.7, ups=0.81, wpb=53.9, bsz=16, num_updates=4230, lr=3.77715e-05, gnorm=8.283, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6958
2022-07-12 14:53:10 - progress_bar.py[line:274] - INFO: epoch 004:    400 / 1283 loss=0.809, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=44.1, ups=0.81, wpb=54.2, bsz=16, num_updates=4240, lr=3.77376e-05, gnorm=6.921, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=6970
2022-07-12 14:53:22 - progress_bar.py[line:274] - INFO: epoch 004:    410 / 1283 loss=0.829, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=41.9, ups=0.81, wpb=52, bsz=16, num_updates=4250, lr=3.77038e-05, gnorm=7.117, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=6983
2022-07-12 14:53:35 - progress_bar.py[line:274] - INFO: epoch 004:    420 / 1283 loss=0.833, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=40.5, ups=0.8, wpb=50.3, bsz=16, num_updates=4260, lr=3.767e-05, gnorm=8.932, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=6995
2022-07-12 14:53:47 - progress_bar.py[line:274] - INFO: epoch 004:    430 / 1283 loss=0.948, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=41.7, ups=0.81, wpb=51.7, bsz=16, num_updates=4270, lr=3.76362e-05, gnorm=8.875, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7008
2022-07-12 14:54:00 - progress_bar.py[line:274] - INFO: epoch 004:    440 / 1283 loss=0.825, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=41.3, ups=0.8, wpb=51.4, bsz=16, num_updates=4280, lr=3.76023e-05, gnorm=6.991, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7020
2022-07-12 14:54:12 - progress_bar.py[line:274] - INFO: epoch 004:    450 / 1283 loss=0.903, loss_v1=0, loss_v2=0, nll_loss=0.517, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=41.9, ups=0.81, wpb=51.5, bsz=16, num_updates=4290, lr=3.75685e-05, gnorm=13.558, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=7032
2022-07-12 14:54:25 - progress_bar.py[line:274] - INFO: epoch 004:    460 / 1283 loss=0.89, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=41.2, ups=0.8, wpb=51.3, bsz=16, num_updates=4300, lr=3.75347e-05, gnorm=7.178, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=7045
2022-07-12 14:54:37 - progress_bar.py[line:274] - INFO: epoch 004:    470 / 1283 loss=0.782, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=43.3, ups=0.81, wpb=53.3, bsz=16, num_updates=4310, lr=3.75008e-05, gnorm=7.638, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=7057
2022-07-12 14:54:49 - progress_bar.py[line:274] - INFO: epoch 004:    480 / 1283 loss=0.99, loss_v1=0, loss_v2=0, nll_loss=0.6, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=42.6, ups=0.82, wpb=52, bsz=16, num_updates=4320, lr=3.7467e-05, gnorm=9.592, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7069
2022-07-12 14:55:02 - progress_bar.py[line:274] - INFO: epoch 004:    490 / 1283 loss=0.841, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=42.8, ups=0.8, wpb=53.4, bsz=16, num_updates=4330, lr=3.74332e-05, gnorm=7.485, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=7082
2022-07-12 14:55:14 - progress_bar.py[line:274] - INFO: epoch 004:    500 / 1283 loss=0.902, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=43.9, ups=0.82, wpb=53.7, bsz=16, num_updates=4340, lr=3.73994e-05, gnorm=8.24, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7094
2022-07-12 14:55:26 - progress_bar.py[line:274] - INFO: epoch 004:    510 / 1283 loss=0.942, loss_v1=0, loss_v2=0, nll_loss=0.555, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=43.9, ups=0.81, wpb=54.1, bsz=16, num_updates=4350, lr=3.73655e-05, gnorm=8.502, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7106
2022-07-12 14:55:38 - progress_bar.py[line:274] - INFO: epoch 004:    520 / 1283 loss=0.835, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=40.5, ups=0.81, wpb=50, bsz=16, num_updates=4360, lr=3.73317e-05, gnorm=7.497, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7119
2022-07-12 14:55:51 - progress_bar.py[line:274] - INFO: epoch 004:    530 / 1283 loss=0.853, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=43.5, ups=0.8, wpb=54.2, bsz=16, num_updates=4370, lr=3.72979e-05, gnorm=9.298, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7131
2022-07-12 14:56:03 - progress_bar.py[line:274] - INFO: epoch 004:    540 / 1283 loss=0.93, loss_v1=0, loss_v2=0, nll_loss=0.56, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=42.3, ups=0.8, wpb=52.8, bsz=16, num_updates=4380, lr=3.72641e-05, gnorm=9.339, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7144
2022-07-12 14:56:16 - progress_bar.py[line:274] - INFO: epoch 004:    550 / 1283 loss=0.872, loss_v1=0, loss_v2=0, nll_loss=0.494, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=45, ups=0.82, wpb=55.1, bsz=16, num_updates=4390, lr=3.72302e-05, gnorm=8.581, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7156
2022-07-12 14:56:28 - progress_bar.py[line:274] - INFO: epoch 004:    560 / 1283 loss=0.892, loss_v1=0, loss_v2=0, nll_loss=0.505, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=41.5, ups=0.8, wpb=51.8, bsz=16, num_updates=4400, lr=3.71964e-05, gnorm=7.854, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7168
2022-07-12 14:56:41 - progress_bar.py[line:274] - INFO: epoch 004:    570 / 1283 loss=0.792, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=40.6, ups=0.8, wpb=50.6, bsz=16, num_updates=4410, lr=3.71626e-05, gnorm=6.659, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7181
2022-07-12 14:56:53 - progress_bar.py[line:274] - INFO: epoch 004:    580 / 1283 loss=0.788, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=41.5, ups=0.81, wpb=51.2, bsz=16, num_updates=4420, lr=3.71287e-05, gnorm=7.294, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7193
2022-07-12 14:57:05 - progress_bar.py[line:274] - INFO: epoch 004:    590 / 1283 loss=0.832, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=43.3, ups=0.81, wpb=53.8, bsz=16, num_updates=4430, lr=3.70949e-05, gnorm=6.868, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7206
2022-07-12 14:57:18 - progress_bar.py[line:274] - INFO: epoch 004:    600 / 1283 loss=0.855, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=41.9, ups=0.81, wpb=51.9, bsz=16, num_updates=4440, lr=3.70611e-05, gnorm=9.297, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7218
2022-07-12 14:57:30 - progress_bar.py[line:274] - INFO: epoch 004:    610 / 1283 loss=0.841, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=42.8, ups=0.81, wpb=52.6, bsz=16, num_updates=4450, lr=3.70273e-05, gnorm=6.769, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7230
2022-07-12 14:57:42 - progress_bar.py[line:274] - INFO: epoch 004:    620 / 1283 loss=0.822, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=42.6, ups=0.81, wpb=52.8, bsz=16, num_updates=4460, lr=3.69934e-05, gnorm=7.035, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7243
2022-07-12 14:57:55 - progress_bar.py[line:274] - INFO: epoch 004:    630 / 1283 loss=0.84, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=41.6, ups=0.82, wpb=50.9, bsz=16, num_updates=4470, lr=3.69596e-05, gnorm=8.669, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7255
2022-07-12 14:58:07 - progress_bar.py[line:274] - INFO: epoch 004:    640 / 1283 loss=0.823, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=43.3, ups=0.8, wpb=54, bsz=16, num_updates=4480, lr=3.69258e-05, gnorm=7.263, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7267
2022-07-12 14:58:19 - progress_bar.py[line:274] - INFO: epoch 004:    650 / 1283 loss=0.785, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=43, ups=0.81, wpb=52.9, bsz=16, num_updates=4490, lr=3.6892e-05, gnorm=8.15, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7280
2022-07-12 14:58:32 - progress_bar.py[line:274] - INFO: epoch 004:    660 / 1283 loss=0.859, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=41.5, ups=0.81, wpb=51, bsz=16, num_updates=4500, lr=3.68581e-05, gnorm=9.022, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=7292
2022-07-12 14:58:44 - progress_bar.py[line:274] - INFO: epoch 004:    670 / 1283 loss=0.811, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=42.7, ups=0.8, wpb=53.1, bsz=16, num_updates=4510, lr=3.68243e-05, gnorm=9.118, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7304
2022-07-12 14:58:56 - progress_bar.py[line:274] - INFO: epoch 004:    680 / 1283 loss=0.736, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=43.1, ups=0.81, wpb=53.3, bsz=16, num_updates=4520, lr=3.67905e-05, gnorm=6.39, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7317
2022-07-12 14:59:09 - progress_bar.py[line:274] - INFO: epoch 004:    690 / 1283 loss=0.861, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=42.6, ups=0.82, wpb=52.3, bsz=16, num_updates=4530, lr=3.67566e-05, gnorm=7.93, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7329
2022-07-12 14:59:21 - progress_bar.py[line:274] - INFO: epoch 004:    700 / 1283 loss=0.85, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=48.8, nsentences=16, sample_size=48.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=39.7, ups=0.81, wpb=48.8, bsz=16, num_updates=4540, lr=3.67228e-05, gnorm=6.391, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7341
2022-07-12 14:59:33 - progress_bar.py[line:274] - INFO: epoch 004:    710 / 1283 loss=0.752, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=42.6, ups=0.82, wpb=52, bsz=16, num_updates=4550, lr=3.6689e-05, gnorm=6.649, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7353
2022-07-12 14:59:46 - progress_bar.py[line:274] - INFO: epoch 004:    720 / 1283 loss=0.764, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=43.7, ups=0.81, wpb=53.9, bsz=16, num_updates=4560, lr=3.66552e-05, gnorm=8.038, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7366
2022-07-12 14:59:58 - progress_bar.py[line:274] - INFO: epoch 004:    730 / 1283 loss=0.831, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=41.9, ups=0.81, wpb=51.7, bsz=16, num_updates=4570, lr=3.66213e-05, gnorm=5.848, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7378
2022-07-12 15:00:10 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-12 15:00:11 - progress_bar.py[line:274] - INFO: epoch 004:    741 / 1283 loss=0.804, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=39.4, ups=0.75, wpb=52.7, bsz=16, num_updates=4580, lr=3.65875e-05, gnorm=6.146, clip=100, loss_scale=32, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=7391
2022-07-12 15:00:23 - progress_bar.py[line:274] - INFO: epoch 004:    751 / 1283 loss=0.74, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=44.1, ups=0.82, wpb=54, bsz=16, num_updates=4590, lr=3.65537e-05, gnorm=5.805, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7404
2022-07-12 15:00:36 - progress_bar.py[line:274] - INFO: epoch 004:    761 / 1283 loss=0.812, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=42, ups=0.81, wpb=51.9, bsz=16, num_updates=4600, lr=3.65199e-05, gnorm=7.437, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=7416
2022-07-12 15:00:48 - progress_bar.py[line:274] - INFO: epoch 004:    771 / 1283 loss=0.943, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=41.4, ups=0.81, wpb=51.2, bsz=16, num_updates=4610, lr=3.6486e-05, gnorm=8.924, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7428
2022-07-12 15:01:01 - progress_bar.py[line:274] - INFO: epoch 004:    781 / 1283 loss=0.785, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=42.7, ups=0.81, wpb=52.9, bsz=16, num_updates=4620, lr=3.64522e-05, gnorm=7.022, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=7441
2022-07-12 15:01:13 - progress_bar.py[line:274] - INFO: epoch 004:    791 / 1283 loss=0.727, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=54.5, nsentences=16, sample_size=54.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=44.3, ups=0.81, wpb=54.5, bsz=16, num_updates=4630, lr=3.64184e-05, gnorm=5.746, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7453
2022-07-12 15:01:25 - progress_bar.py[line:274] - INFO: epoch 004:    801 / 1283 loss=0.76, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=41.4, ups=0.81, wpb=50.8, bsz=16, num_updates=4640, lr=3.63845e-05, gnorm=6.378, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7465
2022-07-12 15:01:38 - progress_bar.py[line:274] - INFO: epoch 004:    811 / 1283 loss=0.804, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=40.7, ups=0.81, wpb=50.5, bsz=16, num_updates=4650, lr=3.63507e-05, gnorm=7.952, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=7478
2022-07-12 15:01:50 - progress_bar.py[line:274] - INFO: epoch 004:    821 / 1283 loss=0.85, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=43.2, ups=0.8, wpb=53.8, bsz=16, num_updates=4660, lr=3.63169e-05, gnorm=9.545, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7490
2022-07-12 15:02:02 - progress_bar.py[line:274] - INFO: epoch 004:    831 / 1283 loss=0.764, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=41, ups=0.81, wpb=50.4, bsz=16, num_updates=4670, lr=3.62831e-05, gnorm=7.078, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7503
2022-07-12 15:02:15 - progress_bar.py[line:274] - INFO: epoch 004:    841 / 1283 loss=0.767, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=40.8, ups=0.8, wpb=51.3, bsz=16, num_updates=4680, lr=3.62492e-05, gnorm=7.077, clip=100, loss_scale=32, train_wall=13, gb_free=1.9, ema_decay=0.9999, wall=7515
2022-07-12 15:02:27 - progress_bar.py[line:274] - INFO: epoch 004:    851 / 1283 loss=0.822, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=42.8, ups=0.82, wpb=52.3, bsz=16, num_updates=4690, lr=3.62154e-05, gnorm=6.981, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7527
2022-07-12 15:02:39 - progress_bar.py[line:274] - INFO: epoch 004:    861 / 1283 loss=0.87, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=40.3, ups=0.81, wpb=49.7, bsz=16, num_updates=4700, lr=3.61816e-05, gnorm=6.464, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=7540
2022-07-12 15:02:52 - progress_bar.py[line:274] - INFO: epoch 004:    871 / 1283 loss=0.844, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=41.8, ups=0.81, wpb=51.7, bsz=16, num_updates=4710, lr=3.61478e-05, gnorm=8.248, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7552
2022-07-12 15:03:04 - progress_bar.py[line:274] - INFO: epoch 004:    881 / 1283 loss=0.804, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=42, ups=0.82, wpb=51.4, bsz=16, num_updates=4720, lr=3.61139e-05, gnorm=7.378, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7564
2022-07-12 15:03:16 - progress_bar.py[line:274] - INFO: epoch 004:    891 / 1283 loss=0.894, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=42.8, ups=0.81, wpb=52.9, bsz=16, num_updates=4730, lr=3.60801e-05, gnorm=8.739, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7577
2022-07-12 15:03:29 - progress_bar.py[line:274] - INFO: epoch 004:    901 / 1283 loss=0.703, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=56.8, nsentences=16, sample_size=56.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=45.9, ups=0.81, wpb=56.8, bsz=16, num_updates=4740, lr=3.60463e-05, gnorm=6.021, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7589
2022-07-12 15:03:41 - progress_bar.py[line:274] - INFO: epoch 004:    911 / 1283 loss=0.884, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=39.6, ups=0.8, wpb=49.8, bsz=16, num_updates=4750, lr=3.60124e-05, gnorm=8.904, clip=100, loss_scale=32, train_wall=13, gb_free=2, ema_decay=0.9999, wall=7602
2022-07-12 15:03:54 - progress_bar.py[line:274] - INFO: epoch 004:    921 / 1283 loss=0.824, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=40.7, ups=0.81, wpb=50, bsz=16, num_updates=4760, lr=3.59786e-05, gnorm=8.571, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7614
2022-07-12 15:04:06 - progress_bar.py[line:274] - INFO: epoch 004:    931 / 1283 loss=0.809, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=41.7, ups=0.8, wpb=51.8, bsz=16, num_updates=4770, lr=3.59448e-05, gnorm=6.925, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=7626
2022-07-12 15:04:18 - progress_bar.py[line:274] - INFO: epoch 004:    941 / 1283 loss=0.749, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=42.1, ups=0.81, wpb=51.8, bsz=16, num_updates=4780, lr=3.5911e-05, gnorm=5.605, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7639
2022-07-12 15:04:31 - progress_bar.py[line:274] - INFO: epoch 004:    951 / 1283 loss=0.816, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=43.3, ups=0.81, wpb=53.7, bsz=16, num_updates=4790, lr=3.58771e-05, gnorm=7.608, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=7651
2022-07-12 15:04:43 - progress_bar.py[line:274] - INFO: epoch 004:    961 / 1283 loss=0.774, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=42.9, ups=0.81, wpb=53, bsz=16, num_updates=4800, lr=3.58433e-05, gnorm=8.707, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7663
2022-07-12 15:04:55 - progress_bar.py[line:274] - INFO: epoch 004:    971 / 1283 loss=0.84, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=40.9, ups=0.81, wpb=50.5, bsz=16, num_updates=4810, lr=3.58095e-05, gnorm=9.396, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7676
2022-07-12 15:05:08 - progress_bar.py[line:274] - INFO: epoch 004:    981 / 1283 loss=0.802, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=40.1, ups=0.8, wpb=49.9, bsz=16, num_updates=4820, lr=3.57757e-05, gnorm=7.092, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7688
2022-07-12 15:05:20 - progress_bar.py[line:274] - INFO: epoch 004:    991 / 1283 loss=0.873, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=40.8, ups=0.8, wpb=51.1, bsz=16, num_updates=4830, lr=3.57418e-05, gnorm=7.982, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7701
2022-07-12 15:05:33 - progress_bar.py[line:274] - INFO: epoch 004:   1001 / 1283 loss=0.773, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=43, ups=0.81, wpb=52.9, bsz=16, num_updates=4840, lr=3.5708e-05, gnorm=6.699, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7713
2022-07-12 15:05:45 - progress_bar.py[line:274] - INFO: epoch 004:   1011 / 1283 loss=0.858, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=48.5, nsentences=16, sample_size=48.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=38.8, ups=0.8, wpb=48.5, bsz=16, num_updates=4850, lr=3.56742e-05, gnorm=8.31, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7725
2022-07-12 15:05:57 - progress_bar.py[line:274] - INFO: epoch 004:   1021 / 1283 loss=0.845, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=41.3, ups=0.82, wpb=50.3, bsz=16, num_updates=4860, lr=3.56403e-05, gnorm=7.226, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7738
2022-07-12 15:06:10 - progress_bar.py[line:274] - INFO: epoch 004:   1031 / 1283 loss=0.814, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=42.3, ups=0.81, wpb=52.3, bsz=16, num_updates=4870, lr=3.56065e-05, gnorm=6.849, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7750
2022-07-12 15:06:22 - progress_bar.py[line:274] - INFO: epoch 004:   1041 / 1283 loss=0.808, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=42.8, ups=0.81, wpb=52.6, bsz=16, num_updates=4880, lr=3.55727e-05, gnorm=6.953, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7762
2022-07-12 15:06:35 - progress_bar.py[line:274] - INFO: epoch 004:   1051 / 1283 loss=0.84, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=41.8, ups=0.8, wpb=52.1, bsz=16, num_updates=4890, lr=3.55389e-05, gnorm=7.072, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7775
2022-07-12 15:06:47 - progress_bar.py[line:274] - INFO: epoch 004:   1061 / 1283 loss=0.797, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=43.7, ups=0.81, wpb=53.9, bsz=16, num_updates=4900, lr=3.5505e-05, gnorm=7.277, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7787
2022-07-12 15:06:59 - progress_bar.py[line:274] - INFO: epoch 004:   1071 / 1283 loss=0.902, loss_v1=0, loss_v2=0, nll_loss=0.502, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=41.3, ups=0.82, wpb=50.4, bsz=16, num_updates=4910, lr=3.54712e-05, gnorm=9.58, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7799
2022-07-12 15:07:11 - progress_bar.py[line:274] - INFO: epoch 004:   1081 / 1283 loss=0.839, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=54.3, nsentences=16, sample_size=54.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=43.9, ups=0.81, wpb=54.3, bsz=16, num_updates=4920, lr=3.54374e-05, gnorm=7.38, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7812
2022-07-12 15:07:24 - progress_bar.py[line:274] - INFO: epoch 004:   1091 / 1283 loss=0.839, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=42.3, ups=0.82, wpb=51.5, bsz=16, num_updates=4930, lr=3.54036e-05, gnorm=7.937, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7824
2022-07-12 15:07:36 - progress_bar.py[line:274] - INFO: epoch 004:   1101 / 1283 loss=0.774, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=41, ups=0.81, wpb=50.6, bsz=16, num_updates=4940, lr=3.53697e-05, gnorm=6.873, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=7836
2022-07-12 15:07:48 - progress_bar.py[line:274] - INFO: epoch 004:   1111 / 1283 loss=0.807, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=40.5, ups=0.81, wpb=49.9, bsz=16, num_updates=4950, lr=3.53359e-05, gnorm=6.704, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=7849
2022-07-12 15:08:01 - progress_bar.py[line:274] - INFO: epoch 004:   1121 / 1283 loss=0.762, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=42, ups=0.8, wpb=52.4, bsz=16, num_updates=4960, lr=3.53021e-05, gnorm=7.09, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=7861
2022-07-12 15:08:13 - progress_bar.py[line:274] - INFO: epoch 004:   1131 / 1283 loss=0.812, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=41.7, ups=0.82, wpb=50.9, bsz=16, num_updates=4970, lr=3.52682e-05, gnorm=7.406, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=7873
2022-07-12 15:08:25 - progress_bar.py[line:274] - INFO: epoch 004:   1141 / 1283 loss=0.757, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=42.7, ups=0.8, wpb=53.2, bsz=16, num_updates=4980, lr=3.52344e-05, gnorm=7.387, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7886
2022-07-12 15:08:38 - progress_bar.py[line:274] - INFO: epoch 004:   1151 / 1283 loss=0.898, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=41.3, ups=0.81, wpb=51.1, bsz=16, num_updates=4990, lr=3.52006e-05, gnorm=8.449, clip=100, loss_scale=32, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=7898
2022-07-12 15:08:50 - progress_bar.py[line:274] - INFO: epoch 004:   1161 / 1283 loss=0.818, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=43.1, ups=0.81, wpb=53.1, bsz=16, num_updates=5000, lr=3.51668e-05, gnorm=6.908, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7910
2022-07-12 15:09:02 - progress_bar.py[line:274] - INFO: epoch 004:   1171 / 1283 loss=0.777, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=42.1, ups=0.81, wpb=51.9, bsz=16, num_updates=5010, lr=3.51329e-05, gnorm=8.062, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=7923
2022-07-12 15:09:15 - progress_bar.py[line:274] - INFO: epoch 004:   1181 / 1283 loss=0.766, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=42.6, ups=0.81, wpb=52.8, bsz=16, num_updates=5020, lr=3.50991e-05, gnorm=7.191, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=7935
2022-07-12 15:09:27 - progress_bar.py[line:274] - INFO: epoch 004:   1191 / 1283 loss=0.745, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=42.9, ups=0.81, wpb=52.9, bsz=16, num_updates=5030, lr=3.50653e-05, gnorm=6.377, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=7947
2022-07-12 15:09:40 - progress_bar.py[line:274] - INFO: epoch 004:   1201 / 1283 loss=0.857, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=40.8, ups=0.8, wpb=51, bsz=16, num_updates=5040, lr=3.50315e-05, gnorm=9.673, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7960
2022-07-12 15:09:52 - progress_bar.py[line:274] - INFO: epoch 004:   1211 / 1283 loss=0.766, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=41.8, ups=0.81, wpb=51.4, bsz=16, num_updates=5050, lr=3.49976e-05, gnorm=7.49, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7972
2022-07-12 15:10:04 - progress_bar.py[line:274] - INFO: epoch 004:   1221 / 1283 loss=0.837, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=40.9, ups=0.81, wpb=50.3, bsz=16, num_updates=5060, lr=3.49638e-05, gnorm=8.741, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=7985
2022-07-12 15:10:17 - progress_bar.py[line:274] - INFO: epoch 004:   1231 / 1283 loss=0.774, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=41.3, ups=0.82, wpb=50.6, bsz=16, num_updates=5070, lr=3.493e-05, gnorm=6.168, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=7997
2022-07-12 15:10:29 - progress_bar.py[line:274] - INFO: epoch 004:   1241 / 1283 loss=0.809, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=40.4, ups=0.81, wpb=50, bsz=16, num_updates=5080, lr=3.48962e-05, gnorm=6.558, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8009
2022-07-12 15:10:41 - progress_bar.py[line:274] - INFO: epoch 004:   1251 / 1283 loss=0.776, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=42.2, ups=0.81, wpb=51.8, bsz=16, num_updates=5090, lr=3.48623e-05, gnorm=9.573, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=8021
2022-07-12 15:10:54 - progress_bar.py[line:274] - INFO: epoch 004:   1261 / 1283 loss=0.768, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=42.7, ups=0.8, wpb=53.6, bsz=16, num_updates=5100, lr=3.48285e-05, gnorm=8.645, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=8034
2022-07-12 15:11:06 - progress_bar.py[line:274] - INFO: epoch 004:   1271 / 1283 loss=0.759, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=40.9, ups=0.79, wpb=51.5, bsz=16, num_updates=5110, lr=3.47947e-05, gnorm=6.821, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=8047
2022-07-12 15:11:19 - progress_bar.py[line:274] - INFO: epoch 004:   1281 / 1283 loss=0.864, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=48.4, nsentences=16, sample_size=48.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=39.3, ups=0.81, wpb=48.4, bsz=16, num_updates=5120, lr=3.47608e-05, gnorm=8.759, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8059
2022-07-12 15:11:21 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 15:19:20 - progress_bar.py[line:282] - INFO: epoch 004 | valid on 'valid' subset | loss 1.373 | loss_v1 0 | loss_v2 0 | nll_loss 1.074 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.11 | vqa_score 0.2422 | wps 30.7 | wpb 13.6 | bsz 4 | num_updates 5122 | best_vqa_score 0.2422
2022-07-12 15:19:20 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 4 @ 5122 updates
2022-07-12 15:19:20 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints4.pt
2022-07-12 15:19:51 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints4.pt
2022-07-12 15:20:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints4.pt (epoch 4 @ 5122 updates, score 0.2422) (writing took 87.44115825800691 seconds)
2022-07-12 15:20:47 - train.py[line:323] - INFO: end of epoch 4 (average epoch stats below)
2022-07-12 15:20:47 - progress_bar.py[line:282] - INFO: epoch 004 | loss 0.85 | loss_v1 0 | loss_v2 0 | nll_loss 0.452 | ntokens 51.941 | nsentences 15.996 | sample_size 51.941 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.37 | wps 30.9 | ups 0.6 | wpb 51.9 | bsz 16 | num_updates 5122 | lr 3.47541e-05 | gnorm 7.779 | clip 100 | loss_scale 64 | train_wall 1579 | gb_free 2.3 | ema_decay 0.9999 | wall 8627
2022-07-12 15:20:47 - trainer.py[line:639] - INFO: loading train data for epoch 5
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 15:20:49 - trainer.py[line:703] - INFO: begin training epoch 5
2022-07-12 15:20:49 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 15:21:00 - progress_bar.py[line:274] - INFO: epoch 005:      8 / 1283 loss=0.81, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=48.4, nsentences=15.6, sample_size=48.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=0.8, ups=0.02, wpb=48.4, bsz=15.6, num_updates=5130, lr=3.4727e-05, gnorm=7.426, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8640
2022-07-12 15:21:12 - progress_bar.py[line:274] - INFO: epoch 005:     18 / 1283 loss=0.731, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=41.6, ups=0.81, wpb=51.3, bsz=16, num_updates=5140, lr=3.46932e-05, gnorm=6.739, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=8652
2022-07-12 15:21:24 - progress_bar.py[line:274] - INFO: epoch 005:     28 / 1283 loss=0.71, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=43.2, ups=0.81, wpb=53.5, bsz=16, num_updates=5150, lr=3.46594e-05, gnorm=6.387, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8664
2022-07-12 15:21:37 - progress_bar.py[line:274] - INFO: epoch 005:     38 / 1283 loss=0.784, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=43.6, ups=0.82, wpb=53.5, bsz=16, num_updates=5160, lr=3.46255e-05, gnorm=7.446, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8677
2022-07-12 15:21:49 - progress_bar.py[line:274] - INFO: epoch 005:     48 / 1283 loss=0.757, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=43.3, ups=0.82, wpb=52.7, bsz=16, num_updates=5170, lr=3.45917e-05, gnorm=6.802, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=8689
2022-07-12 15:22:01 - progress_bar.py[line:274] - INFO: epoch 005:     58 / 1283 loss=0.742, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=40.3, ups=0.81, wpb=49.8, bsz=16, num_updates=5180, lr=3.45579e-05, gnorm=6.031, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8701
2022-07-12 15:22:13 - progress_bar.py[line:274] - INFO: epoch 005:     68 / 1283 loss=0.74, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=42, ups=0.81, wpb=51.7, bsz=16, num_updates=5190, lr=3.45241e-05, gnorm=7.403, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=8714
2022-07-12 15:22:26 - progress_bar.py[line:274] - INFO: epoch 005:     78 / 1283 loss=0.748, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=42.9, ups=0.81, wpb=52.9, bsz=16, num_updates=5200, lr=3.44902e-05, gnorm=6.758, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=8726
2022-07-12 15:22:38 - progress_bar.py[line:274] - INFO: epoch 005:     88 / 1283 loss=0.845, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=43.4, ups=0.82, wpb=53.1, bsz=16, num_updates=5210, lr=3.44564e-05, gnorm=8.499, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=8738
2022-07-12 15:22:50 - progress_bar.py[line:274] - INFO: epoch 005:     98 / 1283 loss=0.75, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=41.4, ups=0.81, wpb=50.8, bsz=16, num_updates=5220, lr=3.44226e-05, gnorm=6.919, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8750
2022-07-12 15:23:03 - progress_bar.py[line:274] - INFO: epoch 005:    108 / 1283 loss=0.761, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=41.2, ups=0.8, wpb=51.5, bsz=16, num_updates=5230, lr=3.43887e-05, gnorm=7.684, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8763
2022-07-12 15:23:15 - progress_bar.py[line:274] - INFO: epoch 005:    118 / 1283 loss=0.806, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=40.8, ups=0.81, wpb=50.5, bsz=16, num_updates=5240, lr=3.43549e-05, gnorm=7.532, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=8775
2022-07-12 15:23:27 - progress_bar.py[line:274] - INFO: epoch 005:    128 / 1283 loss=0.754, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=42.1, ups=0.81, wpb=51.7, bsz=16, num_updates=5250, lr=3.43211e-05, gnorm=6.36, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=8788
2022-07-12 15:23:40 - progress_bar.py[line:274] - INFO: epoch 005:    138 / 1283 loss=0.791, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=5260, lr=3.42873e-05, gnorm=6.74, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=8800
2022-07-12 15:23:52 - progress_bar.py[line:274] - INFO: epoch 005:    148 / 1283 loss=0.906, loss_v1=0, loss_v2=0, nll_loss=0.522, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=44.2, ups=0.82, wpb=54, bsz=16, num_updates=5270, lr=3.42534e-05, gnorm=7.77, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8812
2022-07-12 15:24:04 - progress_bar.py[line:274] - INFO: epoch 005:    158 / 1283 loss=0.812, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=43.7, ups=0.82, wpb=53.4, bsz=16, num_updates=5280, lr=3.42196e-05, gnorm=6.927, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8824
2022-07-12 15:24:17 - progress_bar.py[line:274] - INFO: epoch 005:    168 / 1283 loss=0.73, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=42.8, ups=0.81, wpb=53, bsz=16, num_updates=5290, lr=3.41858e-05, gnorm=7.305, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8837
2022-07-12 15:24:29 - progress_bar.py[line:274] - INFO: epoch 005:    178 / 1283 loss=0.858, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=49.2, nsentences=16, sample_size=49.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=39.8, ups=0.81, wpb=49.2, bsz=16, num_updates=5300, lr=3.4152e-05, gnorm=9.502, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=8849
2022-07-12 15:24:41 - progress_bar.py[line:274] - INFO: epoch 005:    188 / 1283 loss=0.816, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=41.2, ups=0.81, wpb=50.5, bsz=16, num_updates=5310, lr=3.41181e-05, gnorm=9.628, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=8861
2022-07-12 15:24:54 - progress_bar.py[line:274] - INFO: epoch 005:    198 / 1283 loss=0.753, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=43.4, ups=0.81, wpb=53.5, bsz=16, num_updates=5320, lr=3.40843e-05, gnorm=6.352, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=8874
2022-07-12 15:25:06 - progress_bar.py[line:274] - INFO: epoch 005:    208 / 1283 loss=0.752, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=41.6, ups=0.81, wpb=51.2, bsz=16, num_updates=5330, lr=3.40505e-05, gnorm=8.579, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=8886
2022-07-12 15:25:18 - progress_bar.py[line:274] - INFO: epoch 005:    218 / 1283 loss=0.804, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=41.9, ups=0.81, wpb=52.1, bsz=16, num_updates=5340, lr=3.40166e-05, gnorm=7.757, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=8898
2022-07-12 15:25:31 - progress_bar.py[line:274] - INFO: epoch 005:    228 / 1283 loss=0.822, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=43, ups=0.81, wpb=53, bsz=16, num_updates=5350, lr=3.39828e-05, gnorm=7.464, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=8911
2022-07-12 15:25:43 - progress_bar.py[line:274] - INFO: epoch 005:    238 / 1283 loss=0.71, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=41.7, ups=0.81, wpb=51.6, bsz=16, num_updates=5360, lr=3.3949e-05, gnorm=6.1, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=8923
2022-07-12 15:25:55 - progress_bar.py[line:274] - INFO: epoch 005:    248 / 1283 loss=0.711, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=42.8, ups=0.81, wpb=53.1, bsz=16, num_updates=5370, lr=3.39152e-05, gnorm=6.672, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=8936
2022-07-12 15:26:08 - progress_bar.py[line:274] - INFO: epoch 005:    258 / 1283 loss=0.781, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=43, ups=0.82, wpb=52.5, bsz=16, num_updates=5380, lr=3.38813e-05, gnorm=6.925, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=8948
2022-07-12 15:26:20 - progress_bar.py[line:274] - INFO: epoch 005:    268 / 1283 loss=0.79, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=48.7, nsentences=16, sample_size=48.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=39.9, ups=0.82, wpb=48.7, bsz=16, num_updates=5390, lr=3.38475e-05, gnorm=8.528, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=8960
2022-07-12 15:26:32 - progress_bar.py[line:274] - INFO: epoch 005:    278 / 1283 loss=0.759, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=41.1, ups=0.79, wpb=51.9, bsz=16, num_updates=5400, lr=3.38137e-05, gnorm=6.052, clip=100, loss_scale=64, train_wall=13, gb_free=1.9, ema_decay=0.9999, wall=8973
2022-07-12 15:26:45 - progress_bar.py[line:274] - INFO: epoch 005:    288 / 1283 loss=0.777, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=41.8, ups=0.81, wpb=51.8, bsz=16, num_updates=5410, lr=3.37799e-05, gnorm=6.601, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8985
2022-07-12 15:26:57 - progress_bar.py[line:274] - INFO: epoch 005:    298 / 1283 loss=0.731, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=42, ups=0.81, wpb=51.7, bsz=16, num_updates=5420, lr=3.3746e-05, gnorm=6.504, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=8997
2022-07-12 15:27:09 - progress_bar.py[line:274] - INFO: epoch 005:    308 / 1283 loss=0.789, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=43.1, ups=0.81, wpb=53.5, bsz=16, num_updates=5430, lr=3.37122e-05, gnorm=6.385, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9010
2022-07-12 15:27:22 - progress_bar.py[line:274] - INFO: epoch 005:    318 / 1283 loss=0.738, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=43.4, ups=0.82, wpb=53.2, bsz=16, num_updates=5440, lr=3.36784e-05, gnorm=8.135, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9022
2022-07-12 15:27:34 - progress_bar.py[line:274] - INFO: epoch 005:    328 / 1283 loss=0.775, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=42.2, ups=0.8, wpb=52.7, bsz=16, num_updates=5450, lr=3.36445e-05, gnorm=9.121, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9034
2022-07-12 15:27:47 - progress_bar.py[line:274] - INFO: epoch 005:    338 / 1283 loss=0.807, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=41.6, ups=0.8, wpb=52.1, bsz=16, num_updates=5460, lr=3.36107e-05, gnorm=7.575, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=9047
2022-07-12 15:27:59 - progress_bar.py[line:274] - INFO: epoch 005:    348 / 1283 loss=0.81, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=40.1, ups=0.81, wpb=49.5, bsz=16, num_updates=5470, lr=3.35769e-05, gnorm=9.576, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9059
2022-07-12 15:28:11 - progress_bar.py[line:274] - INFO: epoch 005:    358 / 1283 loss=0.775, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=40.6, ups=0.81, wpb=49.8, bsz=16, num_updates=5480, lr=3.35431e-05, gnorm=6.759, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9072
2022-07-12 15:28:24 - progress_bar.py[line:274] - INFO: epoch 005:    368 / 1283 loss=0.763, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=41.8, ups=0.81, wpb=51.6, bsz=16, num_updates=5490, lr=3.35092e-05, gnorm=7.523, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9084
2022-07-12 15:28:36 - progress_bar.py[line:274] - INFO: epoch 005:    378 / 1283 loss=0.74, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=42.1, ups=0.81, wpb=52.1, bsz=16, num_updates=5500, lr=3.34754e-05, gnorm=7.309, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=9096
2022-07-12 15:28:48 - progress_bar.py[line:274] - INFO: epoch 005:    388 / 1283 loss=0.773, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=42.8, ups=0.81, wpb=52.6, bsz=16, num_updates=5510, lr=3.34416e-05, gnorm=8.578, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9109
2022-07-12 15:29:01 - progress_bar.py[line:274] - INFO: epoch 005:    398 / 1283 loss=0.68, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=43.9, ups=0.81, wpb=54.4, bsz=16, num_updates=5520, lr=3.34078e-05, gnorm=5.853, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9121
2022-07-12 15:29:13 - progress_bar.py[line:274] - INFO: epoch 005:    408 / 1283 loss=0.74, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=41.4, ups=0.81, wpb=51.1, bsz=16, num_updates=5530, lr=3.33739e-05, gnorm=6.695, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9133
2022-07-12 15:29:25 - progress_bar.py[line:274] - INFO: epoch 005:    418 / 1283 loss=0.703, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=42, ups=0.81, wpb=51.6, bsz=16, num_updates=5540, lr=3.33401e-05, gnorm=5.852, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9146
2022-07-12 15:29:38 - progress_bar.py[line:274] - INFO: epoch 005:    428 / 1283 loss=0.756, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=40.9, ups=0.8, wpb=51.3, bsz=16, num_updates=5550, lr=3.33063e-05, gnorm=6.93, clip=100, loss_scale=64, train_wall=13, gb_free=2.1, ema_decay=0.9999, wall=9158
2022-07-12 15:29:50 - progress_bar.py[line:274] - INFO: epoch 005:    438 / 1283 loss=0.781, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=41.1, ups=0.8, wpb=51, bsz=16, num_updates=5560, lr=3.32724e-05, gnorm=8.307, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9171
2022-07-12 15:30:03 - progress_bar.py[line:274] - INFO: epoch 005:    448 / 1283 loss=0.722, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=42.9, ups=0.81, wpb=52.9, bsz=16, num_updates=5570, lr=3.32386e-05, gnorm=7.178, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9183
2022-07-12 15:30:15 - progress_bar.py[line:274] - INFO: epoch 005:    458 / 1283 loss=0.782, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=39.7, ups=0.8, wpb=49.5, bsz=16, num_updates=5580, lr=3.32048e-05, gnorm=7.748, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9195
2022-07-12 15:30:28 - progress_bar.py[line:274] - INFO: epoch 005:    468 / 1283 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=44, ups=0.81, wpb=54.6, bsz=16, num_updates=5590, lr=3.3171e-05, gnorm=5.404, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9208
2022-07-12 15:30:40 - progress_bar.py[line:274] - INFO: epoch 005:    478 / 1283 loss=0.753, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=41.2, ups=0.8, wpb=51.2, bsz=16, num_updates=5600, lr=3.31371e-05, gnorm=7.37, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=9220
2022-07-12 15:30:52 - progress_bar.py[line:274] - INFO: epoch 005:    488 / 1283 loss=0.791, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=43.7, ups=0.8, wpb=54.6, bsz=16, num_updates=5610, lr=3.31033e-05, gnorm=8.752, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9233
2022-07-12 15:31:05 - progress_bar.py[line:274] - INFO: epoch 005:    498 / 1283 loss=0.732, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=43, ups=0.81, wpb=53.1, bsz=16, num_updates=5620, lr=3.30695e-05, gnorm=6.695, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=9245
2022-07-12 15:31:17 - progress_bar.py[line:274] - INFO: epoch 005:    508 / 1283 loss=0.792, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=54.3, nsentences=16, sample_size=54.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=44.2, ups=0.81, wpb=54.3, bsz=16, num_updates=5630, lr=3.30357e-05, gnorm=7.666, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9257
2022-07-12 15:31:30 - progress_bar.py[line:274] - INFO: epoch 005:    518 / 1283 loss=0.718, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=41.3, ups=0.81, wpb=51.1, bsz=16, num_updates=5640, lr=3.30018e-05, gnorm=8.346, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9270
2022-07-12 15:31:42 - progress_bar.py[line:274] - INFO: epoch 005:    528 / 1283 loss=0.723, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=53, nsentences=15.9, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=42.7, ups=0.81, wpb=53, bsz=15.9, num_updates=5650, lr=3.2968e-05, gnorm=7.774, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9282
2022-07-12 15:31:54 - progress_bar.py[line:274] - INFO: epoch 005:    538 / 1283 loss=0.785, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=41.9, ups=0.8, wpb=52.5, bsz=16, num_updates=5660, lr=3.29342e-05, gnorm=8.354, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9295
2022-07-12 15:32:07 - progress_bar.py[line:274] - INFO: epoch 005:    548 / 1283 loss=0.76, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=45.1, ups=0.82, wpb=55.1, bsz=16, num_updates=5670, lr=3.29003e-05, gnorm=9.045, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9307
2022-07-12 15:32:19 - progress_bar.py[line:274] - INFO: epoch 005:    558 / 1283 loss=0.71, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=41.6, ups=0.8, wpb=52, bsz=16, num_updates=5680, lr=3.28665e-05, gnorm=7.073, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9319
2022-07-12 15:32:32 - progress_bar.py[line:274] - INFO: epoch 005:    568 / 1283 loss=0.724, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=40.5, ups=0.8, wpb=50.8, bsz=16, num_updates=5690, lr=3.28327e-05, gnorm=5.854, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9332
2022-07-12 15:32:44 - progress_bar.py[line:274] - INFO: epoch 005:    578 / 1283 loss=0.693, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=41, ups=0.8, wpb=50.9, bsz=16, num_updates=5700, lr=3.27989e-05, gnorm=5.849, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9344
2022-07-12 15:32:57 - progress_bar.py[line:274] - INFO: epoch 005:    588 / 1283 loss=0.688, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=43.1, ups=0.8, wpb=53.7, bsz=16, num_updates=5710, lr=3.2765e-05, gnorm=6.752, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9357
2022-07-12 15:33:09 - progress_bar.py[line:274] - INFO: epoch 005:    598 / 1283 loss=0.707, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=42.7, ups=0.8, wpb=53.3, bsz=16, num_updates=5720, lr=3.27312e-05, gnorm=6.063, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9369
2022-07-12 15:33:21 - progress_bar.py[line:274] - INFO: epoch 005:    608 / 1283 loss=0.723, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=41.7, ups=0.81, wpb=51.2, bsz=16, num_updates=5730, lr=3.26974e-05, gnorm=8.4, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9382
2022-07-12 15:33:34 - progress_bar.py[line:274] - INFO: epoch 005:    618 / 1283 loss=0.748, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=42.3, ups=0.8, wpb=53.1, bsz=16, num_updates=5740, lr=3.26636e-05, gnorm=7.877, clip=100, loss_scale=128, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=9394
2022-07-12 15:33:46 - progress_bar.py[line:274] - INFO: epoch 005:    628 / 1283 loss=0.671, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.3, ups=0.81, wpb=52, bsz=16, num_updates=5750, lr=3.26297e-05, gnorm=6.06, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9406
2022-07-12 15:33:59 - progress_bar.py[line:274] - INFO: epoch 005:    638 / 1283 loss=0.733, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=41.2, ups=0.8, wpb=51.8, bsz=16, num_updates=5760, lr=3.25959e-05, gnorm=6.808, clip=100, loss_scale=128, train_wall=13, gb_free=2.1, ema_decay=0.9999, wall=9419
2022-07-12 15:34:11 - progress_bar.py[line:274] - INFO: epoch 005:    648 / 1283 loss=0.711, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=42.1, ups=0.81, wpb=51.9, bsz=16, num_updates=5770, lr=3.25621e-05, gnorm=6.24, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9431
2022-07-12 15:34:23 - progress_bar.py[line:274] - INFO: epoch 005:    658 / 1283 loss=0.689, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=43.5, ups=0.82, wpb=53.3, bsz=16, num_updates=5780, lr=3.25282e-05, gnorm=7.875, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9444
2022-07-12 15:34:33 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 10.73 GiB total capacity; 8.27 GiB already allocated; 113.38 MiB free; 9.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 15:34:33 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8367 MB |    9225 MB |  698538 GB |  698530 GB |
|       from large pool |    8222 MB |    9021 MB |  692830 GB |  692822 GB |
|       from small pool |     145 MB |     207 MB |    5708 GB |    5708 GB |
|---------------------------------------------------------------------------|
| Active memory         |    8367 MB |    9225 MB |  698538 GB |  698530 GB |
|       from large pool |    8222 MB |    9021 MB |  692830 GB |  692822 GB |
|       from small pool |     145 MB |     207 MB |    5708 GB |    5708 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9504 MB |    9568 MB |   28368 MB |   18864 MB |
|       from large pool |    9354 MB |    9354 MB |   27416 MB |   18062 MB |
|       from small pool |     150 MB |     214 MB |     952 MB |     802 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1136 MB |    3305 MB |  672060 GB |  672058 GB |
|       from large pool |    1131 MB |    3303 MB |  665600 GB |  665599 GB |
|       from small pool |       4 MB |      18 MB |    6459 GB |    6459 GB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  155150 K  |  155146 K  |
|       from large pool |     835    |     910    |   46877 K  |   46876 K  |
|       from small pool |    3218    |    3470    |  108273 K  |  108269 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  155150 K  |  155146 K  |
|       from large pool |     835    |     910    |   46877 K  |   46876 K  |
|       from small pool |    3218    |    3470    |  108273 K  |  108269 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     178    |     210    |     793    |     615    |
|       from large pool |     103    |     103    |     317    |     214    |
|       from small pool |      75    |     107    |     476    |     401    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      56    |     205    |   85021 K  |   85021 K  |
|       from large pool |      45    |      69    |   23124 K  |   23124 K  |
|       from small pool |      11    |     154    |   61896 K  |   61896 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 15:34:33 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 15:34:36 - progress_bar.py[line:274] - INFO: epoch 005:    669 / 1283 loss=0.705, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=40.9, ups=0.77, wpb=52.9, bsz=16, num_updates=5790, lr=3.24944e-05, gnorm=6.788, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9457
2022-07-12 15:34:49 - progress_bar.py[line:274] - INFO: epoch 005:    679 / 1283 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=43.8, ups=0.81, wpb=54, bsz=16, num_updates=5800, lr=3.24606e-05, gnorm=7, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9469
2022-07-12 15:35:01 - progress_bar.py[line:274] - INFO: epoch 005:    689 / 1283 loss=0.731, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=41.6, ups=0.8, wpb=51.8, bsz=16, num_updates=5810, lr=3.24268e-05, gnorm=6.889, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9481
2022-07-12 15:35:14 - progress_bar.py[line:274] - INFO: epoch 005:    699 / 1283 loss=0.742, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=39.8, ups=0.81, wpb=49.4, bsz=16, num_updates=5820, lr=3.23929e-05, gnorm=6.126, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9494
2022-07-12 15:35:26 - progress_bar.py[line:274] - INFO: epoch 005:    709 / 1283 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.4, ups=0.82, wpb=52, bsz=16, num_updates=5830, lr=3.23591e-05, gnorm=5.557, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9506
2022-07-12 15:35:38 - progress_bar.py[line:274] - INFO: epoch 005:    719 / 1283 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.8, ups=0.81, wpb=52.8, bsz=16, num_updates=5840, lr=3.23253e-05, gnorm=5.442, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9518
2022-07-12 15:35:50 - progress_bar.py[line:274] - INFO: epoch 005:    729 / 1283 loss=0.694, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=42.4, ups=0.81, wpb=52.2, bsz=16, num_updates=5850, lr=3.22915e-05, gnorm=5.869, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9531
2022-07-12 15:36:03 - progress_bar.py[line:274] - INFO: epoch 005:    739 / 1283 loss=0.685, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=42.7, ups=0.81, wpb=53, bsz=16, num_updates=5860, lr=3.22576e-05, gnorm=5.881, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9543
2022-07-12 15:36:15 - progress_bar.py[line:274] - INFO: epoch 005:    749 / 1283 loss=0.73, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=41.9, ups=0.81, wpb=52, bsz=16, num_updates=5870, lr=3.22238e-05, gnorm=7.278, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9555
2022-07-12 15:36:28 - progress_bar.py[line:274] - INFO: epoch 005:    759 / 1283 loss=0.685, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=42.4, ups=0.8, wpb=52.8, bsz=16, num_updates=5880, lr=3.219e-05, gnorm=5.559, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9568
2022-07-12 15:36:40 - progress_bar.py[line:274] - INFO: epoch 005:    769 / 1283 loss=0.808, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=42.5, ups=0.82, wpb=52.1, bsz=16, num_updates=5890, lr=3.21561e-05, gnorm=8.528, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9580
2022-07-12 15:36:52 - progress_bar.py[line:274] - INFO: epoch 005:    779 / 1283 loss=0.763, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=41.1, ups=0.81, wpb=50.9, bsz=16, num_updates=5900, lr=3.21223e-05, gnorm=6.769, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=9593
2022-07-12 15:37:00 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 178.00 MiB (GPU 0; 10.73 GiB total capacity; 8.24 GiB already allocated; 119.19 MiB free; 9.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 15:37:00 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 16        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8436 MB |    8525 MB |  711233 GB |  711225 GB |
|       from large pool |    8291 MB |    8379 MB |  705457 GB |  705449 GB |
|       from small pool |     145 MB |     146 MB |    5775 GB |    5775 GB |
|---------------------------------------------------------------------------|
| Active memory         |    8436 MB |    8525 MB |  711233 GB |  711225 GB |
|       from large pool |    8291 MB |    8379 MB |  705457 GB |  705449 GB |
|       from small pool |     145 MB |     146 MB |    5775 GB |    5775 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9498 MB |    9556 MB |   28786 MB |   19288 MB |
|       from large pool |    9350 MB |    9350 MB |   27766 MB |   18416 MB |
|       from small pool |     148 MB |     206 MB |    1020 MB |     872 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1061 MB |    3288 MB |  685045 GB |  685044 GB |
|       from large pool |    1058 MB |    3284 MB |  678509 GB |  678508 GB |
|       from small pool |       2 MB |      24 MB |    6536 GB |    6536 GB |
|---------------------------------------------------------------------------|
| Allocations           |    4057    |    4058    |  157128 K  |  157124 K  |
|       from large pool |     841    |     842    |   47721 K  |   47720 K  |
|       from small pool |    3216    |    3216    |  109407 K  |  109403 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4057    |    4058    |  157128 K  |  157124 K  |
|       from large pool |     841    |     842    |   47721 K  |   47720 K  |
|       from small pool |    3216    |    3216    |  109407 K  |  109403 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     177    |     206    |     829    |     652    |
|       from large pool |     103    |     103    |     319    |     216    |
|       from small pool |      74    |     103    |     510    |     436    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      44    |     215    |   86022 K  |   86022 K  |
|       from large pool |      34    |      71    |   23531 K  |   23531 K  |
|       from small pool |      10    |     153    |   62490 K  |   62490 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 15:37:00 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 15:37:05 - progress_bar.py[line:274] - INFO: epoch 005:    790 / 1283 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=56.8, nsentences=16, sample_size=56.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=44.4, ups=0.78, wpb=56.8, bsz=16, num_updates=5910, lr=3.20885e-05, gnorm=4.773, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9605
2022-07-12 15:37:17 - progress_bar.py[line:274] - INFO: epoch 005:    800 / 1283 loss=0.701, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=40.8, ups=0.81, wpb=50.1, bsz=16, num_updates=5920, lr=3.20547e-05, gnorm=6.009, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9618
2022-07-12 15:37:30 - progress_bar.py[line:274] - INFO: epoch 005:    810 / 1283 loss=0.699, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=40.1, ups=0.8, wpb=49.9, bsz=16, num_updates=5930, lr=3.20208e-05, gnorm=6.683, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9630
2022-07-12 15:37:42 - progress_bar.py[line:274] - INFO: epoch 005:    820 / 1283 loss=0.736, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=54.9, nsentences=16, sample_size=54.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=43.6, ups=0.79, wpb=54.9, bsz=16, num_updates=5940, lr=3.1987e-05, gnorm=6.724, clip=100, loss_scale=128, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=9643
2022-07-12 15:37:55 - progress_bar.py[line:274] - INFO: epoch 005:    830 / 1283 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=40.6, ups=0.81, wpb=50, bsz=16, num_updates=5950, lr=3.19532e-05, gnorm=5.749, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9655
2022-07-12 15:38:07 - progress_bar.py[line:274] - INFO: epoch 005:    840 / 1283 loss=0.696, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=41.2, ups=0.8, wpb=51.8, bsz=16, num_updates=5960, lr=3.19194e-05, gnorm=5.688, clip=100, loss_scale=128, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=9668
2022-07-12 15:38:20 - progress_bar.py[line:274] - INFO: epoch 005:    850 / 1283 loss=0.742, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=41.7, ups=0.81, wpb=51.4, bsz=16, num_updates=5970, lr=3.18855e-05, gnorm=6.563, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9680
2022-07-12 15:38:32 - progress_bar.py[line:274] - INFO: epoch 005:    860 / 1283 loss=0.757, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=40.5, ups=0.8, wpb=50.3, bsz=16, num_updates=5980, lr=3.18517e-05, gnorm=5.815, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9692
2022-07-12 15:38:45 - progress_bar.py[line:274] - INFO: epoch 005:    870 / 1283 loss=0.733, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=41.4, ups=0.8, wpb=51.6, bsz=16, num_updates=5990, lr=3.18179e-05, gnorm=6.321, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9705
2022-07-12 15:38:57 - progress_bar.py[line:274] - INFO: epoch 005:    880 / 1283 loss=0.729, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=42.1, ups=0.81, wpb=52, bsz=16, num_updates=6000, lr=3.1784e-05, gnorm=6.229, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9717
2022-07-12 15:39:09 - progress_bar.py[line:274] - INFO: epoch 005:    890 / 1283 loss=0.717, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=41.4, ups=0.8, wpb=51.7, bsz=16, num_updates=6010, lr=3.17502e-05, gnorm=5.686, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9730
2022-07-12 15:39:22 - progress_bar.py[line:274] - INFO: epoch 005:    900 / 1283 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=56.5, nsentences=16, sample_size=56.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=45.3, ups=0.8, wpb=56.5, bsz=16, num_updates=6020, lr=3.17164e-05, gnorm=4.419, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9742
2022-07-12 15:39:34 - progress_bar.py[line:274] - INFO: epoch 005:    910 / 1283 loss=0.745, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=40.6, ups=0.8, wpb=50.8, bsz=16, num_updates=6030, lr=3.16826e-05, gnorm=8.715, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9755
2022-07-12 15:39:47 - progress_bar.py[line:274] - INFO: epoch 005:    920 / 1283 loss=0.72, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=40.9, ups=0.81, wpb=50.5, bsz=16, num_updates=6040, lr=3.16487e-05, gnorm=5.624, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9767
2022-07-12 15:39:59 - progress_bar.py[line:274] - INFO: epoch 005:    930 / 1283 loss=0.706, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=41.4, ups=0.8, wpb=51.5, bsz=16, num_updates=6050, lr=3.16149e-05, gnorm=5.681, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9779
2022-07-12 15:40:12 - progress_bar.py[line:274] - INFO: epoch 005:    940 / 1283 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.1, ups=0.81, wpb=51.9, bsz=16, num_updates=6060, lr=3.15811e-05, gnorm=5.71, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9792
2022-07-12 15:40:24 - progress_bar.py[line:274] - INFO: epoch 005:    950 / 1283 loss=0.682, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=42.4, ups=0.81, wpb=52.6, bsz=16, num_updates=6070, lr=3.15473e-05, gnorm=5.888, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9804
2022-07-12 15:40:36 - progress_bar.py[line:274] - INFO: epoch 005:    960 / 1283 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=43.4, ups=0.8, wpb=54.4, bsz=16, num_updates=6080, lr=3.15134e-05, gnorm=5.989, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=9817
2022-07-12 15:40:49 - progress_bar.py[line:274] - INFO: epoch 005:    970 / 1283 loss=0.706, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=40.3, ups=0.81, wpb=49.9, bsz=16, num_updates=6090, lr=3.14796e-05, gnorm=7.842, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9829
2022-07-12 15:41:01 - progress_bar.py[line:274] - INFO: epoch 005:    980 / 1283 loss=0.775, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=40.6, ups=0.81, wpb=50.1, bsz=16, num_updates=6100, lr=3.14458e-05, gnorm=6.479, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9841
2022-07-12 15:41:14 - progress_bar.py[line:274] - INFO: epoch 005:    990 / 1283 loss=0.757, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=41, ups=0.8, wpb=51.3, bsz=16, num_updates=6110, lr=3.14119e-05, gnorm=6.599, clip=100, loss_scale=128, train_wall=12, gb_free=2, ema_decay=0.9999, wall=9854
2022-07-12 15:41:26 - progress_bar.py[line:274] - INFO: epoch 005:   1000 / 1283 loss=0.685, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=41.8, ups=0.81, wpb=51.6, bsz=16, num_updates=6120, lr=3.13781e-05, gnorm=6.92, clip=100, loss_scale=256, train_wall=12, gb_free=2, ema_decay=0.9999, wall=9866
2022-07-12 15:41:31 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-07-12 15:41:40 - progress_bar.py[line:274] - INFO: epoch 005:   1011 / 1283 loss=0.728, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=36.3, ups=0.73, wpb=49.9, bsz=16, num_updates=6130, lr=3.13443e-05, gnorm=6.545, clip=100, loss_scale=128, train_wall=14, gb_free=2.4, ema_decay=0.9999, wall=9880
2022-07-12 15:41:52 - progress_bar.py[line:274] - INFO: epoch 005:   1021 / 1283 loss=0.707, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=41.1, ups=0.82, wpb=50.3, bsz=16, num_updates=6140, lr=3.13105e-05, gnorm=5.715, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9892
2022-07-12 15:42:05 - progress_bar.py[line:274] - INFO: epoch 005:   1031 / 1283 loss=0.7, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=41.8, ups=0.8, wpb=52.3, bsz=16, num_updates=6150, lr=3.12766e-05, gnorm=6.192, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9905
2022-07-12 15:42:17 - progress_bar.py[line:274] - INFO: epoch 005:   1041 / 1283 loss=0.7, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=42.5, ups=0.81, wpb=52.6, bsz=16, num_updates=6160, lr=3.12428e-05, gnorm=5.869, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9917
2022-07-12 15:42:29 - progress_bar.py[line:274] - INFO: epoch 005:   1051 / 1283 loss=0.72, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=41.9, ups=0.8, wpb=52.1, bsz=16, num_updates=6170, lr=3.1209e-05, gnorm=6.717, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9930
2022-07-12 15:42:42 - progress_bar.py[line:274] - INFO: epoch 005:   1061 / 1283 loss=0.72, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=43.3, ups=0.8, wpb=53.9, bsz=16, num_updates=6180, lr=3.11752e-05, gnorm=6.09, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9942
2022-07-12 15:42:54 - progress_bar.py[line:274] - INFO: epoch 005:   1071 / 1283 loss=0.842, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=41.1, ups=0.82, wpb=50.4, bsz=16, num_updates=6190, lr=3.11413e-05, gnorm=10.36, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9954
2022-07-12 15:43:06 - progress_bar.py[line:274] - INFO: epoch 005:   1081 / 1283 loss=0.715, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=54.3, nsentences=16, sample_size=54.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=43.7, ups=0.8, wpb=54.3, bsz=16, num_updates=6200, lr=3.11075e-05, gnorm=7.176, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=9967
2022-07-12 15:43:19 - progress_bar.py[line:274] - INFO: epoch 005:   1091 / 1283 loss=0.723, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=42, ups=0.82, wpb=51.5, bsz=16, num_updates=6210, lr=3.10737e-05, gnorm=7.021, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9979
2022-07-12 15:43:31 - progress_bar.py[line:274] - INFO: epoch 005:   1101 / 1283 loss=0.694, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=40.5, ups=0.8, wpb=50.6, bsz=16, num_updates=6220, lr=3.10398e-05, gnorm=7.22, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=9991
2022-07-12 15:43:44 - progress_bar.py[line:274] - INFO: epoch 005:   1111 / 1283 loss=0.716, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=40.5, ups=0.81, wpb=49.9, bsz=16, num_updates=6230, lr=3.1006e-05, gnorm=5.734, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=10004
2022-07-12 15:43:56 - progress_bar.py[line:274] - INFO: epoch 005:   1121 / 1283 loss=0.668, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=42, ups=0.8, wpb=52.4, bsz=16, num_updates=6240, lr=3.09722e-05, gnorm=6.6, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=10016
2022-07-12 15:44:08 - progress_bar.py[line:274] - INFO: epoch 005:   1131 / 1283 loss=0.726, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=41.9, ups=0.82, wpb=50.9, bsz=16, num_updates=6250, lr=3.09384e-05, gnorm=7.756, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10028
2022-07-12 15:44:21 - progress_bar.py[line:274] - INFO: epoch 005:   1141 / 1283 loss=0.682, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=42.9, ups=0.81, wpb=53.2, bsz=16, num_updates=6260, lr=3.09045e-05, gnorm=7.196, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=10041
2022-07-12 15:44:32 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 10.73 GiB total capacity; 8.25 GiB already allocated; 131.38 MiB free; 9.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 15:44:32 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8344 MB |    9133 MB |  749995 GB |  749987 GB |
|       from large pool |    8198 MB |    8930 MB |  744015 GB |  744007 GB |
|       from small pool |     145 MB |     205 MB |    5979 GB |    5979 GB |
|---------------------------------------------------------------------------|
| Active memory         |    8344 MB |    9133 MB |  749995 GB |  749987 GB |
|       from large pool |    8198 MB |    8930 MB |  744015 GB |  744007 GB |
|       from small pool |     145 MB |     205 MB |    5979 GB |    5979 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9486 MB |    9550 MB |   29200 MB |   19714 MB |
|       from large pool |    9336 MB |    9336 MB |   28106 MB |   18770 MB |
|       from small pool |     150 MB |     214 MB |    1094 MB |     944 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1141 MB |    3552 MB |  723905 GB |  723904 GB |
|       from large pool |    1137 MB |    3550 MB |  717131 GB |  717130 GB |
|       from small pool |       4 MB |      18 MB |    6773 GB |    6773 GB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  163169 K  |  163165 K  |
|       from large pool |     835    |     904    |   50297 K  |   50297 K  |
|       from small pool |    3218    |    3476    |  112871 K  |  112868 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  163169 K  |  163165 K  |
|       from large pool |     835    |     904    |   50297 K  |   50297 K  |
|       from small pool |    3218    |    3476    |  112871 K  |  112868 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     178    |     210    |     868    |     690    |
|       from large pool |     103    |     103    |     321    |     218    |
|       from small pool |      75    |     107    |     547    |     472    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |     203    |   89075 K  |   89075 K  |
|       from large pool |      30    |      75    |   24770 K  |   24770 K  |
|       from small pool |      13    |     147    |   64304 K  |   64304 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 15:44:32 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 15:44:33 - progress_bar.py[line:274] - INFO: epoch 005:   1152 / 1283 loss=0.746, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=40.6, ups=0.78, wpb=52.2, bsz=16, num_updates=6270, lr=3.08707e-05, gnorm=9.729, clip=100, loss_scale=128, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=10054
2022-07-12 15:44:46 - progress_bar.py[line:274] - INFO: epoch 005:   1162 / 1283 loss=0.704, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=42.9, ups=0.81, wpb=52.9, bsz=16, num_updates=6280, lr=3.08369e-05, gnorm=7.417, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=10066
2022-07-12 15:44:50 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 15:44:59 - progress_bar.py[line:274] - INFO: epoch 005:   1173 / 1283 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=38.4, ups=0.74, wpb=52.3, bsz=16, num_updates=6290, lr=3.08031e-05, gnorm=5.544, clip=100, loss_scale=64, train_wall=14, gb_free=2.3, ema_decay=0.9999, wall=10080
2022-07-12 15:45:01 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 168.00 MiB (GPU 0; 10.73 GiB total capacity; 8.33 GiB already allocated; 161.19 MiB free; 9.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 15:45:01 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8260 MB |    8940 MB |  752480 GB |  752472 GB |
|       from large pool |    8114 MB |    8751 MB |  746486 GB |  746478 GB |
|       from small pool |     145 MB |     190 MB |    5993 GB |    5993 GB |
|---------------------------------------------------------------------------|
| Active memory         |    8260 MB |    8940 MB |  752480 GB |  752472 GB |
|       from large pool |    8114 MB |    8751 MB |  746486 GB |  746478 GB |
|       from small pool |     145 MB |     190 MB |    5993 GB |    5993 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9456 MB |    9502 MB |   29914 MB |   20458 MB |
|       from large pool |    9308 MB |    9308 MB |   28762 MB |   19454 MB |
|       from small pool |     148 MB |     194 MB |    1152 MB |    1004 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1027 MB |    3040 MB |  726406 GB |  726405 GB |
|       from large pool |    1025 MB |    3029 MB |  719617 GB |  719616 GB |
|       from small pool |       2 MB |      24 MB |    6788 GB |    6788 GB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  163556 K  |  163552 K  |
|       from large pool |     835    |     905    |   50463 K  |   50462 K  |
|       from small pool |    3218    |    3476    |  113093 K  |  113090 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  163556 K  |  163552 K  |
|       from large pool |     835    |     905    |   50463 K  |   50462 K  |
|       from small pool |    3218    |    3476    |  113093 K  |  113090 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     177    |     200    |     901    |     724    |
|       from large pool |     103    |     103    |     325    |     222    |
|       from small pool |      74    |      97    |     576    |     502    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      48    |     206    |   89270 K  |   89270 K  |
|       from large pool |      36    |      73    |   24849 K  |   24849 K  |
|       from small pool |      12    |     143    |   64421 K  |   64421 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 15:45:01 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 15:45:12 - progress_bar.py[line:274] - INFO: epoch 005:   1184 / 1283 loss=0.711, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=40.9, ups=0.78, wpb=52.6, bsz=16, num_updates=6300, lr=3.07692e-05, gnorm=7.176, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=10093
2022-07-12 15:45:25 - progress_bar.py[line:274] - INFO: epoch 005:   1194 / 1283 loss=0.717, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=40.8, ups=0.81, wpb=50.7, bsz=16, num_updates=6310, lr=3.07354e-05, gnorm=5.987, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10105
2022-07-12 15:45:37 - progress_bar.py[line:274] - INFO: epoch 005:   1204 / 1283 loss=0.703, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=41.7, ups=0.8, wpb=51.8, bsz=16, num_updates=6320, lr=3.07016e-05, gnorm=7.616, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=10117
2022-07-12 15:45:49 - progress_bar.py[line:274] - INFO: epoch 005:   1214 / 1283 loss=0.748, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=40.1, ups=0.81, wpb=49.6, bsz=16, num_updates=6330, lr=3.06677e-05, gnorm=8.048, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10130
2022-07-12 15:46:02 - progress_bar.py[line:274] - INFO: epoch 005:   1224 / 1283 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=41.8, ups=0.81, wpb=51.5, bsz=16, num_updates=6340, lr=3.06339e-05, gnorm=5.357, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=10142
2022-07-12 15:46:14 - progress_bar.py[line:274] - INFO: epoch 005:   1234 / 1283 loss=0.679, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=41.2, ups=0.81, wpb=51.1, bsz=16, num_updates=6350, lr=3.06001e-05, gnorm=6.063, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=10154
2022-07-12 15:46:27 - progress_bar.py[line:274] - INFO: epoch 005:   1244 / 1283 loss=0.705, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=40.5, ups=0.81, wpb=50.2, bsz=16, num_updates=6360, lr=3.05663e-05, gnorm=6.124, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=10167
2022-07-12 15:46:39 - progress_bar.py[line:274] - INFO: epoch 005:   1254 / 1283 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=43.1, ups=0.81, wpb=53.4, bsz=16, num_updates=6370, lr=3.05324e-05, gnorm=7.56, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=10179
2022-07-12 15:46:52 - progress_bar.py[line:274] - INFO: epoch 005:   1264 / 1283 loss=0.693, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=41.5, ups=0.8, wpb=52, bsz=16, num_updates=6380, lr=3.04986e-05, gnorm=6.963, clip=100, loss_scale=64, train_wall=13, gb_free=2.1, ema_decay=0.9999, wall=10192
2022-07-12 15:46:57 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 10.73 GiB total capacity; 8.40 GiB already allocated; 153.00 MiB free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 15:46:57 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8327 MB |    9096 MB |  762419 GB |  762411 GB |
|       from large pool |    8181 MB |    8896 MB |  756373 GB |  756365 GB |
|       from small pool |     145 MB |     202 MB |    6046 GB |    6046 GB |
|---------------------------------------------------------------------------|
| Active memory         |    8327 MB |    9096 MB |  762419 GB |  762411 GB |
|       from large pool |    8181 MB |    8896 MB |  756373 GB |  756365 GB |
|       from small pool |     145 MB |     202 MB |    6046 GB |    6046 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9464 MB |    9522 MB |   31886 MB |   22422 MB |
|       from large pool |    9316 MB |    9316 MB |   30440 MB |   21124 MB |
|       from small pool |     148 MB |     206 MB |    1446 MB |    1298 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1136 MB |    3489 MB |  736354 GB |  736353 GB |
|       from large pool |    1134 MB |    3487 MB |  729504 GB |  729503 GB |
|       from small pool |       2 MB |      22 MB |    6849 GB |    6849 GB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  165105 K  |  165101 K  |
|       from large pool |     835    |     904    |   51123 K  |   51122 K  |
|       from small pool |    3218    |    3476    |  113981 K  |  113978 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  165105 K  |  165101 K  |
|       from large pool |     835    |     904    |   51123 K  |   51122 K  |
|       from small pool |    3218    |    3476    |  113981 K  |  113978 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     177    |     206    |    1058    |     881    |
|       from large pool |     103    |     103    |     335    |     232    |
|       from small pool |      74    |     103    |     723    |     649    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      50    |     192    |   90054 K  |   90054 K  |
|       from large pool |      40    |      78    |   25167 K  |   25167 K  |
|       from small pool |      10    |     133    |   64887 K  |   64887 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 15:46:57 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 15:47:05 - progress_bar.py[line:274] - INFO: epoch 005:   1275 / 1283 loss=0.734, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=38.2, ups=0.77, wpb=49.8, bsz=16, num_updates=6390, lr=3.04648e-05, gnorm=6.661, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=10205
2022-07-12 15:47:14 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 15:55:15 - progress_bar.py[line:282] - INFO: epoch 005 | valid on 'valid' subset | loss 1.394 | loss_v1 0 | loss_v2 0 | nll_loss 1.098 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.14 | vqa_score 0.2846 | wps 30.5 | wpb 13.6 | bsz 4 | num_updates 6398 | best_vqa_score 0.2846
2022-07-12 15:55:15 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 5 @ 6398 updates
2022-07-12 15:55:15 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints5.pt
2022-07-12 15:55:46 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints5.pt
2022-07-12 15:56:42 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints5.pt (epoch 5 @ 6398 updates, score 0.2846) (writing took 87.18157784902724 seconds)
2022-07-12 15:56:42 - train.py[line:323] - INFO: end of epoch 5 (average epoch stats below)
2022-07-12 15:56:42 - progress_bar.py[line:282] - INFO: epoch 005 | loss 0.733 | loss_v1 0 | loss_v2 0 | nll_loss 0.314 | ntokens 51.952 | nsentences 15.996 | sample_size 51.952 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.24 | wps 30.8 | ups 0.59 | wpb 52 | bsz 16 | num_updates 6398 | lr 3.04377e-05 | gnorm 6.945 | clip 100 | loss_scale 64 | train_wall 1577 | gb_free 2.3 | ema_decay 0.9999 | wall 10782
2022-07-12 15:56:42 - trainer.py[line:639] - INFO: loading train data for epoch 6
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 15:56:44 - trainer.py[line:703] - INFO: begin training epoch 6
2022-07-12 15:56:44 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 15:56:47 - progress_bar.py[line:274] - INFO: epoch 006:      2 / 1283 loss=0.707, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=47.5, nsentences=15.6, sample_size=47.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=0.8, ups=0.02, wpb=47.5, bsz=15.6, num_updates=6400, lr=3.0431e-05, gnorm=6.853, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10788
2022-07-12 15:57:00 - progress_bar.py[line:274] - INFO: epoch 006:     12 / 1283 loss=0.722, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=42.1, ups=0.81, wpb=51.9, bsz=16, num_updates=6410, lr=3.03971e-05, gnorm=8.507, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=10800
2022-07-12 15:57:12 - progress_bar.py[line:274] - INFO: epoch 006:     22 / 1283 loss=0.712, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=41.7, ups=0.81, wpb=51.3, bsz=16, num_updates=6420, lr=3.03633e-05, gnorm=6.072, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=10812
2022-07-12 15:57:24 - progress_bar.py[line:274] - INFO: epoch 006:     32 / 1283 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=54.3, nsentences=16, sample_size=54.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=44.4, ups=0.82, wpb=54.3, bsz=16, num_updates=6430, lr=3.03295e-05, gnorm=6.815, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=10824
2022-07-12 15:57:36 - progress_bar.py[line:274] - INFO: epoch 006:     42 / 1283 loss=0.683, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=42.7, ups=0.82, wpb=52.3, bsz=16, num_updates=6440, lr=3.02956e-05, gnorm=6.716, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=10837
2022-07-12 15:57:46 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 10.73 GiB total capacity; 8.35 GiB already allocated; 156.81 MiB free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 15:57:46 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8283 MB |    9003 MB |     771 TB |     771 TB |
|       from large pool |    8138 MB |    8810 MB |     764 TB |     764 TB |
|       from small pool |     145 MB |     195 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8283 MB |    9003 MB |     771 TB |     771 TB |
|       from large pool |    8138 MB |    8810 MB |     764 TB |     764 TB |
|       from small pool |     145 MB |     195 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9460 MB |    9524 MB |   32632 MB |   23172 MB |
|       from large pool |    9310 MB |    9310 MB |   31110 MB |   21800 MB |
|       from small pool |     150 MB |     214 MB |    1522 MB |    1372 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1176 MB |    3267 MB |  761337 GB |  761335 GB |
|       from large pool |    1171 MB |    3265 MB |  753809 GB |  753808 GB |
|       from small pool |       4 MB |      22 MB |    7527 GB |    7527 GB |
|---------------------------------------------------------------------------|
| Allocations           |    4055    |    4378    |  180936 K  |  180932 K  |
|       from large pool |     835    |     904    |   53043 K  |   53042 K  |
|       from small pool |    3220    |    3494    |  127892 K  |  127889 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4055    |    4378    |  180936 K  |  180932 K  |
|       from large pool |     835    |     904    |   53043 K  |   53042 K  |
|       from small pool |    3220    |    3494    |  127892 K  |  127889 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     178    |     210    |    1100    |     922    |
|       from large pool |     103    |     103    |     339    |     236    |
|       from small pool |      75    |     107    |     761    |     686    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |     196    |   99736 K  |   99736 K  |
|       from large pool |      36    |      68    |   26083 K  |   26082 K  |
|       from small pool |      11    |     136    |   73653 K  |   73653 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 15:57:46 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 15:57:49 - progress_bar.py[line:274] - INFO: epoch 006:     53 / 1283 loss=0.706, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=39.7, ups=0.77, wpb=51.4, bsz=16, num_updates=6450, lr=3.02618e-05, gnorm=9.669, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10850
2022-07-12 15:58:02 - progress_bar.py[line:274] - INFO: epoch 006:     63 / 1283 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=40.7, ups=0.8, wpb=50.7, bsz=16, num_updates=6460, lr=3.0228e-05, gnorm=6.489, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=10862
2022-07-12 15:58:14 - progress_bar.py[line:274] - INFO: epoch 006:     73 / 1283 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.8, ups=0.81, wpb=52.6, bsz=16, num_updates=6470, lr=3.01942e-05, gnorm=4.862, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=10874
2022-07-12 15:58:27 - progress_bar.py[line:274] - INFO: epoch 006:     83 / 1283 loss=0.644, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.5, ups=0.8, wpb=53, bsz=16, num_updates=6480, lr=3.01603e-05, gnorm=5.034, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10887
2022-07-12 15:58:39 - progress_bar.py[line:274] - INFO: epoch 006:     93 / 1283 loss=0.714, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=43, ups=0.82, wpb=52.7, bsz=16, num_updates=6490, lr=3.01265e-05, gnorm=7.408, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10899
2022-07-12 15:58:51 - progress_bar.py[line:274] - INFO: epoch 006:    103 / 1283 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=40.7, ups=0.81, wpb=50.6, bsz=16, num_updates=6500, lr=3.00927e-05, gnorm=5.504, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=10912
2022-07-12 15:59:04 - progress_bar.py[line:274] - INFO: epoch 006:    113 / 1283 loss=0.73, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=39.6, ups=0.8, wpb=49.7, bsz=16, num_updates=6510, lr=3.00589e-05, gnorm=5.788, clip=100, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=10924
2022-07-12 15:59:16 - progress_bar.py[line:274] - INFO: epoch 006:    123 / 1283 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.7, ups=0.81, wpb=52.6, bsz=16, num_updates=6520, lr=3.0025e-05, gnorm=5.266, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10936
2022-07-12 15:59:29 - progress_bar.py[line:274] - INFO: epoch 006:    133 / 1283 loss=0.711, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=40.8, ups=0.81, wpb=50.6, bsz=16, num_updates=6530, lr=2.99912e-05, gnorm=7.678, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10949
2022-07-12 15:59:41 - progress_bar.py[line:274] - INFO: epoch 006:    143 / 1283 loss=0.715, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=43.9, ups=0.82, wpb=53.7, bsz=16, num_updates=6540, lr=2.99574e-05, gnorm=5.695, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10961
2022-07-12 15:59:53 - progress_bar.py[line:274] - INFO: epoch 006:    153 / 1283 loss=0.734, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=42.4, ups=0.81, wpb=52.5, bsz=16, num_updates=6550, lr=2.99236e-05, gnorm=6.505, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10973
2022-07-12 16:00:06 - progress_bar.py[line:274] - INFO: epoch 006:    163 / 1283 loss=0.678, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=43.5, ups=0.81, wpb=53.7, bsz=16, num_updates=6560, lr=2.98897e-05, gnorm=7.313, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10986
2022-07-12 16:00:18 - progress_bar.py[line:274] - INFO: epoch 006:    173 / 1283 loss=0.644, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=41.6, ups=0.81, wpb=51.5, bsz=16, num_updates=6570, lr=2.98559e-05, gnorm=5.358, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=10998
2022-07-12 16:00:30 - progress_bar.py[line:274] - INFO: epoch 006:    183 / 1283 loss=0.737, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=41.2, ups=0.81, wpb=50.9, bsz=16, num_updates=6580, lr=2.98221e-05, gnorm=6.874, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11011
2022-07-12 16:00:43 - progress_bar.py[line:274] - INFO: epoch 006:    193 / 1283 loss=0.728, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=42.2, ups=0.81, wpb=52.2, bsz=16, num_updates=6590, lr=2.97882e-05, gnorm=6.782, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11023
2022-07-12 16:00:55 - progress_bar.py[line:274] - INFO: epoch 006:    203 / 1283 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=41.7, ups=0.81, wpb=51.6, bsz=16, num_updates=6600, lr=2.97544e-05, gnorm=5.724, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11035
2022-07-12 16:01:07 - progress_bar.py[line:274] - INFO: epoch 006:    213 / 1283 loss=0.698, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=41.1, ups=0.81, wpb=51, bsz=16, num_updates=6610, lr=2.97206e-05, gnorm=5.224, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11048
2022-07-12 16:01:12 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-12 16:01:21 - progress_bar.py[line:274] - INFO: epoch 006:    224 / 1283 loss=0.731, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=38.8, ups=0.74, wpb=52.5, bsz=16, num_updates=6620, lr=2.96868e-05, gnorm=5.895, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=11061
2022-07-12 16:01:34 - progress_bar.py[line:274] - INFO: epoch 006:    234 / 1283 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.1, ups=0.8, wpb=52.7, bsz=16, num_updates=6630, lr=2.96529e-05, gnorm=5.347, clip=100, loss_scale=32, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=11074
2022-07-12 16:01:46 - progress_bar.py[line:274] - INFO: epoch 006:    244 / 1283 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.5, ups=0.8, wpb=52.9, bsz=16, num_updates=6640, lr=2.96191e-05, gnorm=5.255, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11086
2022-07-12 16:01:58 - progress_bar.py[line:274] - INFO: epoch 006:    254 / 1283 loss=0.684, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=42.2, ups=0.81, wpb=52, bsz=16, num_updates=6650, lr=2.95853e-05, gnorm=9.716, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11099
2022-07-12 16:02:11 - progress_bar.py[line:274] - INFO: epoch 006:    264 / 1283 loss=0.676, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=42.2, ups=0.82, wpb=51.7, bsz=16, num_updates=6660, lr=2.95515e-05, gnorm=5.61, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11111
2022-07-12 16:02:23 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 10.73 GiB total capacity; 8.19 GiB already allocated; 155.00 MiB free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 16:02:23 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8286 MB |    9009 MB |     794 TB |     794 TB |
|       from large pool |    8140 MB |    8815 MB |     787 TB |     787 TB |
|       from small pool |     145 MB |     196 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8286 MB |    9009 MB |     794 TB |     794 TB |
|       from large pool |    8140 MB |    8815 MB |     787 TB |     787 TB |
|       from small pool |     145 MB |     196 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9462 MB |    9512 MB |   33638 MB |   24176 MB |
|       from large pool |    9310 MB |    9310 MB |   31950 MB |   22640 MB |
|       from small pool |     152 MB |     202 MB |    1688 MB |    1536 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1175 MB |    3525 MB |  785090 GB |  785089 GB |
|       from large pool |    1169 MB |    3522 MB |  777418 GB |  777417 GB |
|       from small pool |       6 MB |      17 MB |    7671 GB |    7671 GB |
|---------------------------------------------------------------------------|
| Allocations           |    4057    |    4380    |  184646 K  |  184642 K  |
|       from large pool |     835    |     904    |   54626 K  |   54625 K  |
|       from small pool |    3222    |    3490    |  130020 K  |  130017 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4057    |    4380    |  184646 K  |  184642 K  |
|       from large pool |     835    |     904    |   54626 K  |   54625 K  |
|       from small pool |    3222    |    3490    |  130020 K  |  130017 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     179    |     204    |    1188    |    1009    |
|       from large pool |     103    |     103    |     344    |     241    |
|       from small pool |      76    |     101    |     844    |     768    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      59    |     197    |  101612 K  |  101612 K  |
|       from large pool |      44    |      80    |   26843 K  |   26843 K  |
|       from small pool |      15    |     139    |   74768 K  |   74768 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 16:02:23 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 16:02:24 - progress_bar.py[line:274] - INFO: epoch 006:    275 / 1283 loss=0.729, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=49.3, nsentences=16, sample_size=49.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=36.5, ups=0.74, wpb=49.3, bsz=16, num_updates=6670, lr=2.95176e-05, gnorm=7.666, clip=100, loss_scale=32, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=11124
2022-07-12 16:02:36 - progress_bar.py[line:274] - INFO: epoch 006:    285 / 1283 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.5, ups=0.81, wpb=52.6, bsz=16, num_updates=6680, lr=2.94838e-05, gnorm=5.089, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11137
2022-07-12 16:02:49 - progress_bar.py[line:274] - INFO: epoch 006:    295 / 1283 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.9, ups=0.8, wpb=52.2, bsz=16, num_updates=6690, lr=2.945e-05, gnorm=4.548, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=11149
2022-07-12 16:03:01 - progress_bar.py[line:274] - INFO: epoch 006:    305 / 1283 loss=0.696, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=41.9, ups=0.8, wpb=52.1, bsz=16, num_updates=6700, lr=2.94161e-05, gnorm=6.106, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11162
2022-07-12 16:03:14 - progress_bar.py[line:274] - INFO: epoch 006:    315 / 1283 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=44.3, ups=0.81, wpb=54.4, bsz=16, num_updates=6710, lr=2.93823e-05, gnorm=4.16, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11174
2022-07-12 16:03:26 - progress_bar.py[line:274] - INFO: epoch 006:    325 / 1283 loss=0.696, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=42.4, ups=0.8, wpb=53, bsz=16, num_updates=6720, lr=2.93485e-05, gnorm=8.491, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11186
2022-07-12 16:03:35 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 10.73 GiB total capacity; 8.40 GiB already allocated; 140.81 MiB free; 9.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 16:03:35 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8326 MB |    9109 MB |     800 TB |     800 TB |
|       from large pool |    8180 MB |    8907 MB |     793 TB |     793 TB |
|       from small pool |     145 MB |     204 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8326 MB |    9109 MB |     800 TB |     800 TB |
|       from large pool |    8180 MB |    8907 MB |     793 TB |     793 TB |
|       from small pool |     145 MB |     204 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9476 MB |    9534 MB |   34402 MB |   24926 MB |
|       from large pool |    9324 MB |    9324 MB |   32638 MB |   23314 MB |
|       from small pool |     152 MB |     214 MB |    1764 MB |    1612 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     977 MB |    3214 MB |     772 TB |     772 TB |
|       from large pool |     971 MB |    3209 MB |     765 TB |     765 TB |
|       from small pool |       6 MB |      26 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4057    |    4380    |  185606 K  |  185602 K  |
|       from large pool |     835    |     904    |   55035 K  |   55034 K  |
|       from small pool |    3222    |    3496    |  130570 K  |  130567 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4057    |    4380    |  185606 K  |  185602 K  |
|       from large pool |     835    |     904    |   55035 K  |   55034 K  |
|       from small pool |    3222    |    3496    |  130570 K  |  130567 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     179    |     210    |    1230    |    1051    |
|       from large pool |     103    |     103    |     348    |     245    |
|       from small pool |      76    |     107    |     882    |     806    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      44    |     200    |  102097 K  |  102097 K  |
|       from large pool |      31    |      73    |   27040 K  |   27040 K  |
|       from small pool |      13    |     144    |   75057 K  |   75057 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 16:03:35 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 16:03:40 - progress_bar.py[line:274] - INFO: epoch 006:    336 / 1283 loss=0.7, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=38, ups=0.74, wpb=51.2, bsz=16, num_updates=6730, lr=2.93147e-05, gnorm=6.088, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=11200
2022-07-12 16:03:52 - progress_bar.py[line:274] - INFO: epoch 006:    346 / 1283 loss=0.701, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=49.5, nsentences=15.9, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=39.7, ups=0.8, wpb=49.5, bsz=15.9, num_updates=6740, lr=2.92808e-05, gnorm=7.009, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11212
2022-07-12 16:04:04 - progress_bar.py[line:274] - INFO: epoch 006:    356 / 1283 loss=0.724, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=40.4, ups=0.82, wpb=49.5, bsz=16, num_updates=6750, lr=2.9247e-05, gnorm=7.48, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11225
2022-07-12 16:04:17 - progress_bar.py[line:274] - INFO: epoch 006:    366 / 1283 loss=0.682, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=41.7, ups=0.8, wpb=51.9, bsz=16, num_updates=6760, lr=2.92132e-05, gnorm=6.671, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11237
2022-07-12 16:04:29 - progress_bar.py[line:274] - INFO: epoch 006:    376 / 1283 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=40.6, ups=0.8, wpb=50.5, bsz=16, num_updates=6770, lr=2.91794e-05, gnorm=5.812, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=11249
2022-07-12 16:04:42 - progress_bar.py[line:274] - INFO: epoch 006:    386 / 1283 loss=0.683, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=42.2, ups=0.8, wpb=53.1, bsz=16, num_updates=6780, lr=2.91455e-05, gnorm=7.551, clip=100, loss_scale=32, train_wall=13, gb_free=2.1, ema_decay=0.9999, wall=11262
2022-07-12 16:04:54 - progress_bar.py[line:274] - INFO: epoch 006:    396 / 1283 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=55, nsentences=16, sample_size=55, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=44.6, ups=0.81, wpb=55, bsz=16, num_updates=6790, lr=2.91117e-05, gnorm=5.206, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11274
2022-07-12 16:05:06 - progress_bar.py[line:274] - INFO: epoch 006:    406 / 1283 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=41.6, ups=0.81, wpb=51.5, bsz=16, num_updates=6800, lr=2.90779e-05, gnorm=6.089, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11287
2022-07-12 16:05:19 - progress_bar.py[line:274] - INFO: epoch 006:    416 / 1283 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=43.2, ups=0.81, wpb=53.1, bsz=16, num_updates=6810, lr=2.9044e-05, gnorm=5.253, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11299
2022-07-12 16:05:31 - progress_bar.py[line:274] - INFO: epoch 006:    426 / 1283 loss=0.718, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=48.6, nsentences=16, sample_size=48.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=39, ups=0.8, wpb=48.6, bsz=16, num_updates=6820, lr=2.90102e-05, gnorm=7.043, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11311
2022-07-12 16:05:44 - progress_bar.py[line:274] - INFO: epoch 006:    436 / 1283 loss=0.689, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=41.5, ups=0.8, wpb=52, bsz=16, num_updates=6830, lr=2.89764e-05, gnorm=7.094, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11324
2022-07-12 16:05:56 - progress_bar.py[line:274] - INFO: epoch 006:    446 / 1283 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42.4, ups=0.81, wpb=52.6, bsz=16, num_updates=6840, lr=2.89426e-05, gnorm=5.12, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11336
2022-07-12 16:06:09 - progress_bar.py[line:274] - INFO: epoch 006:    456 / 1283 loss=0.728, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=39.8, ups=0.8, wpb=49.7, bsz=16, num_updates=6850, lr=2.89087e-05, gnorm=7.989, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=11349
2022-07-12 16:06:21 - progress_bar.py[line:274] - INFO: epoch 006:    466 / 1283 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=44, ups=0.81, wpb=54.4, bsz=16, num_updates=6860, lr=2.88749e-05, gnorm=4.956, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11361
2022-07-12 16:06:33 - progress_bar.py[line:274] - INFO: epoch 006:    476 / 1283 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=40.6, ups=0.81, wpb=49.8, bsz=16, num_updates=6870, lr=2.88411e-05, gnorm=5.54, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11374
2022-07-12 16:06:46 - progress_bar.py[line:274] - INFO: epoch 006:    486 / 1283 loss=0.688, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=55.5, nsentences=16, sample_size=55.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=44.9, ups=0.81, wpb=55.5, bsz=16, num_updates=6880, lr=2.88073e-05, gnorm=6.291, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11386
2022-07-12 16:06:58 - progress_bar.py[line:274] - INFO: epoch 006:    496 / 1283 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=54.3, nsentences=16, sample_size=54.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=43.8, ups=0.81, wpb=54.3, bsz=16, num_updates=6890, lr=2.87734e-05, gnorm=6.095, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11398
2022-07-12 16:07:10 - progress_bar.py[line:274] - INFO: epoch 006:    506 / 1283 loss=0.771, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=42.9, ups=0.81, wpb=52.7, bsz=16, num_updates=6900, lr=2.87396e-05, gnorm=9.782, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=11411
2022-07-12 16:07:23 - progress_bar.py[line:274] - INFO: epoch 006:    516 / 1283 loss=0.678, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=42.2, ups=0.81, wpb=52.2, bsz=16, num_updates=6910, lr=2.87058e-05, gnorm=8.668, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11423
2022-07-12 16:07:35 - progress_bar.py[line:274] - INFO: epoch 006:    526 / 1283 loss=0.703, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=41.2, ups=0.79, wpb=51.8, bsz=16, num_updates=6920, lr=2.86719e-05, gnorm=10.342, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=11436
2022-07-12 16:07:48 - progress_bar.py[line:274] - INFO: epoch 006:    536 / 1283 loss=0.686, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=43, ups=0.8, wpb=53.4, bsz=16, num_updates=6930, lr=2.86381e-05, gnorm=6.867, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11448
2022-07-12 16:08:00 - progress_bar.py[line:274] - INFO: epoch 006:    546 / 1283 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=56.1, nsentences=16, sample_size=56.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=45.4, ups=0.81, wpb=56.1, bsz=16, num_updates=6940, lr=2.86043e-05, gnorm=6.047, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11460
2022-07-12 16:08:12 - progress_bar.py[line:274] - INFO: epoch 006:    556 / 1283 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.1, ups=0.81, wpb=52.1, bsz=16, num_updates=6950, lr=2.85705e-05, gnorm=5.114, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11473
2022-07-12 16:08:25 - progress_bar.py[line:274] - INFO: epoch 006:    566 / 1283 loss=0.673, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=39.8, ups=0.8, wpb=49.9, bsz=16, num_updates=6960, lr=2.85366e-05, gnorm=6.152, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11485
2022-07-12 16:08:37 - progress_bar.py[line:274] - INFO: epoch 006:    576 / 1283 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.9, ups=0.8, wpb=52.3, bsz=16, num_updates=6970, lr=2.85028e-05, gnorm=4.679, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11498
2022-07-12 16:08:50 - progress_bar.py[line:274] - INFO: epoch 006:    586 / 1283 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.4, ups=0.8, wpb=53, bsz=16, num_updates=6980, lr=2.8469e-05, gnorm=5.324, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11510
2022-07-12 16:09:02 - progress_bar.py[line:274] - INFO: epoch 006:    596 / 1283 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42, ups=0.8, wpb=52.2, bsz=16, num_updates=6990, lr=2.84352e-05, gnorm=3.739, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11523
2022-07-12 16:09:15 - progress_bar.py[line:274] - INFO: epoch 006:    606 / 1283 loss=0.664, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=41.7, ups=0.81, wpb=51.5, bsz=16, num_updates=7000, lr=2.84013e-05, gnorm=6.869, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11535
2022-07-12 16:09:27 - progress_bar.py[line:274] - INFO: epoch 006:    616 / 1283 loss=0.719, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=41.8, ups=0.8, wpb=52.4, bsz=16, num_updates=7010, lr=2.83675e-05, gnorm=7.004, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=11548
2022-07-12 16:09:40 - progress_bar.py[line:274] - INFO: epoch 006:    626 / 1283 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=43.7, ups=0.81, wpb=53.6, bsz=16, num_updates=7020, lr=2.83337e-05, gnorm=3.688, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11560
2022-07-12 16:09:52 - progress_bar.py[line:274] - INFO: epoch 006:    636 / 1283 loss=0.697, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=41.1, ups=0.8, wpb=51.6, bsz=16, num_updates=7030, lr=2.82998e-05, gnorm=7.971, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=11572
2022-07-12 16:10:05 - progress_bar.py[line:274] - INFO: epoch 006:    646 / 1283 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=42.8, ups=0.81, wpb=53.2, bsz=16, num_updates=7040, lr=2.8266e-05, gnorm=4.492, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11585
2022-07-12 16:10:17 - progress_bar.py[line:274] - INFO: epoch 006:    656 / 1283 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=41.4, ups=0.8, wpb=51.5, bsz=16, num_updates=7050, lr=2.82322e-05, gnorm=8.911, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11597
2022-07-12 16:10:28 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 10.73 GiB total capacity; 8.45 GiB already allocated; 151.19 MiB free; 9.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 16:10:28 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8370 MB |    9223 MB |     834 TB |     834 TB |
|       from large pool |    8224 MB |    9018 MB |     828 TB |     828 TB |
|       from small pool |     145 MB |     207 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8370 MB |    9223 MB |     834 TB |     834 TB |
|       from large pool |    8224 MB |    9018 MB |     828 TB |     828 TB |
|       from small pool |     145 MB |     207 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9466 MB |    9530 MB |   42764 MB |   33298 MB |
|       from large pool |    9316 MB |    9316 MB |   40674 MB |   31358 MB |
|       from small pool |     150 MB |     214 MB |    2090 MB |    1940 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1095 MB |    3366 MB |     807 TB |     807 TB |
|       from large pool |    1091 MB |    3364 MB |     799 TB |     799 TB |
|       from small pool |       4 MB |      20 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  191133 K  |  191129 K  |
|       from large pool |     835    |     910    |   57394 K  |   57393 K  |
|       from small pool |    3218    |    3470    |  133739 K  |  133736 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  191133 K  |  191129 K  |
|       from large pool |     835    |     910    |   57394 K  |   57393 K  |
|       from small pool |    3218    |    3470    |  133739 K  |  133736 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     202    |    1443    |    1273    |
|       from large pool |      95    |      95    |     398    |     303    |
|       from small pool |      75    |     107    |    1045    |     970    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      52    |     198    |  104874 K  |  104874 K  |
|       from large pool |      40    |      77    |   28154 K  |   28154 K  |
|       from small pool |      12    |     139    |   76719 K  |   76719 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 16:10:28 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 16:10:30 - progress_bar.py[line:274] - INFO: epoch 006:    667 / 1283 loss=0.679, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=42, ups=0.78, wpb=53.8, bsz=16, num_updates=7060, lr=2.81984e-05, gnorm=7.998, clip=100, loss_scale=32, train_wall=12, gb_free=1.7, ema_decay=0.9999, wall=11610
2022-07-12 16:10:42 - progress_bar.py[line:274] - INFO: epoch 006:    677 / 1283 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=43.4, ups=0.81, wpb=53.6, bsz=16, num_updates=7070, lr=2.81645e-05, gnorm=6.877, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11622
2022-07-12 16:10:54 - progress_bar.py[line:274] - INFO: epoch 006:    687 / 1283 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.1, ups=0.81, wpb=51.8, bsz=16, num_updates=7080, lr=2.81307e-05, gnorm=5.563, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11635
2022-07-12 16:11:07 - progress_bar.py[line:274] - INFO: epoch 006:    697 / 1283 loss=0.664, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=40.4, ups=0.8, wpb=50.4, bsz=16, num_updates=7090, lr=2.80969e-05, gnorm=5.421, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11647
2022-07-12 16:11:19 - progress_bar.py[line:274] - INFO: epoch 006:    707 / 1283 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.1, ups=0.81, wpb=50.4, bsz=16, num_updates=7100, lr=2.80631e-05, gnorm=6.211, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11659
2022-07-12 16:11:31 - progress_bar.py[line:274] - INFO: epoch 006:    717 / 1283 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42.7, ups=0.81, wpb=52.6, bsz=16, num_updates=7110, lr=2.80292e-05, gnorm=4.324, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11672
2022-07-12 16:11:44 - progress_bar.py[line:274] - INFO: epoch 006:    727 / 1283 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.8, ups=0.81, wpb=53, bsz=16, num_updates=7120, lr=2.79954e-05, gnorm=3.694, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11684
2022-07-12 16:11:56 - progress_bar.py[line:274] - INFO: epoch 006:    737 / 1283 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=43.1, ups=0.81, wpb=53.3, bsz=16, num_updates=7130, lr=2.79616e-05, gnorm=4.567, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11696
2022-07-12 16:12:00 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-12 16:12:10 - progress_bar.py[line:274] - INFO: epoch 006:    748 / 1283 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=38.4, ups=0.74, wpb=51.8, bsz=16, num_updates=7140, lr=2.79277e-05, gnorm=5.373, clip=100, loss_scale=32, train_wall=13, gb_free=2.5, ema_decay=0.9999, wall=11710
2022-07-12 16:12:22 - progress_bar.py[line:274] - INFO: epoch 006:    758 / 1283 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=43, ups=0.81, wpb=53.2, bsz=16, num_updates=7150, lr=2.78939e-05, gnorm=4.739, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11722
2022-07-12 16:12:34 - progress_bar.py[line:274] - INFO: epoch 006:    768 / 1283 loss=0.716, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=42.4, ups=0.81, wpb=52, bsz=16, num_updates=7160, lr=2.78601e-05, gnorm=7.01, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11735
2022-07-12 16:12:47 - progress_bar.py[line:274] - INFO: epoch 006:    778 / 1283 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=40.9, ups=0.8, wpb=50.8, bsz=16, num_updates=7170, lr=2.78263e-05, gnorm=5.122, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11747
2022-07-12 16:12:59 - progress_bar.py[line:274] - INFO: epoch 006:    788 / 1283 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=55.2, nsentences=16, sample_size=55.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=44.7, ups=0.81, wpb=55.2, bsz=16, num_updates=7180, lr=2.77924e-05, gnorm=4.676, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11759
2022-07-12 16:13:11 - progress_bar.py[line:274] - INFO: epoch 006:    798 / 1283 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=43, ups=0.81, wpb=53, bsz=16, num_updates=7190, lr=2.77586e-05, gnorm=5.578, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11772
2022-07-12 16:13:24 - progress_bar.py[line:274] - INFO: epoch 006:    808 / 1283 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=48.9, nsentences=16, sample_size=48.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=39, ups=0.8, wpb=48.9, bsz=16, num_updates=7200, lr=2.77248e-05, gnorm=5.192, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=11784
2022-07-12 16:13:36 - progress_bar.py[line:274] - INFO: epoch 006:    818 / 1283 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.6, ups=0.8, wpb=53.2, bsz=16, num_updates=7210, lr=2.7691e-05, gnorm=5.476, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=11797
2022-07-12 16:13:49 - progress_bar.py[line:274] - INFO: epoch 006:    828 / 1283 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.3, ups=0.81, wpb=51, bsz=16, num_updates=7220, lr=2.76571e-05, gnorm=3.463, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11809
2022-07-12 16:14:01 - progress_bar.py[line:274] - INFO: epoch 006:    838 / 1283 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=40.7, ups=0.79, wpb=51.3, bsz=16, num_updates=7230, lr=2.76233e-05, gnorm=5.514, clip=100, loss_scale=32, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=11822
2022-07-12 16:14:14 - progress_bar.py[line:274] - INFO: epoch 006:    848 / 1283 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=42.1, ups=0.81, wpb=51.8, bsz=16, num_updates=7240, lr=2.75895e-05, gnorm=6.4, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11834
2022-07-12 16:14:26 - progress_bar.py[line:274] - INFO: epoch 006:    858 / 1283 loss=0.7, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=40.8, ups=0.81, wpb=50.5, bsz=16, num_updates=7250, lr=2.75556e-05, gnorm=5.732, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11846
2022-07-12 16:14:39 - progress_bar.py[line:274] - INFO: epoch 006:    868 / 1283 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.5, ups=0.8, wpb=51.8, bsz=16, num_updates=7260, lr=2.75218e-05, gnorm=5.177, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11859
2022-07-12 16:14:51 - progress_bar.py[line:274] - INFO: epoch 006:    878 / 1283 loss=0.714, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=41.7, ups=0.8, wpb=52, bsz=16, num_updates=7270, lr=2.7488e-05, gnorm=7.615, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11871
2022-07-12 16:15:03 - progress_bar.py[line:274] - INFO: epoch 006:    888 / 1283 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.9, ups=0.81, wpb=51.7, bsz=16, num_updates=7280, lr=2.74542e-05, gnorm=3.83, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11884
2022-07-12 16:15:16 - progress_bar.py[line:274] - INFO: epoch 006:    898 / 1283 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=43.7, ups=0.81, wpb=54.1, bsz=16, num_updates=7290, lr=2.74203e-05, gnorm=5.747, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11896
2022-07-12 16:15:28 - progress_bar.py[line:274] - INFO: epoch 006:    908 / 1283 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=43, ups=0.8, wpb=53.6, bsz=16, num_updates=7300, lr=2.73865e-05, gnorm=6.078, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11908
2022-07-12 16:15:41 - progress_bar.py[line:274] - INFO: epoch 006:    918 / 1283 loss=0.713, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=40.7, ups=0.81, wpb=50.5, bsz=16, num_updates=7310, lr=2.73527e-05, gnorm=7.28, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11921
2022-07-12 16:15:53 - progress_bar.py[line:274] - INFO: epoch 006:    928 / 1283 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=41, ups=0.8, wpb=50.9, bsz=16, num_updates=7320, lr=2.73189e-05, gnorm=5.348, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11933
2022-07-12 16:16:06 - progress_bar.py[line:274] - INFO: epoch 006:    938 / 1283 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42.1, ups=0.8, wpb=52.5, bsz=16, num_updates=7330, lr=2.7285e-05, gnorm=5.048, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=11946
2022-07-12 16:16:18 - progress_bar.py[line:274] - INFO: epoch 006:    948 / 1283 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.8, ups=0.81, wpb=51.9, bsz=16, num_updates=7340, lr=2.72512e-05, gnorm=4.44, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11958
2022-07-12 16:16:30 - progress_bar.py[line:274] - INFO: epoch 006:    958 / 1283 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42.9, ups=0.81, wpb=52.9, bsz=16, num_updates=7350, lr=2.72174e-05, gnorm=6.5, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=11970
2022-07-12 16:16:43 - progress_bar.py[line:274] - INFO: epoch 006:    968 / 1283 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.2, ups=0.81, wpb=52.3, bsz=16, num_updates=7360, lr=2.71835e-05, gnorm=4.907, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11983
2022-07-12 16:16:55 - progress_bar.py[line:274] - INFO: epoch 006:    978 / 1283 loss=0.692, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=40.8, ups=0.81, wpb=50.1, bsz=16, num_updates=7370, lr=2.71497e-05, gnorm=5.487, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=11995
2022-07-12 16:17:07 - progress_bar.py[line:274] - INFO: epoch 006:    988 / 1283 loss=0.702, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=40.4, ups=0.8, wpb=50.4, bsz=16, num_updates=7380, lr=2.71159e-05, gnorm=6.664, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=12008
2022-07-12 16:17:20 - progress_bar.py[line:274] - INFO: epoch 006:    998 / 1283 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42.7, ups=0.82, wpb=52.3, bsz=16, num_updates=7390, lr=2.70821e-05, gnorm=5.28, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=12020
2022-07-12 16:17:32 - progress_bar.py[line:274] - INFO: epoch 006:   1008 / 1283 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=39.2, ups=0.79, wpb=49.8, bsz=16, num_updates=7400, lr=2.70482e-05, gnorm=7.734, clip=100, loss_scale=32, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=12033
2022-07-12 16:17:45 - progress_bar.py[line:274] - INFO: epoch 006:   1018 / 1283 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.3, ups=0.82, wpb=50.5, bsz=16, num_updates=7410, lr=2.70144e-05, gnorm=5.08, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=12045
2022-07-12 16:17:57 - progress_bar.py[line:274] - INFO: epoch 006:   1028 / 1283 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=41.1, ups=0.81, wpb=50.7, bsz=16, num_updates=7420, lr=2.69806e-05, gnorm=5.493, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=12057
2022-07-12 16:18:09 - progress_bar.py[line:274] - INFO: epoch 006:   1038 / 1283 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=42.6, ups=0.8, wpb=53, bsz=16, num_updates=7430, lr=2.69468e-05, gnorm=6.749, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12070
2022-07-12 16:18:22 - progress_bar.py[line:274] - INFO: epoch 006:   1048 / 1283 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.6, ups=0.8, wpb=51.9, bsz=16, num_updates=7440, lr=2.69129e-05, gnorm=5.044, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12082
2022-07-12 16:18:34 - progress_bar.py[line:274] - INFO: epoch 006:   1058 / 1283 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=44.3, ups=0.81, wpb=54.4, bsz=16, num_updates=7450, lr=2.68791e-05, gnorm=5.308, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12094
2022-07-12 16:18:46 - progress_bar.py[line:274] - INFO: epoch 006:   1068 / 1283 loss=0.687, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=41.2, ups=0.81, wpb=50.8, bsz=16, num_updates=7460, lr=2.68453e-05, gnorm=6.431, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=12107
2022-07-12 16:18:59 - progress_bar.py[line:274] - INFO: epoch 006:   1078 / 1283 loss=0.701, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=41.9, ups=0.8, wpb=52.2, bsz=16, num_updates=7470, lr=2.68114e-05, gnorm=8.905, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12119
2022-07-12 16:19:11 - progress_bar.py[line:274] - INFO: epoch 006:   1088 / 1283 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=42.9, ups=0.81, wpb=52.8, bsz=16, num_updates=7480, lr=2.67776e-05, gnorm=6.662, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=12131
2022-07-12 16:19:24 - progress_bar.py[line:274] - INFO: epoch 006:   1098 / 1283 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.1, ups=0.81, wpb=50.8, bsz=16, num_updates=7490, lr=2.67438e-05, gnorm=5.429, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12144
2022-07-12 16:19:36 - progress_bar.py[line:274] - INFO: epoch 006:   1108 / 1283 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=39.7, ups=0.8, wpb=49.4, bsz=16, num_updates=7500, lr=2.671e-05, gnorm=5.629, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12156
2022-07-12 16:19:49 - progress_bar.py[line:274] - INFO: epoch 006:   1118 / 1283 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42, ups=0.8, wpb=52.6, bsz=16, num_updates=7510, lr=2.66761e-05, gnorm=5.031, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12169
2022-07-12 16:20:01 - progress_bar.py[line:274] - INFO: epoch 006:   1128 / 1283 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.4, ups=0.81, wpb=52.1, bsz=16, num_updates=7520, lr=2.66423e-05, gnorm=5.765, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12181
2022-07-12 16:20:13 - progress_bar.py[line:274] - INFO: epoch 006:   1138 / 1283 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=41, ups=0.8, wpb=51.2, bsz=16, num_updates=7530, lr=2.66085e-05, gnorm=5.033, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12194
2022-07-12 16:20:26 - progress_bar.py[line:274] - INFO: epoch 006:   1148 / 1283 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=42.4, ups=0.81, wpb=52.6, bsz=16, num_updates=7540, lr=2.65747e-05, gnorm=5.459, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12206
2022-07-12 16:20:38 - progress_bar.py[line:274] - INFO: epoch 006:   1158 / 1283 loss=0.665, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=42.8, ups=0.8, wpb=53.3, bsz=16, num_updates=7550, lr=2.65408e-05, gnorm=6.696, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=12218
2022-07-12 16:20:50 - progress_bar.py[line:274] - INFO: epoch 006:   1168 / 1283 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=41.6, ups=0.81, wpb=51.2, bsz=16, num_updates=7560, lr=2.6507e-05, gnorm=5.833, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12231
2022-07-12 16:21:03 - progress_bar.py[line:274] - INFO: epoch 006:   1178 / 1283 loss=0.661, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=41.8, ups=0.81, wpb=51.8, bsz=16, num_updates=7570, lr=2.64732e-05, gnorm=6.578, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12243
2022-07-12 16:21:15 - progress_bar.py[line:274] - INFO: epoch 006:   1188 / 1283 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=43.9, ups=0.81, wpb=54.4, bsz=16, num_updates=7580, lr=2.64393e-05, gnorm=4.536, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=12255
2022-07-12 16:21:28 - progress_bar.py[line:274] - INFO: epoch 006:   1198 / 1283 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=40.5, ups=0.8, wpb=50.5, bsz=16, num_updates=7590, lr=2.64055e-05, gnorm=7.068, clip=100, loss_scale=32, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=12268
2022-07-12 16:21:40 - progress_bar.py[line:274] - INFO: epoch 006:   1208 / 1283 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=40.8, ups=0.8, wpb=50.8, bsz=16, num_updates=7600, lr=2.63717e-05, gnorm=7.334, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=12280
2022-07-12 16:21:53 - progress_bar.py[line:274] - INFO: epoch 006:   1218 / 1283 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=40.8, ups=0.81, wpb=50.7, bsz=16, num_updates=7610, lr=2.63379e-05, gnorm=7.232, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=12293
2022-07-12 16:22:05 - progress_bar.py[line:274] - INFO: epoch 006:   1228 / 1283 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.4, ups=0.82, wpb=51.8, bsz=16, num_updates=7620, lr=2.6304e-05, gnorm=5.394, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=12305
2022-07-12 16:22:17 - progress_bar.py[line:274] - INFO: epoch 006:   1238 / 1283 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=49.1, nsentences=16, sample_size=49.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=39.4, ups=0.8, wpb=49.1, bsz=16, num_updates=7630, lr=2.62702e-05, gnorm=5.425, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=12317
2022-07-12 16:22:30 - progress_bar.py[line:274] - INFO: epoch 006:   1248 / 1283 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.5, ups=0.82, wpb=52, bsz=16, num_updates=7640, lr=2.62364e-05, gnorm=5.325, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12330
2022-07-12 16:22:42 - progress_bar.py[line:274] - INFO: epoch 006:   1258 / 1283 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.7, ups=0.8, wpb=52.3, bsz=16, num_updates=7650, lr=2.62026e-05, gnorm=5.007, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=12342
2022-07-12 16:22:55 - progress_bar.py[line:274] - INFO: epoch 006:   1268 / 1283 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42, ups=0.79, wpb=53.1, bsz=16, num_updates=7660, lr=2.61687e-05, gnorm=5.339, clip=100, loss_scale=64, train_wall=13, gb_free=2.5, ema_decay=0.9999, wall=12355
2022-07-12 16:23:07 - progress_bar.py[line:274] - INFO: epoch 006:   1278 / 1283 loss=0.68, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=48.3, nsentences=16, sample_size=48.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=38.9, ups=0.81, wpb=48.3, bsz=16, num_updates=7670, lr=2.61349e-05, gnorm=8.522, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12367
2022-07-12 16:23:13 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 16:31:15 - progress_bar.py[line:282] - INFO: epoch 006 | valid on 'valid' subset | loss 1.409 | loss_v1 0 | loss_v2 0 | nll_loss 1.127 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.18 | vqa_score 0.3063 | wps 30.5 | wpb 13.6 | bsz 4 | num_updates 7675 | best_vqa_score 0.3063
2022-07-12 16:31:15 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 6 @ 7675 updates
2022-07-12 16:31:15 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints6.pt
2022-07-12 16:31:47 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints6.pt
2022-07-12 16:32:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints6.pt (epoch 6 @ 7675 updates, score 0.3063) (writing took 88.64469858101802 seconds)
2022-07-12 16:32:44 - train.py[line:323] - INFO: end of epoch 6 (average epoch stats below)
2022-07-12 16:32:44 - progress_bar.py[line:282] - INFO: epoch 006 | loss 0.658 | loss_v1 0 | loss_v2 0 | nll_loss 0.231 | ntokens 51.944 | nsentences 15.996 | sample_size 51.944 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.17 | wps 30.7 | ups 0.59 | wpb 51.9 | bsz 16 | num_updates 7675 | lr 2.6118e-05 | gnorm 6.09 | clip 100 | loss_scale 64 | train_wall 1580 | gb_free 2.3 | ema_decay 0.9999 | wall 12944
2022-07-12 16:32:44 - trainer.py[line:639] - INFO: loading train data for epoch 7
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 16:32:46 - trainer.py[line:703] - INFO: begin training epoch 7
2022-07-12 16:32:46 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 16:32:52 - progress_bar.py[line:274] - INFO: epoch 007:      5 / 1283 loss=0.696, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=47.5, nsentences=15.6, sample_size=47.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=0.8, ups=0.02, wpb=47.5, bsz=15.6, num_updates=7680, lr=2.61011e-05, gnorm=7.248, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12953
2022-07-12 16:33:05 - progress_bar.py[line:274] - INFO: epoch 007:     15 / 1283 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42.2, ups=0.81, wpb=52.2, bsz=16, num_updates=7690, lr=2.60672e-05, gnorm=5.098, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12965
2022-07-12 16:33:17 - progress_bar.py[line:274] - INFO: epoch 007:     25 / 1283 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.1, ups=0.81, wpb=52, bsz=16, num_updates=7700, lr=2.60334e-05, gnorm=3.938, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=12977
2022-07-12 16:33:24 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 10.73 GiB total capacity; 8.41 GiB already allocated; 176.81 MiB free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 16:33:24 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8339 MB |    9123 MB |     922 TB |     922 TB |
|       from large pool |    8194 MB |    8921 MB |     914 TB |     914 TB |
|       from small pool |     145 MB |     204 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8339 MB |    9123 MB |     922 TB |     922 TB |
|       from large pool |    8194 MB |    8921 MB |     914 TB |     914 TB |
|       from small pool |     145 MB |     204 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9440 MB |    9504 MB |   44080 MB |   34640 MB |
|       from large pool |    9290 MB |    9290 MB |   41770 MB |   32480 MB |
|       from small pool |     150 MB |     214 MB |    2310 MB |    2160 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1100 MB |    3313 MB |     890 TB |     890 TB |
|       from large pool |    1095 MB |    3311 MB |     881 TB |     881 TB |
|       from small pool |       4 MB |      24 MB |       8 TB |       8 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4055    |    4378    |  216697 K  |  216693 K  |
|       from large pool |     835    |     904    |   63446 K  |   63445 K  |
|       from small pool |    3220    |    3494    |  153251 K  |  153248 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4055    |    4378    |  216697 K  |  216693 K  |
|       from large pool |     835    |     904    |   63446 K  |   63445 K  |
|       from small pool |    3220    |    3494    |  153251 K  |  153248 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     174    |     206    |    1560    |    1386    |
|       from large pool |      99    |      99    |     405    |     306    |
|       from small pool |      75    |     107    |    1155    |    1080    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      50    |     197    |  119740 K  |  119740 K  |
|       from large pool |      38    |      78    |   31142 K  |   31142 K  |
|       from small pool |      12    |     146    |   88598 K  |   88598 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 16:33:24 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 16:33:30 - progress_bar.py[line:274] - INFO: epoch 007:     36 / 1283 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=41.9, ups=0.77, wpb=54.6, bsz=16, num_updates=7710, lr=2.59996e-05, gnorm=6.116, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=12991
2022-07-12 16:33:43 - progress_bar.py[line:274] - INFO: epoch 007:     46 / 1283 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.7, ups=0.81, wpb=52.5, bsz=16, num_updates=7720, lr=2.59658e-05, gnorm=4.66, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13003
2022-07-12 16:33:55 - progress_bar.py[line:274] - INFO: epoch 007:     56 / 1283 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=40.9, ups=0.81, wpb=50.3, bsz=16, num_updates=7730, lr=2.59319e-05, gnorm=6.277, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13015
2022-07-12 16:34:07 - progress_bar.py[line:274] - INFO: epoch 007:     66 / 1283 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.6, ups=0.8, wpb=52.1, bsz=16, num_updates=7740, lr=2.58981e-05, gnorm=5.301, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13028
2022-07-12 16:34:20 - progress_bar.py[line:274] - INFO: epoch 007:     76 / 1283 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43.3, ups=0.81, wpb=53.1, bsz=16, num_updates=7750, lr=2.58643e-05, gnorm=5.477, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13040
2022-07-12 16:34:32 - progress_bar.py[line:274] - INFO: epoch 007:     86 / 1283 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=41.6, ups=0.81, wpb=51.3, bsz=16, num_updates=7760, lr=2.58305e-05, gnorm=6.271, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13052
2022-07-12 16:34:44 - progress_bar.py[line:274] - INFO: epoch 007:     96 / 1283 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.3, ups=0.81, wpb=52, bsz=16, num_updates=7770, lr=2.57966e-05, gnorm=4.404, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13064
2022-07-12 16:34:57 - progress_bar.py[line:274] - INFO: epoch 007:    106 / 1283 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.7, ups=0.8, wpb=52.1, bsz=16, num_updates=7780, lr=2.57628e-05, gnorm=5.557, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13077
2022-07-12 16:35:09 - progress_bar.py[line:274] - INFO: epoch 007:    116 / 1283 loss=0.681, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=39.7, ups=0.8, wpb=49.4, bsz=16, num_updates=7790, lr=2.5729e-05, gnorm=5.23, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=13089
2022-07-12 16:35:21 - progress_bar.py[line:274] - INFO: epoch 007:    126 / 1283 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=43, ups=0.82, wpb=52.6, bsz=16, num_updates=7800, lr=2.56951e-05, gnorm=4.661, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13102
2022-07-12 16:35:34 - progress_bar.py[line:274] - INFO: epoch 007:    136 / 1283 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=41.1, ups=0.81, wpb=51, bsz=16, num_updates=7810, lr=2.56613e-05, gnorm=5.999, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13114
2022-07-12 16:35:46 - progress_bar.py[line:274] - INFO: epoch 007:    146 / 1283 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=43.4, ups=0.81, wpb=53.4, bsz=16, num_updates=7820, lr=2.56275e-05, gnorm=6.562, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=13126
2022-07-12 16:35:58 - progress_bar.py[line:274] - INFO: epoch 007:    156 / 1283 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=43.5, ups=0.82, wpb=53.2, bsz=16, num_updates=7830, lr=2.55937e-05, gnorm=5.948, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=13139
2022-07-12 16:36:11 - progress_bar.py[line:274] - INFO: epoch 007:    166 / 1283 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=43.6, ups=0.81, wpb=53.7, bsz=16, num_updates=7840, lr=2.55598e-05, gnorm=8.622, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13151
2022-07-12 16:36:23 - progress_bar.py[line:274] - INFO: epoch 007:    176 / 1283 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=40.6, ups=0.81, wpb=49.9, bsz=16, num_updates=7850, lr=2.5526e-05, gnorm=6.691, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13163
2022-07-12 16:36:35 - progress_bar.py[line:274] - INFO: epoch 007:    186 / 1283 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=40.4, ups=0.8, wpb=50.3, bsz=16, num_updates=7860, lr=2.54922e-05, gnorm=6.394, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13176
2022-07-12 16:36:48 - progress_bar.py[line:274] - INFO: epoch 007:    196 / 1283 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.9, ups=0.81, wpb=52.8, bsz=16, num_updates=7870, lr=2.54584e-05, gnorm=4.931, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13188
2022-07-12 16:37:00 - progress_bar.py[line:274] - INFO: epoch 007:    206 / 1283 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.3, ups=0.82, wpb=51.8, bsz=16, num_updates=7880, lr=2.54245e-05, gnorm=5.581, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13200
2022-07-12 16:37:12 - progress_bar.py[line:274] - INFO: epoch 007:    216 / 1283 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.7, ups=0.81, wpb=52.7, bsz=16, num_updates=7890, lr=2.53907e-05, gnorm=5.355, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13213
2022-07-12 16:37:25 - progress_bar.py[line:274] - INFO: epoch 007:    226 / 1283 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.5, ups=0.81, wpb=52.7, bsz=16, num_updates=7900, lr=2.53569e-05, gnorm=7.355, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=13225
2022-07-12 16:37:37 - progress_bar.py[line:274] - INFO: epoch 007:    236 / 1283 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.6, ups=0.8, wpb=52, bsz=16, num_updates=7910, lr=2.5323e-05, gnorm=5.053, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=13237
2022-07-12 16:37:50 - progress_bar.py[line:274] - INFO: epoch 007:    246 / 1283 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42, ups=0.81, wpb=51.9, bsz=16, num_updates=7920, lr=2.52892e-05, gnorm=5.394, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13250
2022-07-12 16:38:02 - progress_bar.py[line:274] - INFO: epoch 007:    256 / 1283 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=43.3, ups=0.82, wpb=53.1, bsz=16, num_updates=7930, lr=2.52554e-05, gnorm=5.207, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13262
2022-07-12 16:38:14 - progress_bar.py[line:274] - INFO: epoch 007:    266 / 1283 loss=0.673, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=40.5, ups=0.82, wpb=49.5, bsz=16, num_updates=7940, lr=2.52216e-05, gnorm=7.383, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13274
2022-07-12 16:38:27 - progress_bar.py[line:274] - INFO: epoch 007:    276 / 1283 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=40.7, ups=0.79, wpb=51.3, bsz=16, num_updates=7950, lr=2.51877e-05, gnorm=5.428, clip=100, loss_scale=64, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=13287
2022-07-12 16:38:39 - progress_bar.py[line:274] - INFO: epoch 007:    286 / 1283 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.8, ups=0.81, wpb=51.7, bsz=16, num_updates=7960, lr=2.51539e-05, gnorm=6.004, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13299
2022-07-12 16:38:51 - progress_bar.py[line:274] - INFO: epoch 007:    296 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.9, ups=0.81, wpb=51.7, bsz=16, num_updates=7970, lr=2.51201e-05, gnorm=3.991, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13312
2022-07-12 16:39:04 - progress_bar.py[line:274] - INFO: epoch 007:    306 / 1283 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=43.2, ups=0.81, wpb=53.5, bsz=16, num_updates=7980, lr=2.50863e-05, gnorm=4.697, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13324
2022-07-12 16:39:16 - progress_bar.py[line:274] - INFO: epoch 007:    316 / 1283 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.8, ups=0.81, wpb=53.1, bsz=16, num_updates=7990, lr=2.50524e-05, gnorm=4.189, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13336
2022-07-12 16:39:29 - progress_bar.py[line:274] - INFO: epoch 007:    326 / 1283 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.2, ups=0.8, wpb=53, bsz=16, num_updates=8000, lr=2.50186e-05, gnorm=5.53, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=13349
2022-07-12 16:39:41 - progress_bar.py[line:274] - INFO: epoch 007:    336 / 1283 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=41.7, ups=0.8, wpb=51.8, bsz=16, num_updates=8010, lr=2.49848e-05, gnorm=5.939, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=13361
2022-07-12 16:39:54 - progress_bar.py[line:274] - INFO: epoch 007:    346 / 1283 loss=0.668, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=40.3, ups=0.81, wpb=49.9, bsz=16, num_updates=8020, lr=2.4951e-05, gnorm=5.552, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13374
2022-07-12 16:40:06 - progress_bar.py[line:274] - INFO: epoch 007:    356 / 1283 loss=0.664, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=49.3, nsentences=16, sample_size=49.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=40.4, ups=0.82, wpb=49.3, bsz=16, num_updates=8030, lr=2.49171e-05, gnorm=6.183, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13386
2022-07-12 16:40:18 - progress_bar.py[line:274] - INFO: epoch 007:    366 / 1283 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42, ups=0.81, wpb=51.9, bsz=16, num_updates=8040, lr=2.48833e-05, gnorm=5.896, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13398
2022-07-12 16:40:31 - progress_bar.py[line:274] - INFO: epoch 007:    376 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=40.4, ups=0.8, wpb=50.6, bsz=16, num_updates=8050, lr=2.48495e-05, gnorm=4.384, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=13411
2022-07-12 16:40:43 - progress_bar.py[line:274] - INFO: epoch 007:    386 / 1283 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=42.8, ups=0.81, wpb=53, bsz=16, num_updates=8060, lr=2.48156e-05, gnorm=6.424, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=13423
2022-07-12 16:40:55 - progress_bar.py[line:274] - INFO: epoch 007:    396 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=44.6, ups=0.81, wpb=55.1, bsz=16, num_updates=8070, lr=2.47818e-05, gnorm=4.335, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13436
2022-07-12 16:41:08 - progress_bar.py[line:274] - INFO: epoch 007:    406 / 1283 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=41.7, ups=0.81, wpb=51.7, bsz=16, num_updates=8080, lr=2.4748e-05, gnorm=6.103, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=13448
2022-07-12 16:41:20 - progress_bar.py[line:274] - INFO: epoch 007:    416 / 1283 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.3, ups=0.82, wpb=53, bsz=16, num_updates=8090, lr=2.47142e-05, gnorm=3.17, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13460
2022-07-12 16:41:33 - progress_bar.py[line:274] - INFO: epoch 007:    426 / 1283 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=48.4, nsentences=16, sample_size=48.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=38.9, ups=0.8, wpb=48.4, bsz=16, num_updates=8100, lr=2.46803e-05, gnorm=10.087, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13473
2022-07-12 16:41:45 - progress_bar.py[line:274] - INFO: epoch 007:    436 / 1283 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.7, ups=0.8, wpb=52, bsz=16, num_updates=8110, lr=2.46465e-05, gnorm=7.522, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13485
2022-07-12 16:41:57 - progress_bar.py[line:274] - INFO: epoch 007:    446 / 1283 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.3, ups=0.8, wpb=52.8, bsz=16, num_updates=8120, lr=2.46127e-05, gnorm=4.255, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13498
2022-07-12 16:42:10 - progress_bar.py[line:274] - INFO: epoch 007:    456 / 1283 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=39.7, ups=0.8, wpb=49.5, bsz=16, num_updates=8130, lr=2.45789e-05, gnorm=6.841, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13510
2022-07-12 16:42:22 - progress_bar.py[line:274] - INFO: epoch 007:    466 / 1283 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=44.3, ups=0.81, wpb=54.6, bsz=16, num_updates=8140, lr=2.4545e-05, gnorm=4.853, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13522
2022-07-12 16:42:34 - progress_bar.py[line:274] - INFO: epoch 007:    476 / 1283 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=49.4, nsentences=15.9, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.5, ups=0.82, wpb=49.4, bsz=15.9, num_updates=8150, lr=2.45112e-05, gnorm=5.519, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13535
2022-07-12 16:42:47 - progress_bar.py[line:274] - INFO: epoch 007:    486 / 1283 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=55.5, nsentences=16, sample_size=55.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=45, ups=0.81, wpb=55.5, bsz=16, num_updates=8160, lr=2.44774e-05, gnorm=6.908, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13547
2022-07-12 16:42:59 - progress_bar.py[line:274] - INFO: epoch 007:    496 / 1283 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=54.3, nsentences=16, sample_size=54.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=43.7, ups=0.81, wpb=54.3, bsz=16, num_updates=8170, lr=2.44435e-05, gnorm=4.582, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13559
2022-07-12 16:43:11 - progress_bar.py[line:274] - INFO: epoch 007:    506 / 1283 loss=0.665, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=43, ups=0.82, wpb=52.7, bsz=16, num_updates=8180, lr=2.44097e-05, gnorm=6.577, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=13572
2022-07-12 16:43:24 - progress_bar.py[line:274] - INFO: epoch 007:    516 / 1283 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42, ups=0.8, wpb=52.2, bsz=16, num_updates=8190, lr=2.43759e-05, gnorm=5.59, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13584
2022-07-12 16:43:36 - progress_bar.py[line:274] - INFO: epoch 007:    526 / 1283 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.3, ups=0.8, wpb=51.8, bsz=16, num_updates=8200, lr=2.43421e-05, gnorm=5.925, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13597
2022-07-12 16:43:49 - progress_bar.py[line:274] - INFO: epoch 007:    536 / 1283 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=43, ups=0.81, wpb=53.4, bsz=16, num_updates=8210, lr=2.43082e-05, gnorm=5.582, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13609
2022-07-12 16:44:01 - progress_bar.py[line:274] - INFO: epoch 007:    546 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=56.1, nsentences=16, sample_size=56.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=45.2, ups=0.81, wpb=56.1, bsz=16, num_updates=8220, lr=2.42744e-05, gnorm=5.698, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13622
2022-07-12 16:44:14 - progress_bar.py[line:274] - INFO: epoch 007:    556 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.1, ups=0.81, wpb=52.1, bsz=16, num_updates=8230, lr=2.42406e-05, gnorm=3.918, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13634
2022-07-12 16:44:26 - progress_bar.py[line:274] - INFO: epoch 007:    566 / 1283 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=39.9, ups=0.8, wpb=49.9, bsz=16, num_updates=8240, lr=2.42068e-05, gnorm=5.548, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13646
2022-07-12 16:44:39 - progress_bar.py[line:274] - INFO: epoch 007:    576 / 1283 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.7, ups=0.8, wpb=52.3, bsz=16, num_updates=8250, lr=2.41729e-05, gnorm=4.355, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13659
2022-07-12 16:44:51 - progress_bar.py[line:274] - INFO: epoch 007:    586 / 1283 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43.1, ups=0.81, wpb=53, bsz=16, num_updates=8260, lr=2.41391e-05, gnorm=4.578, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13671
2022-07-12 16:45:03 - progress_bar.py[line:274] - INFO: epoch 007:    596 / 1283 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42.1, ups=0.81, wpb=52.2, bsz=16, num_updates=8270, lr=2.41053e-05, gnorm=4.888, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13684
2022-07-12 16:45:16 - progress_bar.py[line:274] - INFO: epoch 007:    606 / 1283 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.8, ups=0.81, wpb=51.5, bsz=16, num_updates=8280, lr=2.40714e-05, gnorm=6.006, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13696
2022-07-12 16:45:28 - progress_bar.py[line:274] - INFO: epoch 007:    616 / 1283 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=41.9, ups=0.8, wpb=52.4, bsz=16, num_updates=8290, lr=2.40376e-05, gnorm=5.951, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13708
2022-07-12 16:45:40 - progress_bar.py[line:274] - INFO: epoch 007:    626 / 1283 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.6, ups=0.81, wpb=53.6, bsz=16, num_updates=8300, lr=2.40038e-05, gnorm=4.792, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13721
2022-07-12 16:45:53 - progress_bar.py[line:274] - INFO: epoch 007:    636 / 1283 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.2, ups=0.8, wpb=51.6, bsz=16, num_updates=8310, lr=2.397e-05, gnorm=5.315, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13733
2022-07-12 16:46:05 - progress_bar.py[line:274] - INFO: epoch 007:    646 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.9, ups=0.81, wpb=53.2, bsz=16, num_updates=8320, lr=2.39361e-05, gnorm=4.739, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13746
2022-07-12 16:46:18 - progress_bar.py[line:274] - INFO: epoch 007:    656 / 1283 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.7, ups=0.81, wpb=51.5, bsz=16, num_updates=8330, lr=2.39023e-05, gnorm=4.374, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13758
2022-07-12 16:46:29 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 10.73 GiB total capacity; 8.21 GiB already allocated; 77.00 MiB free; 9.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 16:46:29 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 46        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8404 MB |    8491 MB |     988 TB |     988 TB |
|       from large pool |    8258 MB |    8345 MB |     979 TB |     979 TB |
|       from small pool |     145 MB |     146 MB |       8 TB |       8 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8404 MB |    8491 MB |     988 TB |     988 TB |
|       from large pool |    8258 MB |    8345 MB |     979 TB |     979 TB |
|       from small pool |     145 MB |     146 MB |       8 TB |       8 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9540 MB |    9594 MB |   44950 MB |   35410 MB |
|       from large pool |    9392 MB |    9392 MB |   42454 MB |   33062 MB |
|       from small pool |     148 MB |     202 MB |    2496 MB |    2348 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1135 MB |    3376 MB |     957 TB |     957 TB |
|       from large pool |    1133 MB |    3372 MB |     948 TB |     948 TB |
|       from small pool |       2 MB |      22 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4057    |    4058    |  227214 K  |  227210 K  |
|       from large pool |     841    |     842    |   67933 K  |   67932 K  |
|       from small pool |    3216    |    3216    |  159281 K  |  159278 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4057    |    4058    |  227214 K  |  227210 K  |
|       from large pool |     841    |     842    |   67933 K  |   67932 K  |
|       from small pool |    3216    |    3216    |  159281 K  |  159278 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     173    |     200    |    1657    |    1484    |
|       from large pool |      99    |      99    |     409    |     310    |
|       from small pool |      74    |     101    |    1248    |    1174    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |     199    |  125004 K  |  125004 K  |
|       from large pool |      42    |      77    |   33254 K  |   33254 K  |
|       from small pool |       5    |     143    |   91749 K  |   91749 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 16:46:29 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 16:46:31 - progress_bar.py[line:274] - INFO: epoch 007:    667 / 1283 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.9, ups=0.78, wpb=53.8, bsz=16, num_updates=8340, lr=2.38685e-05, gnorm=5.088, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13771
2022-07-12 16:46:43 - progress_bar.py[line:274] - INFO: epoch 007:    677 / 1283 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.3, ups=0.81, wpb=53.6, bsz=16, num_updates=8350, lr=2.38347e-05, gnorm=4.526, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13783
2022-07-12 16:46:55 - progress_bar.py[line:274] - INFO: epoch 007:    687 / 1283 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.2, ups=0.81, wpb=51.8, bsz=16, num_updates=8360, lr=2.38008e-05, gnorm=4.226, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13795
2022-07-12 16:47:08 - progress_bar.py[line:274] - INFO: epoch 007:    697 / 1283 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=40.7, ups=0.81, wpb=50.4, bsz=16, num_updates=8370, lr=2.3767e-05, gnorm=4.291, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13808
2022-07-12 16:47:20 - progress_bar.py[line:274] - INFO: epoch 007:    707 / 1283 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.3, ups=0.82, wpb=50.4, bsz=16, num_updates=8380, lr=2.37332e-05, gnorm=4.212, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13820
2022-07-12 16:47:32 - progress_bar.py[line:274] - INFO: epoch 007:    717 / 1283 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.8, ups=0.81, wpb=52.6, bsz=16, num_updates=8390, lr=2.36993e-05, gnorm=3.894, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13832
2022-07-12 16:47:45 - progress_bar.py[line:274] - INFO: epoch 007:    727 / 1283 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.8, ups=0.81, wpb=53, bsz=16, num_updates=8400, lr=2.36655e-05, gnorm=3.836, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=13845
2022-07-12 16:47:57 - progress_bar.py[line:274] - INFO: epoch 007:    737 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43, ups=0.81, wpb=53.3, bsz=16, num_updates=8410, lr=2.36317e-05, gnorm=3.887, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13857
2022-07-12 16:48:09 - progress_bar.py[line:274] - INFO: epoch 007:    747 / 1283 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=8420, lr=2.35979e-05, gnorm=7.227, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=13869
2022-07-12 16:48:22 - progress_bar.py[line:274] - INFO: epoch 007:    757 / 1283 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.2, ups=0.81, wpb=52, bsz=16, num_updates=8430, lr=2.3564e-05, gnorm=3.729, clip=90, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=13882
2022-07-12 16:48:34 - progress_bar.py[line:274] - INFO: epoch 007:    767 / 1283 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=43, ups=0.81, wpb=53, bsz=16, num_updates=8440, lr=2.35302e-05, gnorm=5.417, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13894
2022-07-12 16:48:46 - progress_bar.py[line:274] - INFO: epoch 007:    777 / 1283 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=40.6, ups=0.8, wpb=50.5, bsz=16, num_updates=8450, lr=2.34964e-05, gnorm=5.076, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=13907
2022-07-12 16:48:57 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 178.00 MiB (GPU 0; 10.73 GiB total capacity; 8.24 GiB already allocated; 74.62 MiB free; 9.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 16:48:57 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8434 MB |    8522 MB |    1000 TB |    1000 TB |
|       from large pool |    8288 MB |    8377 MB |     992 TB |     992 TB |
|       from small pool |     145 MB |     146 MB |       8 TB |       8 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8434 MB |    8522 MB |    1000 TB |    1000 TB |
|       from large pool |    8288 MB |    8377 MB |     992 TB |     992 TB |
|       from small pool |     145 MB |     146 MB |       8 TB |       8 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9542 MB |    9600 MB |   45528 MB |   35986 MB |
|       from large pool |    9394 MB |    9394 MB |   42966 MB |   33572 MB |
|       from small pool |     148 MB |     206 MB |    2562 MB |    2414 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1107 MB |    3382 MB |     970 TB |     970 TB |
|       from large pool |    1105 MB |    3376 MB |     961 TB |     961 TB |
|       from small pool |       2 MB |      26 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4057    |    4058    |  229192 K  |  229188 K  |
|       from large pool |     841    |     842    |   68777 K  |   68776 K  |
|       from small pool |    3216    |    3216    |  160415 K  |  160411 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4057    |    4058    |  229192 K  |  229188 K  |
|       from large pool |     841    |     842    |   68777 K  |   68776 K  |
|       from small pool |    3216    |    3216    |  160415 K  |  160411 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     173    |     202    |    1693    |    1520    |
|       from large pool |      99    |      99    |     412    |     313    |
|       from small pool |      74    |     103    |    1281    |    1207    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      45    |     219    |  125996 K  |  125996 K  |
|       from large pool |      36    |      76    |   33653 K  |   33653 K  |
|       from small pool |       9    |     159    |   92343 K  |   92343 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 16:48:57 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 16:48:59 - progress_bar.py[line:274] - INFO: epoch 007:    788 / 1283 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=43.3, ups=0.79, wpb=55.1, bsz=16, num_updates=8460, lr=2.34626e-05, gnorm=5.771, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13919
2022-07-12 16:49:11 - progress_bar.py[line:274] - INFO: epoch 007:    798 / 1283 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.6, ups=0.8, wpb=53, bsz=16, num_updates=8470, lr=2.34287e-05, gnorm=4.673, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13932
2022-07-12 16:49:24 - progress_bar.py[line:274] - INFO: epoch 007:    808 / 1283 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=48.9, nsentences=16, sample_size=48.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=39.3, ups=0.8, wpb=48.9, bsz=16, num_updates=8480, lr=2.33949e-05, gnorm=4.968, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=13944
2022-07-12 16:49:36 - progress_bar.py[line:274] - INFO: epoch 007:    818 / 1283 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.7, ups=0.8, wpb=53.2, bsz=16, num_updates=8490, lr=2.33611e-05, gnorm=4.967, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=13957
2022-07-12 16:49:49 - progress_bar.py[line:274] - INFO: epoch 007:    828 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.3, ups=0.81, wpb=51, bsz=16, num_updates=8500, lr=2.33272e-05, gnorm=4.397, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13969
2022-07-12 16:50:01 - progress_bar.py[line:274] - INFO: epoch 007:    838 / 1283 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.6, ups=0.79, wpb=51.3, bsz=16, num_updates=8510, lr=2.32934e-05, gnorm=5.917, clip=100, loss_scale=128, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=13982
2022-07-12 16:50:14 - progress_bar.py[line:274] - INFO: epoch 007:    848 / 1283 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.1, ups=0.81, wpb=51.8, bsz=16, num_updates=8520, lr=2.32596e-05, gnorm=4.123, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=13994
2022-07-12 16:50:26 - progress_bar.py[line:274] - INFO: epoch 007:    858 / 1283 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.5, ups=0.8, wpb=50.5, bsz=16, num_updates=8530, lr=2.32258e-05, gnorm=5.579, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14006
2022-07-12 16:50:39 - progress_bar.py[line:274] - INFO: epoch 007:    868 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.1, ups=0.79, wpb=51.8, bsz=16, num_updates=8540, lr=2.31919e-05, gnorm=5.405, clip=100, loss_scale=128, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=14019
2022-07-12 16:50:51 - progress_bar.py[line:274] - INFO: epoch 007:    878 / 1283 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=42, ups=0.81, wpb=52, bsz=16, num_updates=8550, lr=2.31581e-05, gnorm=6.055, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14031
2022-07-12 16:51:04 - progress_bar.py[line:274] - INFO: epoch 007:    888 / 1283 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.7, ups=0.81, wpb=51.7, bsz=16, num_updates=8560, lr=2.31243e-05, gnorm=5.299, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14044
2022-07-12 16:51:16 - progress_bar.py[line:274] - INFO: epoch 007:    898 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=43.5, ups=0.8, wpb=54.1, bsz=16, num_updates=8570, lr=2.30905e-05, gnorm=4.093, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14056
2022-07-12 16:51:28 - progress_bar.py[line:274] - INFO: epoch 007:    908 / 1283 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43, ups=0.8, wpb=53.6, bsz=16, num_updates=8580, lr=2.30566e-05, gnorm=4.074, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14069
2022-07-12 16:51:41 - progress_bar.py[line:274] - INFO: epoch 007:    918 / 1283 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=40.6, ups=0.8, wpb=50.5, bsz=16, num_updates=8590, lr=2.30228e-05, gnorm=4.984, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14081
2022-07-12 16:51:53 - progress_bar.py[line:274] - INFO: epoch 007:    928 / 1283 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=40.7, ups=0.8, wpb=50.9, bsz=16, num_updates=8600, lr=2.2989e-05, gnorm=4.358, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14094
2022-07-12 16:52:06 - progress_bar.py[line:274] - INFO: epoch 007:    938 / 1283 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.3, ups=0.81, wpb=52.5, bsz=16, num_updates=8610, lr=2.29551e-05, gnorm=4.005, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14106
2022-07-12 16:52:18 - progress_bar.py[line:274] - INFO: epoch 007:    948 / 1283 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.9, ups=0.81, wpb=51.9, bsz=16, num_updates=8620, lr=2.29213e-05, gnorm=5.773, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14118
2022-07-12 16:52:31 - progress_bar.py[line:274] - INFO: epoch 007:    958 / 1283 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.8, ups=0.81, wpb=52.9, bsz=16, num_updates=8630, lr=2.28875e-05, gnorm=4.94, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=14131
2022-07-12 16:52:43 - progress_bar.py[line:274] - INFO: epoch 007:    968 / 1283 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.1, ups=0.81, wpb=52.3, bsz=16, num_updates=8640, lr=2.28537e-05, gnorm=5.514, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14143
2022-07-12 16:52:55 - progress_bar.py[line:274] - INFO: epoch 007:    978 / 1283 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=40.8, ups=0.81, wpb=50.1, bsz=16, num_updates=8650, lr=2.28198e-05, gnorm=5.443, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14155
2022-07-12 16:53:08 - progress_bar.py[line:274] - INFO: epoch 007:    988 / 1283 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=40.3, ups=0.8, wpb=50.4, bsz=16, num_updates=8660, lr=2.2786e-05, gnorm=4.648, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=14168
2022-07-12 16:53:20 - progress_bar.py[line:274] - INFO: epoch 007:    998 / 1283 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.1, ups=0.81, wpb=52.3, bsz=16, num_updates=8670, lr=2.27522e-05, gnorm=3.71, clip=100, loss_scale=256, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14180
2022-07-12 16:53:28 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2022-07-12 16:53:34 - progress_bar.py[line:274] - INFO: epoch 007:   1009 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=36.3, ups=0.73, wpb=50, bsz=16, num_updates=8680, lr=2.27184e-05, gnorm=4.226, clip=100, loss_scale=128, train_wall=14, gb_free=2.3, ema_decay=0.9999, wall=14194
2022-07-12 16:53:46 - progress_bar.py[line:274] - INFO: epoch 007:   1019 / 1283 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.1, ups=0.82, wpb=50.2, bsz=16, num_updates=8690, lr=2.26845e-05, gnorm=3.954, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14206
2022-07-12 16:53:59 - progress_bar.py[line:274] - INFO: epoch 007:   1029 / 1283 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.3, ups=0.81, wpb=52.1, bsz=16, num_updates=8700, lr=2.26507e-05, gnorm=3.698, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=14219
2022-07-12 16:54:11 - progress_bar.py[line:274] - INFO: epoch 007:   1039 / 1283 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=42.2, ups=0.81, wpb=52, bsz=16, num_updates=8710, lr=2.26169e-05, gnorm=6.515, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14231
2022-07-12 16:54:23 - progress_bar.py[line:274] - INFO: epoch 007:   1049 / 1283 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.7, ups=0.8, wpb=52.1, bsz=16, num_updates=8720, lr=2.2583e-05, gnorm=5.728, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14244
2022-07-12 16:54:36 - progress_bar.py[line:274] - INFO: epoch 007:   1059 / 1283 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=55, nsentences=16, sample_size=55, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=44.6, ups=0.81, wpb=55, bsz=16, num_updates=8730, lr=2.25492e-05, gnorm=4.967, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14256
2022-07-12 16:54:48 - progress_bar.py[line:274] - INFO: epoch 007:   1069 / 1283 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=40.9, ups=0.81, wpb=50.4, bsz=16, num_updates=8740, lr=2.25154e-05, gnorm=6.033, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14268
2022-07-12 16:55:01 - progress_bar.py[line:274] - INFO: epoch 007:   1079 / 1283 loss=0.676, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=41.9, ups=0.8, wpb=52.5, bsz=16, num_updates=8750, lr=2.24816e-05, gnorm=6.747, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14281
2022-07-12 16:55:13 - progress_bar.py[line:274] - INFO: epoch 007:   1089 / 1283 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=42.6, ups=0.82, wpb=52.2, bsz=16, num_updates=8760, lr=2.24477e-05, gnorm=6.185, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14293
2022-07-12 16:55:25 - progress_bar.py[line:274] - INFO: epoch 007:   1099 / 1283 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41, ups=0.81, wpb=50.7, bsz=16, num_updates=8770, lr=2.24139e-05, gnorm=5.542, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14305
2022-07-12 16:55:38 - progress_bar.py[line:274] - INFO: epoch 007:   1109 / 1283 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.2, ups=0.81, wpb=49.9, bsz=16, num_updates=8780, lr=2.23801e-05, gnorm=5.975, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14318
2022-07-12 16:55:50 - progress_bar.py[line:274] - INFO: epoch 007:   1119 / 1283 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.4, ups=0.8, wpb=52.9, bsz=16, num_updates=8790, lr=2.23463e-05, gnorm=4.594, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14330
2022-07-12 16:56:02 - progress_bar.py[line:274] - INFO: epoch 007:   1129 / 1283 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.9, ups=0.82, wpb=51.2, bsz=16, num_updates=8800, lr=2.23124e-05, gnorm=5.427, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=14343
2022-07-12 16:56:15 - progress_bar.py[line:274] - INFO: epoch 007:   1139 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.1, ups=0.81, wpb=50.8, bsz=16, num_updates=8810, lr=2.22786e-05, gnorm=3.621, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=14355
2022-07-12 16:56:27 - progress_bar.py[line:274] - INFO: epoch 007:   1149 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=43.6, ups=0.81, wpb=53.9, bsz=16, num_updates=8820, lr=2.22448e-05, gnorm=5.389, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14367
2022-07-12 16:56:39 - progress_bar.py[line:274] - INFO: epoch 007:   1159 / 1283 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=42.5, ups=0.8, wpb=53.1, bsz=16, num_updates=8830, lr=2.22109e-05, gnorm=6.512, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14380
2022-07-12 16:56:47 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 16:56:53 - progress_bar.py[line:274] - INFO: epoch 007:   1170 / 1283 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=37.5, ups=0.73, wpb=51.2, bsz=16, num_updates=8840, lr=2.21771e-05, gnorm=5.466, clip=100, loss_scale=64, train_wall=14, gb_free=2.4, ema_decay=0.9999, wall=14393
2022-07-12 16:57:06 - progress_bar.py[line:274] - INFO: epoch 007:   1180 / 1283 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42, ups=0.81, wpb=52.1, bsz=16, num_updates=8850, lr=2.21433e-05, gnorm=5.246, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14406
2022-07-12 16:57:18 - progress_bar.py[line:274] - INFO: epoch 007:   1190 / 1283 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43.1, ups=0.81, wpb=53.2, bsz=16, num_updates=8860, lr=2.21095e-05, gnorm=4.74, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14418
2022-07-12 16:57:30 - progress_bar.py[line:274] - INFO: epoch 007:   1200 / 1283 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.3, ups=0.8, wpb=50.4, bsz=16, num_updates=8870, lr=2.20756e-05, gnorm=4.366, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=14431
2022-07-12 16:57:43 - progress_bar.py[line:274] - INFO: epoch 007:   1210 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.6, ups=0.81, wpb=51.3, bsz=16, num_updates=8880, lr=2.20418e-05, gnorm=4.901, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=14443
2022-07-12 16:57:55 - progress_bar.py[line:274] - INFO: epoch 007:   1220 / 1283 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.3, ups=0.81, wpb=51.2, bsz=16, num_updates=8890, lr=2.2008e-05, gnorm=4.661, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=14455
2022-07-12 16:58:07 - progress_bar.py[line:274] - INFO: epoch 007:   1230 / 1283 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.5, ups=0.82, wpb=50.7, bsz=16, num_updates=8900, lr=2.19742e-05, gnorm=5.446, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=14468
2022-07-12 16:58:20 - progress_bar.py[line:274] - INFO: epoch 007:   1240 / 1283 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=39.6, ups=0.8, wpb=49.6, bsz=16, num_updates=8910, lr=2.19403e-05, gnorm=4.648, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14480
2022-07-12 16:58:32 - progress_bar.py[line:274] - INFO: epoch 007:   1250 / 1283 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.4, ups=0.82, wpb=52, bsz=16, num_updates=8920, lr=2.19065e-05, gnorm=4.451, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=14492
2022-07-12 16:58:45 - progress_bar.py[line:274] - INFO: epoch 007:   1260 / 1283 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.7, ups=0.79, wpb=52.6, bsz=16, num_updates=8930, lr=2.18727e-05, gnorm=5.435, clip=100, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=14505
2022-07-12 16:58:57 - progress_bar.py[line:274] - INFO: epoch 007:   1270 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.2, ups=0.8, wpb=52.8, bsz=16, num_updates=8940, lr=2.18388e-05, gnorm=3.979, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=14518
2022-07-12 16:59:10 - progress_bar.py[line:274] - INFO: epoch 007:   1280 / 1283 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=48.3, nsentences=16, sample_size=48.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=38.9, ups=0.81, wpb=48.3, bsz=16, num_updates=8950, lr=2.1805e-05, gnorm=5.017, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=14530
2022-07-12 16:59:13 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 17:07:16 - progress_bar.py[line:282] - INFO: epoch 007 | valid on 'valid' subset | loss 1.395 | loss_v1 0 | loss_v2 0 | nll_loss 1.125 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.18 | vqa_score 0.3235 | wps 30.4 | wpb 13.6 | bsz 4 | num_updates 8953 | best_vqa_score 0.3235
2022-07-12 17:07:16 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 7 @ 8953 updates
2022-07-12 17:07:16 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints7.pt
2022-07-12 17:07:47 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints7.pt
2022-07-12 17:08:49 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints7.pt (epoch 7 @ 8953 updates, score 0.3235) (writing took 92.8777225210215 seconds)
2022-07-12 17:08:49 - train.py[line:323] - INFO: end of epoch 7 (average epoch stats below)
2022-07-12 17:08:49 - progress_bar.py[line:282] - INFO: epoch 007 | loss 0.611 | loss_v1 0 | loss_v2 0 | nll_loss 0.181 | ntokens 51.949 | nsentences 15.996 | sample_size 51.949 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.13 | wps 30.7 | ups 0.59 | wpb 51.9 | bsz 16 | num_updates 8953 | lr 2.17949e-05 | gnorm 5.292 | clip 99.8 | loss_scale 64 | train_wall 1580 | gb_free 2.3 | ema_decay 0.9999 | wall 15109
2022-07-12 17:08:49 - trainer.py[line:639] - INFO: loading train data for epoch 8
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 17:08:51 - trainer.py[line:703] - INFO: begin training epoch 8
2022-07-12 17:08:51 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 17:09:00 - progress_bar.py[line:274] - INFO: epoch 008:      7 / 1283 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=47.6, nsentences=15.6, sample_size=47.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=0.8, ups=0.02, wpb=47.6, bsz=15.6, num_updates=8960, lr=2.17712e-05, gnorm=6.124, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15120
2022-07-12 17:09:13 - progress_bar.py[line:274] - INFO: epoch 008:     17 / 1283 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.7, ups=0.8, wpb=50.6, bsz=16, num_updates=8970, lr=2.17374e-05, gnorm=4.865, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15133
2022-07-12 17:09:25 - progress_bar.py[line:274] - INFO: epoch 008:     27 / 1283 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=54.7, nsentences=16, sample_size=54.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=44.2, ups=0.81, wpb=54.7, bsz=16, num_updates=8980, lr=2.17035e-05, gnorm=4.394, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15145
2022-07-12 17:09:29 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.73 GiB total capacity; 8.67 GiB already allocated; 7.38 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 17:09:29 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 50        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8882 MB |    8882 MB |    1075 TB |    1075 TB |
|       from large pool |    8682 MB |    8682 MB |    1066 TB |    1066 TB |
|       from small pool |     200 MB |     200 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8882 MB |    8882 MB |    1075 TB |    1075 TB |
|       from large pool |    8682 MB |    8682 MB |    1066 TB |    1066 TB |
|       from small pool |     200 MB |     200 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9610 MB |    9610 MB |   47094 MB |   37484 MB |
|       from large pool |    9402 MB |    9402 MB |   44346 MB |   34944 MB |
|       from small pool |     208 MB |     208 MB |    2748 MB |    2540 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  744468 KB |    3070 MB |    1042 TB |    1042 TB |
|       from large pool |  736464 KB |    3066 MB |    1032 TB |    1032 TB |
|       from small pool |    8004 KB |      22 MB |      10 TB |      10 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4345    |    4376    |  252804 K  |  252799 K  |
|       from large pool |     896    |     896    |   73980 K  |   73979 K  |
|       from small pool |    3449    |    3494    |  178823 K  |  178820 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4345    |    4376    |  252804 K  |  252799 K  |
|       from large pool |     896    |     896    |   73980 K  |   73979 K  |
|       from small pool |    3449    |    3494    |  178823 K  |  178820 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     203    |     203    |    1794    |    1591    |
|       from large pool |      99    |      99    |     420    |     321    |
|       from small pool |     104    |     104    |    1374    |    1270    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      58    |     197    |  139231 K  |  139231 K  |
|       from large pool |      21    |      78    |   36147 K  |   36147 K  |
|       from small pool |      37    |     138    |  103084 K  |  103084 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 17:09:29 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 17:09:38 - progress_bar.py[line:274] - INFO: epoch 008:     38 / 1283 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.3, ups=0.78, wpb=53.3, bsz=16, num_updates=8990, lr=2.16697e-05, gnorm=4.202, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15158
2022-07-12 17:09:50 - progress_bar.py[line:274] - INFO: epoch 008:     48 / 1283 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=43.2, ups=0.82, wpb=52.7, bsz=16, num_updates=9000, lr=2.16359e-05, gnorm=3.267, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15170
2022-07-12 17:10:02 - progress_bar.py[line:274] - INFO: epoch 008:     58 / 1283 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=40.1, ups=0.81, wpb=49.8, bsz=16, num_updates=9010, lr=2.16021e-05, gnorm=3.952, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15183
2022-07-12 17:10:15 - progress_bar.py[line:274] - INFO: epoch 008:     68 / 1283 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.7, ups=0.81, wpb=51.7, bsz=16, num_updates=9020, lr=2.15682e-05, gnorm=4.335, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=15195
2022-07-12 17:10:27 - progress_bar.py[line:274] - INFO: epoch 008:     78 / 1283 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.9, ups=0.81, wpb=52.9, bsz=16, num_updates=9030, lr=2.15344e-05, gnorm=5.14, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15207
2022-07-12 17:10:40 - progress_bar.py[line:274] - INFO: epoch 008:     88 / 1283 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=43, ups=0.81, wpb=53.1, bsz=16, num_updates=9040, lr=2.15006e-05, gnorm=4.824, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=15220
2022-07-12 17:10:52 - progress_bar.py[line:274] - INFO: epoch 008:     98 / 1283 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.2, ups=0.81, wpb=50.8, bsz=16, num_updates=9050, lr=2.14667e-05, gnorm=5.983, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15232
2022-07-12 17:11:04 - progress_bar.py[line:274] - INFO: epoch 008:    108 / 1283 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.5, ups=0.81, wpb=51.5, bsz=16, num_updates=9060, lr=2.14329e-05, gnorm=4.474, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15244
2022-07-12 17:11:17 - progress_bar.py[line:274] - INFO: epoch 008:    118 / 1283 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.7, ups=0.81, wpb=50.5, bsz=16, num_updates=9070, lr=2.13991e-05, gnorm=3.51, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15257
2022-07-12 17:11:29 - progress_bar.py[line:274] - INFO: epoch 008:    128 / 1283 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.2, ups=0.82, wpb=51.7, bsz=16, num_updates=9080, lr=2.13653e-05, gnorm=3.168, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=15269
2022-07-12 17:11:41 - progress_bar.py[line:274] - INFO: epoch 008:    138 / 1283 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.2, ups=0.81, wpb=52.4, bsz=16, num_updates=9090, lr=2.13314e-05, gnorm=4.115, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15282
2022-07-12 17:11:54 - progress_bar.py[line:274] - INFO: epoch 008:    148 / 1283 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=44.1, ups=0.82, wpb=54, bsz=16, num_updates=9100, lr=2.12976e-05, gnorm=5.012, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15294
2022-07-12 17:12:06 - progress_bar.py[line:274] - INFO: epoch 008:    158 / 1283 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43.6, ups=0.82, wpb=53.4, bsz=16, num_updates=9110, lr=2.12638e-05, gnorm=6.281, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15306
2022-07-12 17:12:18 - progress_bar.py[line:274] - INFO: epoch 008:    168 / 1283 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=52.6, nsentences=15.9, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.7, ups=0.81, wpb=52.6, bsz=15.9, num_updates=9120, lr=2.123e-05, gnorm=6.823, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15318
2022-07-12 17:12:31 - progress_bar.py[line:274] - INFO: epoch 008:    178 / 1283 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=49.2, nsentences=16, sample_size=49.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=39.4, ups=0.8, wpb=49.2, bsz=16, num_updates=9130, lr=2.11961e-05, gnorm=5.404, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15331
2022-07-12 17:12:43 - progress_bar.py[line:274] - INFO: epoch 008:    188 / 1283 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=40.8, ups=0.81, wpb=50.5, bsz=16, num_updates=9140, lr=2.11623e-05, gnorm=5.463, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15343
2022-07-12 17:12:55 - progress_bar.py[line:274] - INFO: epoch 008:    198 / 1283 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=43, ups=0.81, wpb=53.3, bsz=16, num_updates=9150, lr=2.11285e-05, gnorm=5.645, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15356
2022-07-12 17:13:08 - progress_bar.py[line:274] - INFO: epoch 008:    208 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.8, ups=0.81, wpb=51.5, bsz=16, num_updates=9160, lr=2.10946e-05, gnorm=4.999, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15368
2022-07-12 17:13:20 - progress_bar.py[line:274] - INFO: epoch 008:    218 / 1283 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42.2, ups=0.81, wpb=52.1, bsz=16, num_updates=9170, lr=2.10608e-05, gnorm=6.949, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=15380
2022-07-12 17:13:32 - progress_bar.py[line:274] - INFO: epoch 008:    228 / 1283 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=43, ups=0.81, wpb=53.1, bsz=16, num_updates=9180, lr=2.1027e-05, gnorm=5.445, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15393
2022-07-12 17:13:45 - progress_bar.py[line:274] - INFO: epoch 008:    238 / 1283 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.6, ups=0.81, wpb=51.6, bsz=16, num_updates=9190, lr=2.09932e-05, gnorm=7.155, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15405
2022-07-12 17:13:57 - progress_bar.py[line:274] - INFO: epoch 008:    248 / 1283 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.6, ups=0.8, wpb=53, bsz=16, num_updates=9200, lr=2.09593e-05, gnorm=4.998, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15418
2022-07-12 17:14:09 - progress_bar.py[line:274] - INFO: epoch 008:    258 / 1283 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=43.1, ups=0.82, wpb=52.6, bsz=16, num_updates=9210, lr=2.09255e-05, gnorm=5.134, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15430
2022-07-12 17:14:22 - progress_bar.py[line:274] - INFO: epoch 008:    268 / 1283 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=48.6, nsentences=16, sample_size=48.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=39.7, ups=0.82, wpb=48.6, bsz=16, num_updates=9220, lr=2.08917e-05, gnorm=7.533, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15442
2022-07-12 17:14:34 - progress_bar.py[line:274] - INFO: epoch 008:    278 / 1283 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=41.4, ups=0.8, wpb=51.9, bsz=16, num_updates=9230, lr=2.08579e-05, gnorm=6.654, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=15454
2022-07-12 17:14:47 - progress_bar.py[line:274] - INFO: epoch 008:    288 / 1283 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.6, ups=0.81, wpb=51.7, bsz=16, num_updates=9240, lr=2.0824e-05, gnorm=5.328, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15467
2022-07-12 17:14:59 - progress_bar.py[line:274] - INFO: epoch 008:    298 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.1, ups=0.81, wpb=51.7, bsz=16, num_updates=9250, lr=2.07902e-05, gnorm=3.734, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=15479
2022-07-12 17:15:11 - progress_bar.py[line:274] - INFO: epoch 008:    308 / 1283 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=43.1, ups=0.8, wpb=53.6, bsz=16, num_updates=9260, lr=2.07564e-05, gnorm=7.313, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15492
2022-07-12 17:15:24 - progress_bar.py[line:274] - INFO: epoch 008:    318 / 1283 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=43.4, ups=0.82, wpb=53.2, bsz=16, num_updates=9270, lr=2.07225e-05, gnorm=5.426, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15504
2022-07-12 17:15:36 - progress_bar.py[line:274] - INFO: epoch 008:    328 / 1283 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.1, ups=0.8, wpb=52.6, bsz=16, num_updates=9280, lr=2.06887e-05, gnorm=5.571, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15516
2022-07-12 17:15:49 - progress_bar.py[line:274] - INFO: epoch 008:    338 / 1283 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=42.1, ups=0.8, wpb=52.3, bsz=16, num_updates=9290, lr=2.06549e-05, gnorm=5.138, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15529
2022-07-12 17:16:01 - progress_bar.py[line:274] - INFO: epoch 008:    348 / 1283 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=49.3, nsentences=16, sample_size=49.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=39.2, ups=0.79, wpb=49.3, bsz=16, num_updates=9300, lr=2.06211e-05, gnorm=4.094, clip=100, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=15541
2022-07-12 17:16:13 - progress_bar.py[line:274] - INFO: epoch 008:    358 / 1283 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.9, ups=0.82, wpb=50, bsz=16, num_updates=9310, lr=2.05872e-05, gnorm=4.617, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15554
2022-07-12 17:16:26 - progress_bar.py[line:274] - INFO: epoch 008:    368 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.3, ups=0.8, wpb=51.4, bsz=16, num_updates=9320, lr=2.05534e-05, gnorm=4.26, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15566
2022-07-12 17:16:38 - progress_bar.py[line:274] - INFO: epoch 008:    378 / 1283 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42, ups=0.81, wpb=52.2, bsz=16, num_updates=9330, lr=2.05196e-05, gnorm=4.92, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=15578
2022-07-12 17:16:51 - progress_bar.py[line:274] - INFO: epoch 008:    388 / 1283 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.4, ups=0.81, wpb=52.5, bsz=16, num_updates=9340, lr=2.04858e-05, gnorm=5.819, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15591
2022-07-12 17:17:03 - progress_bar.py[line:274] - INFO: epoch 008:    398 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=44.2, ups=0.81, wpb=54.6, bsz=16, num_updates=9350, lr=2.04519e-05, gnorm=4.597, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15603
2022-07-12 17:17:15 - progress_bar.py[line:274] - INFO: epoch 008:    408 / 1283 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.3, ups=0.81, wpb=51.1, bsz=16, num_updates=9360, lr=2.04181e-05, gnorm=5.517, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15616
2022-07-12 17:17:28 - progress_bar.py[line:274] - INFO: epoch 008:    418 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.8, ups=0.81, wpb=51.5, bsz=16, num_updates=9370, lr=2.03843e-05, gnorm=3.316, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15628
2022-07-12 17:17:40 - progress_bar.py[line:274] - INFO: epoch 008:    428 / 1283 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.2, ups=0.8, wpb=51.4, bsz=16, num_updates=9380, lr=2.03504e-05, gnorm=4.57, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15640
2022-07-12 17:17:53 - progress_bar.py[line:274] - INFO: epoch 008:    438 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.5, ups=0.79, wpb=51, bsz=16, num_updates=9390, lr=2.03166e-05, gnorm=4.55, clip=100, loss_scale=128, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=15653
2022-07-12 17:18:05 - progress_bar.py[line:274] - INFO: epoch 008:    448 / 1283 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.7, ups=0.81, wpb=52.8, bsz=16, num_updates=9400, lr=2.02828e-05, gnorm=4.497, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15665
2022-07-12 17:18:18 - progress_bar.py[line:274] - INFO: epoch 008:    458 / 1283 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=39.6, ups=0.8, wpb=49.6, bsz=16, num_updates=9410, lr=2.0249e-05, gnorm=6.211, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15678
2022-07-12 17:18:30 - progress_bar.py[line:274] - INFO: epoch 008:    468 / 1283 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=44.3, ups=0.81, wpb=54.6, bsz=16, num_updates=9420, lr=2.02151e-05, gnorm=3.929, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15690
2022-07-12 17:18:42 - progress_bar.py[line:274] - INFO: epoch 008:    478 / 1283 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.5, ups=0.81, wpb=51.1, bsz=16, num_updates=9430, lr=2.01813e-05, gnorm=5.983, clip=90, loss_scale=128, train_wall=12, gb_free=2, ema_decay=0.9999, wall=15702
2022-07-12 17:18:55 - progress_bar.py[line:274] - INFO: epoch 008:    488 / 1283 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=54.7, nsentences=16, sample_size=54.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=44.2, ups=0.81, wpb=54.7, bsz=16, num_updates=9440, lr=2.01475e-05, gnorm=6.907, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15715
2022-07-12 17:19:07 - progress_bar.py[line:274] - INFO: epoch 008:    498 / 1283 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43.2, ups=0.81, wpb=53.1, bsz=16, num_updates=9450, lr=2.01137e-05, gnorm=4.777, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=15727
2022-07-12 17:19:19 - progress_bar.py[line:274] - INFO: epoch 008:    508 / 1283 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=54.3, nsentences=16, sample_size=54.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=44.1, ups=0.81, wpb=54.3, bsz=16, num_updates=9460, lr=2.00798e-05, gnorm=5.488, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15739
2022-07-12 17:19:32 - progress_bar.py[line:274] - INFO: epoch 008:    518 / 1283 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.2, ups=0.81, wpb=51, bsz=16, num_updates=9470, lr=2.0046e-05, gnorm=4.089, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15752
2022-07-12 17:19:44 - progress_bar.py[line:274] - INFO: epoch 008:    528 / 1283 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.9, ups=0.8, wpb=53.3, bsz=16, num_updates=9480, lr=2.00122e-05, gnorm=4.879, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15764
2022-07-12 17:19:57 - progress_bar.py[line:274] - INFO: epoch 008:    538 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.9, ups=0.8, wpb=52.5, bsz=16, num_updates=9490, lr=1.99784e-05, gnorm=4.868, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15777
2022-07-12 17:20:01 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 17:20:10 - progress_bar.py[line:274] - INFO: epoch 008:    549 / 1283 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=54.5, nsentences=16, sample_size=54.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=40.6, ups=0.75, wpb=54.5, bsz=16, num_updates=9500, lr=1.99445e-05, gnorm=4.673, clip=100, loss_scale=64, train_wall=13, gb_free=2.1, ema_decay=0.9999, wall=15790
2022-07-12 17:20:22 - progress_bar.py[line:274] - INFO: epoch 008:    559 / 1283 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.5, ups=0.8, wpb=51.8, bsz=16, num_updates=9510, lr=1.99107e-05, gnorm=4.225, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15803
2022-07-12 17:20:35 - progress_bar.py[line:274] - INFO: epoch 008:    569 / 1283 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=40.4, ups=0.8, wpb=50.6, bsz=16, num_updates=9520, lr=1.98769e-05, gnorm=4.796, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15815
2022-07-12 17:20:47 - progress_bar.py[line:274] - INFO: epoch 008:    579 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.1, ups=0.81, wpb=51.1, bsz=16, num_updates=9530, lr=1.9843e-05, gnorm=3.306, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15828
2022-07-12 17:21:00 - progress_bar.py[line:274] - INFO: epoch 008:    589 / 1283 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.7, ups=0.8, wpb=53.3, bsz=16, num_updates=9540, lr=1.98092e-05, gnorm=3.914, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15840
2022-07-12 17:21:12 - progress_bar.py[line:274] - INFO: epoch 008:    599 / 1283 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.4, ups=0.81, wpb=52.3, bsz=16, num_updates=9550, lr=1.97754e-05, gnorm=3.467, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15852
2022-07-12 17:21:25 - progress_bar.py[line:274] - INFO: epoch 008:    609 / 1283 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.9, ups=0.81, wpb=51.8, bsz=16, num_updates=9560, lr=1.97416e-05, gnorm=4.289, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15865
2022-07-12 17:21:37 - progress_bar.py[line:274] - INFO: epoch 008:    619 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=43.1, ups=0.8, wpb=53.8, bsz=16, num_updates=9570, lr=1.97077e-05, gnorm=3.884, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15877
2022-07-12 17:21:49 - progress_bar.py[line:274] - INFO: epoch 008:    629 / 1283 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.4, ups=0.81, wpb=50.9, bsz=16, num_updates=9580, lr=1.96739e-05, gnorm=3.273, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15890
2022-07-12 17:22:02 - progress_bar.py[line:274] - INFO: epoch 008:    639 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.5, ups=0.8, wpb=53.1, bsz=16, num_updates=9590, lr=1.96401e-05, gnorm=5.429, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15902
2022-07-12 17:22:14 - progress_bar.py[line:274] - INFO: epoch 008:    649 / 1283 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.5, ups=0.81, wpb=52.6, bsz=16, num_updates=9600, lr=1.96063e-05, gnorm=3.387, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=15914
2022-07-12 17:22:26 - progress_bar.py[line:274] - INFO: epoch 008:    659 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.1, ups=0.82, wpb=51.5, bsz=16, num_updates=9610, lr=1.95724e-05, gnorm=3.731, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15927
2022-07-12 17:22:34 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 10.73 GiB total capacity; 8.45 GiB already allocated; 50.81 MiB free; 9.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 17:22:34 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8376 MB |    9227 MB |    1141 TB |    1141 TB |
|       from large pool |    8230 MB |    9023 MB |    1131 TB |    1131 TB |
|       from small pool |     145 MB |     207 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8376 MB |    9227 MB |    1141 TB |    1141 TB |
|       from large pool |    8230 MB |    9023 MB |    1131 TB |    1131 TB |
|       from small pool |     145 MB |     207 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9566 MB |    9566 MB |   47940 MB |   38374 MB |
|       from large pool |    9416 MB |    9416 MB |   45042 MB |   35626 MB |
|       from small pool |     150 MB |     214 MB |    2898 MB |    2748 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1013 MB |    3344 MB |    1111 TB |    1111 TB |
|       from large pool |    1009 MB |    3342 MB |    1100 TB |    1100 TB |
|       from small pool |       4 MB |      24 MB |      10 TB |      10 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  263321 K  |  263317 K  |
|       from large pool |     835    |     910    |   78466 K  |   78466 K  |
|       from small pool |    3218    |    3470    |  184854 K  |  184851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  263321 K  |  263317 K  |
|       from large pool |     835    |     910    |   78466 K  |   78466 K  |
|       from small pool |    3218    |    3470    |  184854 K  |  184851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     174    |     205    |    1873    |    1699    |
|       from large pool |      99    |      99    |     424    |     325    |
|       from small pool |      75    |     107    |    1449    |    1374    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      55    |     201    |  144512 K  |  144512 K  |
|       from large pool |      44    |      79    |   38266 K  |   38266 K  |
|       from small pool |      11    |     140    |  106246 K  |  106246 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 17:22:34 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 17:22:39 - progress_bar.py[line:274] - INFO: epoch 008:    670 / 1283 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.2, ups=0.77, wpb=53.8, bsz=16, num_updates=9620, lr=1.95386e-05, gnorm=4.772, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=15940
2022-07-12 17:22:52 - progress_bar.py[line:274] - INFO: epoch 008:    680 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=43.4, ups=0.81, wpb=53.3, bsz=16, num_updates=9630, lr=1.95048e-05, gnorm=4.222, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15952
2022-07-12 17:23:04 - progress_bar.py[line:274] - INFO: epoch 008:    690 / 1283 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.4, ups=0.81, wpb=52.3, bsz=16, num_updates=9640, lr=1.94709e-05, gnorm=3.385, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=15964
2022-07-12 17:23:16 - progress_bar.py[line:274] - INFO: epoch 008:    700 / 1283 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=48.8, nsentences=16, sample_size=48.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=39.6, ups=0.81, wpb=48.8, bsz=16, num_updates=9650, lr=1.94371e-05, gnorm=3.904, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15977
2022-07-12 17:23:29 - progress_bar.py[line:274] - INFO: epoch 008:    710 / 1283 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.5, ups=0.82, wpb=52, bsz=16, num_updates=9660, lr=1.94033e-05, gnorm=2.998, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=15989
2022-07-12 17:23:41 - progress_bar.py[line:274] - INFO: epoch 008:    720 / 1283 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.7, ups=0.81, wpb=53.9, bsz=16, num_updates=9670, lr=1.93695e-05, gnorm=3.022, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16001
2022-07-12 17:23:53 - progress_bar.py[line:274] - INFO: epoch 008:    730 / 1283 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42, ups=0.81, wpb=51.7, bsz=16, num_updates=9680, lr=1.93356e-05, gnorm=3.572, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16014
2022-07-12 17:24:05 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-12 17:24:07 - progress_bar.py[line:274] - INFO: epoch 008:    741 / 1283 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=39.3, ups=0.75, wpb=52.7, bsz=16, num_updates=9690, lr=1.93018e-05, gnorm=3.532, clip=100, loss_scale=32, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=16027
2022-07-12 17:24:19 - progress_bar.py[line:274] - INFO: epoch 008:    751 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.5, ups=0.81, wpb=54, bsz=16, num_updates=9700, lr=1.9268e-05, gnorm=3.42, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16039
2022-07-12 17:24:31 - progress_bar.py[line:274] - INFO: epoch 008:    761 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.1, ups=0.81, wpb=51.9, bsz=16, num_updates=9710, lr=1.92342e-05, gnorm=3.709, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=16052
2022-07-12 17:24:44 - progress_bar.py[line:274] - INFO: epoch 008:    771 / 1283 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.4, ups=0.81, wpb=51.2, bsz=16, num_updates=9720, lr=1.92003e-05, gnorm=6.329, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16064
2022-07-12 17:24:56 - progress_bar.py[line:274] - INFO: epoch 008:    781 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.8, ups=0.81, wpb=52.9, bsz=16, num_updates=9730, lr=1.91665e-05, gnorm=3.593, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=16076
2022-07-12 17:25:01 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.73 GiB total capacity; 8.64 GiB already allocated; 7.19 MiB free; 9.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 17:25:01 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8851 MB |    8875 MB |    1153 TB |    1153 TB |
|       from large pool |    8668 MB |    8729 MB |    1144 TB |    1144 TB |
|       from small pool |     183 MB |     183 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8851 MB |    8875 MB |    1153 TB |    1153 TB |
|       from large pool |    8668 MB |    8729 MB |    1144 TB |    1144 TB |
|       from small pool |     183 MB |     183 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9610 MB |    9610 MB |   48572 MB |   38962 MB |
|       from large pool |    9424 MB |    9424 MB |   45570 MB |   36146 MB |
|       from small pool |     186 MB |     206 MB |    3002 MB |    2816 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  777082 KB |    3387 MB |    1124 TB |    1124 TB |
|       from large pool |  774138 KB |    3385 MB |    1113 TB |    1113 TB |
|       from small pool |    2943 KB |      22 MB |      10 TB |      10 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4233    |    4233    |  265298 K  |  265294 K  |
|       from large pool |     878    |     879    |   79310 K  |   79309 K  |
|       from small pool |    3355    |    3355    |  185987 K  |  185984 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4233    |    4233    |  265298 K  |  265294 K  |
|       from large pool |     878    |     879    |   79310 K  |   79309 K  |
|       from small pool |    3355    |    3355    |  185987 K  |  185984 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     192    |     201    |    1928    |    1736    |
|       from large pool |      99    |      99    |     427    |     328    |
|       from small pool |      93    |     103    |    1501    |    1408    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      54    |     206    |  145508 K  |  145508 K  |
|       from large pool |      29    |      85    |   38666 K  |   38666 K  |
|       from small pool |      25    |     142    |  106841 K  |  106841 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 17:25:01 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 17:25:09 - progress_bar.py[line:274] - INFO: epoch 008:    792 / 1283 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.9, ups=0.79, wpb=54.6, bsz=16, num_updates=9740, lr=1.91327e-05, gnorm=2.308, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16089
2022-07-12 17:25:21 - progress_bar.py[line:274] - INFO: epoch 008:    802 / 1283 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.5, ups=0.81, wpb=49.7, bsz=16, num_updates=9750, lr=1.90988e-05, gnorm=4.194, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16101
2022-07-12 17:25:34 - progress_bar.py[line:274] - INFO: epoch 008:    812 / 1283 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.8, ups=0.8, wpb=52.5, bsz=16, num_updates=9760, lr=1.9065e-05, gnorm=7.267, clip=100, loss_scale=32, train_wall=13, gb_free=2, ema_decay=0.9999, wall=16114
2022-07-12 17:25:46 - progress_bar.py[line:274] - INFO: epoch 008:    822 / 1283 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.2, ups=0.81, wpb=52.1, bsz=16, num_updates=9770, lr=1.90312e-05, gnorm=4.948, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16126
2022-07-12 17:25:59 - progress_bar.py[line:274] - INFO: epoch 008:    832 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.8, ups=0.8, wpb=51, bsz=16, num_updates=9780, lr=1.89974e-05, gnorm=4.277, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16139
2022-07-12 17:26:11 - progress_bar.py[line:274] - INFO: epoch 008:    842 / 1283 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.2, ups=0.8, wpb=51.3, bsz=16, num_updates=9790, lr=1.89635e-05, gnorm=2.937, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16151
2022-07-12 17:26:23 - progress_bar.py[line:274] - INFO: epoch 008:    852 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.9, ups=0.81, wpb=51.6, bsz=16, num_updates=9800, lr=1.89297e-05, gnorm=2.729, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16164
2022-07-12 17:26:36 - progress_bar.py[line:274] - INFO: epoch 008:    862 / 1283 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.2, ups=0.81, wpb=51.1, bsz=16, num_updates=9810, lr=1.88959e-05, gnorm=3.753, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16176
2022-07-12 17:26:48 - progress_bar.py[line:274] - INFO: epoch 008:    872 / 1283 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.4, ups=0.81, wpb=51.2, bsz=16, num_updates=9820, lr=1.88621e-05, gnorm=5.251, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16188
2022-07-12 17:27:00 - progress_bar.py[line:274] - INFO: epoch 008:    882 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.9, ups=0.81, wpb=51.5, bsz=16, num_updates=9830, lr=1.88282e-05, gnorm=3.143, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=16201
2022-07-12 17:27:13 - progress_bar.py[line:274] - INFO: epoch 008:    892 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.6, ups=0.81, wpb=52.7, bsz=16, num_updates=9840, lr=1.87944e-05, gnorm=4.018, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16213
2022-07-12 17:27:25 - progress_bar.py[line:274] - INFO: epoch 008:    902 / 1283 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=56.4, nsentences=16, sample_size=56.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=45.3, ups=0.8, wpb=56.4, bsz=16, num_updates=9850, lr=1.87606e-05, gnorm=3.942, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=16225
2022-07-12 17:27:38 - progress_bar.py[line:274] - INFO: epoch 008:    912 / 1283 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=39.6, ups=0.8, wpb=49.6, bsz=16, num_updates=9860, lr=1.87267e-05, gnorm=4.169, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16238
2022-07-12 17:27:50 - progress_bar.py[line:274] - INFO: epoch 008:    922 / 1283 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.5, ups=0.81, wpb=51.1, bsz=16, num_updates=9870, lr=1.86929e-05, gnorm=4.269, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=16250
2022-07-12 17:28:02 - progress_bar.py[line:274] - INFO: epoch 008:    932 / 1283 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.5, ups=0.8, wpb=51.6, bsz=16, num_updates=9880, lr=1.86591e-05, gnorm=3.001, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16263
2022-07-12 17:28:15 - progress_bar.py[line:274] - INFO: epoch 008:    942 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.4, ups=0.81, wpb=52.1, bsz=16, num_updates=9890, lr=1.86253e-05, gnorm=2.961, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16275
2022-07-12 17:28:27 - progress_bar.py[line:274] - INFO: epoch 008:    952 / 1283 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.2, ups=0.81, wpb=53.5, bsz=16, num_updates=9900, lr=1.85914e-05, gnorm=2.984, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16287
2022-07-12 17:28:40 - progress_bar.py[line:274] - INFO: epoch 008:    962 / 1283 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.9, ups=0.8, wpb=52.1, bsz=16, num_updates=9910, lr=1.85576e-05, gnorm=3.84, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16300
2022-07-12 17:28:52 - progress_bar.py[line:274] - INFO: epoch 008:    972 / 1283 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=40.8, ups=0.8, wpb=50.8, bsz=16, num_updates=9920, lr=1.85238e-05, gnorm=3.773, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16312
2022-07-12 17:29:04 - progress_bar.py[line:274] - INFO: epoch 008:    982 / 1283 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.8, ups=0.8, wpb=49.5, bsz=16, num_updates=9930, lr=1.849e-05, gnorm=2.954, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16325
2022-07-12 17:29:17 - progress_bar.py[line:274] - INFO: epoch 008:    992 / 1283 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.6, ups=0.81, wpb=51.7, bsz=16, num_updates=9940, lr=1.84561e-05, gnorm=5.026, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=16337
2022-07-12 17:29:29 - progress_bar.py[line:274] - INFO: epoch 008:   1002 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.1, ups=0.81, wpb=52, bsz=16, num_updates=9950, lr=1.84223e-05, gnorm=4.409, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=16349
2022-07-12 17:29:42 - progress_bar.py[line:274] - INFO: epoch 008:   1012 / 1283 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=48.9, nsentences=16, sample_size=48.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=39, ups=0.8, wpb=48.9, bsz=16, num_updates=9960, lr=1.83885e-05, gnorm=4.625, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=16362
2022-07-12 17:29:54 - progress_bar.py[line:274] - INFO: epoch 008:   1022 / 1283 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.6, ups=0.82, wpb=50.8, bsz=16, num_updates=9970, lr=1.83546e-05, gnorm=4.489, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16374
2022-07-12 17:30:06 - progress_bar.py[line:274] - INFO: epoch 008:   1032 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.9, ups=0.81, wpb=53, bsz=16, num_updates=9980, lr=1.83208e-05, gnorm=4.674, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16387
2022-07-12 17:30:19 - progress_bar.py[line:274] - INFO: epoch 008:   1042 / 1283 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.9, ups=0.8, wpb=52.1, bsz=16, num_updates=9990, lr=1.8287e-05, gnorm=3.643, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16399
2022-07-12 17:30:31 - progress_bar.py[line:274] - INFO: epoch 008:   1052 / 1283 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.7, ups=0.81, wpb=51.7, bsz=16, num_updates=10000, lr=1.82532e-05, gnorm=3.79, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16411
2022-07-12 17:30:44 - progress_bar.py[line:274] - INFO: epoch 008:   1062 / 1283 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=54.7, nsentences=16, sample_size=54.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=44.2, ups=0.81, wpb=54.7, bsz=16, num_updates=10010, lr=1.82193e-05, gnorm=4.572, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16424
2022-07-12 17:30:56 - progress_bar.py[line:274] - INFO: epoch 008:   1072 / 1283 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=40.9, ups=0.81, wpb=50.3, bsz=16, num_updates=10020, lr=1.81855e-05, gnorm=5.079, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16436
2022-07-12 17:31:08 - progress_bar.py[line:274] - INFO: epoch 008:   1082 / 1283 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=43.3, ups=0.81, wpb=53.4, bsz=16, num_updates=10030, lr=1.81517e-05, gnorm=4.834, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16448
2022-07-12 17:31:20 - progress_bar.py[line:274] - INFO: epoch 008:   1092 / 1283 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.8, ups=0.82, wpb=51, bsz=16, num_updates=10040, lr=1.81179e-05, gnorm=2.643, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=16461
2022-07-12 17:31:33 - progress_bar.py[line:274] - INFO: epoch 008:   1102 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.5, ups=0.8, wpb=50.6, bsz=16, num_updates=10050, lr=1.8084e-05, gnorm=6.885, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16473
2022-07-12 17:31:45 - progress_bar.py[line:274] - INFO: epoch 008:   1112 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.4, ups=0.81, wpb=51.3, bsz=16, num_updates=10060, lr=1.80502e-05, gnorm=4.017, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16485
2022-07-12 17:31:58 - progress_bar.py[line:274] - INFO: epoch 008:   1122 / 1283 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42, ups=0.8, wpb=52.7, bsz=16, num_updates=10070, lr=1.80164e-05, gnorm=3.071, clip=100, loss_scale=32, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=16498
2022-07-12 17:32:10 - progress_bar.py[line:274] - INFO: epoch 008:   1132 / 1283 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.2, ups=0.82, wpb=50.5, bsz=16, num_updates=10080, lr=1.79825e-05, gnorm=4.373, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=16510
2022-07-12 17:32:22 - progress_bar.py[line:274] - INFO: epoch 008:   1142 / 1283 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43, ups=0.81, wpb=53.2, bsz=16, num_updates=10090, lr=1.79487e-05, gnorm=3.607, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16523
2022-07-12 17:32:35 - progress_bar.py[line:274] - INFO: epoch 008:   1152 / 1283 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=41.1, ups=0.8, wpb=51.2, bsz=16, num_updates=10100, lr=1.79149e-05, gnorm=4.767, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16535
2022-07-12 17:32:47 - progress_bar.py[line:274] - INFO: epoch 008:   1162 / 1283 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=42.8, ups=0.81, wpb=52.9, bsz=16, num_updates=10110, lr=1.78811e-05, gnorm=5.307, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16547
2022-07-12 17:33:00 - progress_bar.py[line:274] - INFO: epoch 008:   1172 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.2, ups=0.81, wpb=52.1, bsz=16, num_updates=10120, lr=1.78472e-05, gnorm=6.341, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16560
2022-07-12 17:33:12 - progress_bar.py[line:274] - INFO: epoch 008:   1182 / 1283 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.9, ups=0.8, wpb=52.2, bsz=16, num_updates=10130, lr=1.78134e-05, gnorm=4.683, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=16572
2022-07-12 17:33:25 - progress_bar.py[line:274] - INFO: epoch 008:   1192 / 1283 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.5, ups=0.8, wpb=52.1, bsz=16, num_updates=10140, lr=1.77796e-05, gnorm=3.911, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=16585
2022-07-12 17:33:37 - progress_bar.py[line:274] - INFO: epoch 008:   1202 / 1283 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.4, ups=0.8, wpb=51.6, bsz=16, num_updates=10150, lr=1.77458e-05, gnorm=5.354, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=16597
2022-07-12 17:33:49 - progress_bar.py[line:274] - INFO: epoch 008:   1212 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.2, ups=0.81, wpb=50.8, bsz=16, num_updates=10160, lr=1.77119e-05, gnorm=6.326, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16610
2022-07-12 17:34:02 - progress_bar.py[line:274] - INFO: epoch 008:   1222 / 1283 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.3, ups=0.81, wpb=51.3, bsz=16, num_updates=10170, lr=1.76781e-05, gnorm=5.501, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=16622
2022-07-12 17:34:14 - progress_bar.py[line:274] - INFO: epoch 008:   1232 / 1283 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.5, ups=0.81, wpb=51, bsz=16, num_updates=10180, lr=1.76443e-05, gnorm=2.984, clip=90, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=16634
2022-07-12 17:34:26 - progress_bar.py[line:274] - INFO: epoch 008:   1242 / 1283 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=49.1, nsentences=16, sample_size=49.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=39.7, ups=0.81, wpb=49.1, bsz=16, num_updates=10190, lr=1.76104e-05, gnorm=6.37, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16647
2022-07-12 17:34:39 - progress_bar.py[line:274] - INFO: epoch 008:   1252 / 1283 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.5, ups=0.81, wpb=52.3, bsz=16, num_updates=10200, lr=1.75766e-05, gnorm=3.094, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=16659
2022-07-12 17:34:51 - progress_bar.py[line:274] - INFO: epoch 008:   1262 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.6, ups=0.79, wpb=54.2, bsz=16, num_updates=10210, lr=1.75428e-05, gnorm=4.619, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=16672
2022-07-12 17:34:59 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 10.73 GiB total capacity; 8.40 GiB already allocated; 101.38 MiB free; 9.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 17:34:59 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8328 MB |    9099 MB |    1203 TB |    1203 TB |
|       from large pool |    8183 MB |    8899 MB |    1193 TB |    1193 TB |
|       from small pool |     145 MB |     202 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8328 MB |    9099 MB |    1203 TB |    1203 TB |
|       from large pool |    8183 MB |    8899 MB |    1193 TB |    1193 TB |
|       from small pool |     145 MB |     202 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9516 MB |    9574 MB |   50628 MB |   41112 MB |
|       from large pool |    9368 MB |    9368 MB |   47412 MB |   38044 MB |
|       from small pool |     148 MB |     206 MB |    3216 MB |    3068 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1015 MB |    3381 MB |    1175 TB |    1175 TB |
|       from large pool |    1012 MB |    3379 MB |    1164 TB |    1164 TB |
|       from small pool |       2 MB |      20 MB |      11 TB |      11 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  273297 K  |  273293 K  |
|       from large pool |     835    |     904    |   82722 K  |   82721 K  |
|       from small pool |    3218    |    3476    |  190575 K  |  190572 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  273297 K  |  273293 K  |
|       from large pool |     835    |     904    |   82722 K  |   82721 K  |
|       from small pool |    3218    |    3476    |  190575 K  |  190572 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     176    |     205    |    2047    |    1871    |
|       from large pool |     102    |     102    |     439    |     337    |
|       from small pool |      74    |     103    |    1608    |    1534    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      53    |     202    |  149534 K  |  149534 K  |
|       from large pool |      44    |      72    |   40288 K  |   40288 K  |
|       from small pool |       9    |     150    |  109245 K  |  109245 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 17:34:59 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 17:35:04 - progress_bar.py[line:274] - INFO: epoch 008:   1273 / 1283 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=38.3, ups=0.77, wpb=49.6, bsz=16, num_updates=10220, lr=1.7509e-05, gnorm=3.334, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=16685
2022-07-12 17:35:16 - progress_bar.py[line:274] - INFO: epoch 008:   1283 / 1283 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=47.5, nsentences=15.6, sample_size=47.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=39.4, ups=0.83, wpb=47.5, bsz=15.6, num_updates=10230, lr=1.74751e-05, gnorm=3.408, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=16697
2022-07-12 17:35:16 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 17:43:20 - progress_bar.py[line:282] - INFO: epoch 008 | valid on 'valid' subset | loss 1.41 | loss_v1 0 | loss_v2 0 | nll_loss 1.147 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.21 | vqa_score 0.3404 | wps 30.3 | wpb 13.6 | bsz 4 | num_updates 10230 | best_vqa_score 0.3404
2022-07-12 17:43:20 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 8 @ 10230 updates
2022-07-12 17:43:20 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints8.pt
2022-07-12 17:43:52 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints8.pt
2022-07-12 17:44:48 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints8.pt (epoch 8 @ 10230 updates, score 0.3404) (writing took 87.3503434760496 seconds)
2022-07-12 17:44:48 - train.py[line:323] - INFO: end of epoch 8 (average epoch stats below)
2022-07-12 17:44:48 - progress_bar.py[line:282] - INFO: epoch 008 | loss 0.58 | loss_v1 0 | loss_v2 0 | nll_loss 0.152 | ntokens 51.939 | nsentences 15.996 | sample_size 51.939 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.11 | wps 30.7 | ups 0.59 | wpb 51.9 | bsz 16 | num_updates 10230 | lr 1.74751e-05 | gnorm 4.555 | clip 99 | loss_scale 64 | train_wall 1578 | gb_free 2.3 | ema_decay 0.9999 | wall 17268
2022-07-12 17:44:48 - trainer.py[line:639] - INFO: loading train data for epoch 9
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 17:44:50 - trainer.py[line:703] - INFO: begin training epoch 9
2022-07-12 17:44:50 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 17:45:03 - progress_bar.py[line:274] - INFO: epoch 009:     10 / 1283 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=0.9, ups=0.02, wpb=50.9, bsz=16, num_updates=10240, lr=1.74413e-05, gnorm=5.25, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=17283
2022-07-12 17:45:15 - progress_bar.py[line:274] - INFO: epoch 009:     20 / 1283 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41, ups=0.81, wpb=50.9, bsz=16, num_updates=10250, lr=1.74075e-05, gnorm=3.014, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=17295
2022-07-12 17:45:27 - progress_bar.py[line:274] - INFO: epoch 009:     30 / 1283 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.4, ups=0.81, wpb=52.3, bsz=16, num_updates=10260, lr=1.73737e-05, gnorm=4.483, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17308
2022-07-12 17:45:40 - progress_bar.py[line:274] - INFO: epoch 009:     40 / 1283 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=54.5, nsentences=16, sample_size=54.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=44.5, ups=0.82, wpb=54.5, bsz=16, num_updates=10270, lr=1.73398e-05, gnorm=5.782, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17320
2022-07-12 17:45:52 - progress_bar.py[line:274] - INFO: epoch 009:     50 / 1283 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=43.2, ups=0.81, wpb=53.1, bsz=16, num_updates=10280, lr=1.7306e-05, gnorm=3.69, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=17332
2022-07-12 17:46:04 - progress_bar.py[line:274] - INFO: epoch 009:     60 / 1283 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.9, ups=0.81, wpb=49.4, bsz=16, num_updates=10290, lr=1.72722e-05, gnorm=3.469, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=17344
2022-07-12 17:46:17 - progress_bar.py[line:274] - INFO: epoch 009:     70 / 1283 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.9, ups=0.81, wpb=51.5, bsz=16, num_updates=10300, lr=1.72383e-05, gnorm=4.428, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17357
2022-07-12 17:46:29 - progress_bar.py[line:274] - INFO: epoch 009:     80 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.3, ups=0.81, wpb=53.7, bsz=16, num_updates=10310, lr=1.72045e-05, gnorm=3.384, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17369
2022-07-12 17:46:41 - progress_bar.py[line:274] - INFO: epoch 009:     90 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.7, ups=0.82, wpb=52.3, bsz=16, num_updates=10320, lr=1.71707e-05, gnorm=4.806, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17381
2022-07-12 17:46:54 - progress_bar.py[line:274] - INFO: epoch 009:    100 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.5, ups=0.8, wpb=51.8, bsz=16, num_updates=10330, lr=1.71369e-05, gnorm=3.36, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17394
2022-07-12 17:47:06 - progress_bar.py[line:274] - INFO: epoch 009:    110 / 1283 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.4, ups=0.8, wpb=50.3, bsz=16, num_updates=10340, lr=1.7103e-05, gnorm=3.679, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=17406
2022-07-12 17:47:19 - progress_bar.py[line:274] - INFO: epoch 009:    120 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.7, ups=0.81, wpb=51.8, bsz=16, num_updates=10350, lr=1.70692e-05, gnorm=3.813, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17419
2022-07-12 17:47:31 - progress_bar.py[line:274] - INFO: epoch 009:    130 / 1283 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=40.8, ups=0.81, wpb=50.4, bsz=16, num_updates=10360, lr=1.70354e-05, gnorm=4.156, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17431
2022-07-12 17:47:43 - progress_bar.py[line:274] - INFO: epoch 009:    140 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=54.9, nsentences=16, sample_size=54.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=44.6, ups=0.81, wpb=54.9, bsz=16, num_updates=10370, lr=1.70016e-05, gnorm=3.145, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17443
2022-07-12 17:47:55 - progress_bar.py[line:274] - INFO: epoch 009:    150 / 1283 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43.1, ups=0.82, wpb=52.8, bsz=16, num_updates=10380, lr=1.69677e-05, gnorm=4.865, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17456
2022-07-12 17:48:08 - progress_bar.py[line:274] - INFO: epoch 009:    160 / 1283 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.7, ups=0.8, wpb=53.4, bsz=16, num_updates=10390, lr=1.69339e-05, gnorm=4.219, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=17468
2022-07-12 17:48:20 - progress_bar.py[line:274] - INFO: epoch 009:    170 / 1283 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.2, ups=0.82, wpb=51.8, bsz=16, num_updates=10400, lr=1.69001e-05, gnorm=3.895, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17480
2022-07-12 17:48:33 - progress_bar.py[line:274] - INFO: epoch 009:    180 / 1283 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41, ups=0.81, wpb=50.6, bsz=16, num_updates=10410, lr=1.68662e-05, gnorm=5.72, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17493
2022-07-12 17:48:45 - progress_bar.py[line:274] - INFO: epoch 009:    190 / 1283 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.6, ups=0.81, wpb=51.5, bsz=16, num_updates=10420, lr=1.68324e-05, gnorm=5.583, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17505
2022-07-12 17:48:57 - progress_bar.py[line:274] - INFO: epoch 009:    200 / 1283 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.8, ups=0.81, wpb=51.7, bsz=16, num_updates=10430, lr=1.67986e-05, gnorm=3.728, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=17518
2022-07-12 17:49:10 - progress_bar.py[line:274] - INFO: epoch 009:    210 / 1283 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.9, ups=0.82, wpb=51.4, bsz=16, num_updates=10440, lr=1.67648e-05, gnorm=4.136, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17530
2022-07-12 17:49:22 - progress_bar.py[line:274] - INFO: epoch 009:    220 / 1283 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42, ups=0.8, wpb=52.3, bsz=16, num_updates=10450, lr=1.67309e-05, gnorm=3.734, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17542
2022-07-12 17:49:34 - progress_bar.py[line:274] - INFO: epoch 009:    230 / 1283 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.5, ups=0.81, wpb=52.8, bsz=16, num_updates=10460, lr=1.66971e-05, gnorm=4.183, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17555
2022-07-12 17:49:39 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 10.73 GiB total capacity; 8.37 GiB already allocated; 109.38 MiB free; 9.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 17:49:39 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 63        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8302 MB |    9040 MB |    1249 TB |    1249 TB |
|       from large pool |    8157 MB |    8844 MB |    1238 TB |    1238 TB |
|       from small pool |     145 MB |     198 MB |      10 TB |      10 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8302 MB |    9040 MB |    1249 TB |    1249 TB |
|       from large pool |    8157 MB |    8844 MB |    1238 TB |    1238 TB |
|       from small pool |     145 MB |     198 MB |      10 TB |      10 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9508 MB |    9562 MB |   51776 MB |   42268 MB |
|       from large pool |    9358 MB |    9358 MB |   48428 MB |   39070 MB |
|       from small pool |     150 MB |     204 MB |    3348 MB |    3198 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1205 MB |    3213 MB |    1218 TB |    1218 TB |
|       from large pool |    1200 MB |    3211 MB |    1206 TB |    1206 TB |
|       from small pool |       4 MB |      28 MB |      11 TB |      11 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4057    |    4380    |  292308 K  |  292304 K  |
|       from large pool |     835    |     904    |   85947 K  |   85946 K  |
|       from small pool |    3222    |    3496    |  206360 K  |  206357 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4057    |    4380    |  292308 K  |  292304 K  |
|       from large pool |     835    |     904    |   85947 K  |   85946 K  |
|       from small pool |    3222    |    3496    |  206360 K  |  206357 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     177    |     204    |    2119    |    1942    |
|       from large pool |     102    |     102    |     445    |     343    |
|       from small pool |      75    |     102    |    1674    |    1599    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      46    |     219    |  160662 K  |  160662 K  |
|       from large pool |      33    |      69    |   41931 K  |   41931 K  |
|       from small pool |      13    |     164    |  118730 K  |  118730 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 17:49:39 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 17:49:48 - progress_bar.py[line:274] - INFO: epoch 009:    241 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=37.9, ups=0.74, wpb=51.1, bsz=16, num_updates=10470, lr=1.66633e-05, gnorm=3.865, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17568
2022-07-12 17:50:00 - progress_bar.py[line:274] - INFO: epoch 009:    251 / 1283 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.6, ups=0.81, wpb=52.4, bsz=16, num_updates=10480, lr=1.66295e-05, gnorm=3.099, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17580
2022-07-12 17:50:12 - progress_bar.py[line:274] - INFO: epoch 009:    261 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.4, ups=0.82, wpb=52.8, bsz=16, num_updates=10490, lr=1.65956e-05, gnorm=2.955, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17593
2022-07-12 17:50:25 - progress_bar.py[line:274] - INFO: epoch 009:    271 / 1283 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=48.2, nsentences=16, sample_size=48.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=39.3, ups=0.82, wpb=48.2, bsz=16, num_updates=10500, lr=1.65618e-05, gnorm=4.762, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17605
2022-07-12 17:50:37 - progress_bar.py[line:274] - INFO: epoch 009:    281 / 1283 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.5, ups=0.79, wpb=53.6, bsz=16, num_updates=10510, lr=1.6528e-05, gnorm=3.408, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=17617
2022-07-12 17:50:50 - progress_bar.py[line:274] - INFO: epoch 009:    291 / 1283 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.9, ups=0.81, wpb=52, bsz=16, num_updates=10520, lr=1.64941e-05, gnorm=3.144, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17630
2022-07-12 17:51:02 - progress_bar.py[line:274] - INFO: epoch 009:    301 / 1283 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.9, ups=0.8, wpb=52.1, bsz=16, num_updates=10530, lr=1.64603e-05, gnorm=3.404, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17642
2022-07-12 17:51:14 - progress_bar.py[line:274] - INFO: epoch 009:    311 / 1283 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.6, ups=0.81, wpb=52.7, bsz=16, num_updates=10540, lr=1.64265e-05, gnorm=3.966, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17655
2022-07-12 17:51:27 - progress_bar.py[line:274] - INFO: epoch 009:    321 / 1283 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.7, ups=0.8, wpb=53.3, bsz=16, num_updates=10550, lr=1.63927e-05, gnorm=4.673, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17667
2022-07-12 17:51:39 - progress_bar.py[line:274] - INFO: epoch 009:    331 / 1283 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.3, ups=0.81, wpb=52.5, bsz=16, num_updates=10560, lr=1.63588e-05, gnorm=5.879, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17680
2022-07-12 17:51:52 - progress_bar.py[line:274] - INFO: epoch 009:    341 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.3, ups=0.8, wpb=51.8, bsz=16, num_updates=10570, lr=1.6325e-05, gnorm=3.78, clip=100, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=17692
2022-07-12 17:52:04 - progress_bar.py[line:274] - INFO: epoch 009:    351 / 1283 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=48.2, nsentences=16, sample_size=48.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=39, ups=0.81, wpb=48.2, bsz=16, num_updates=10580, lr=1.62912e-05, gnorm=5.691, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17704
2022-07-12 17:52:16 - progress_bar.py[line:274] - INFO: epoch 009:    361 / 1283 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.3, ups=0.82, wpb=53, bsz=16, num_updates=10590, lr=1.62574e-05, gnorm=3.473, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=17717
2022-07-12 17:52:29 - progress_bar.py[line:274] - INFO: epoch 009:    371 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=39.7, ups=0.8, wpb=49.4, bsz=16, num_updates=10600, lr=1.62235e-05, gnorm=4.108, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17729
2022-07-12 17:52:41 - progress_bar.py[line:274] - INFO: epoch 009:    381 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.6, ups=0.8, wpb=51.8, bsz=16, num_updates=10610, lr=1.61897e-05, gnorm=3.099, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=17742
2022-07-12 17:52:54 - progress_bar.py[line:274] - INFO: epoch 009:    391 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=44.8, ups=0.81, wpb=55.1, bsz=16, num_updates=10620, lr=1.61559e-05, gnorm=4.545, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=17754
2022-07-12 17:53:06 - progress_bar.py[line:274] - INFO: epoch 009:    401 / 1283 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.8, ups=0.81, wpb=53.7, bsz=16, num_updates=10630, lr=1.6122e-05, gnorm=2.898, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17766
2022-07-12 17:53:18 - progress_bar.py[line:274] - INFO: epoch 009:    411 / 1283 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.4, ups=0.8, wpb=51.5, bsz=16, num_updates=10640, lr=1.60882e-05, gnorm=5.075, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17779
2022-07-12 17:53:31 - progress_bar.py[line:274] - INFO: epoch 009:    421 / 1283 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.6, ups=0.81, wpb=50.2, bsz=16, num_updates=10650, lr=1.60544e-05, gnorm=2.834, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17791
2022-07-12 17:53:43 - progress_bar.py[line:274] - INFO: epoch 009:    431 / 1283 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.9, ups=0.81, wpb=51.6, bsz=16, num_updates=10660, lr=1.60206e-05, gnorm=4.66, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17803
2022-07-12 17:53:56 - progress_bar.py[line:274] - INFO: epoch 009:    441 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.2, ups=0.8, wpb=51.5, bsz=16, num_updates=10670, lr=1.59867e-05, gnorm=3.422, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17816
2022-07-12 17:54:08 - progress_bar.py[line:274] - INFO: epoch 009:    451 / 1283 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.8, ups=0.81, wpb=50.3, bsz=16, num_updates=10680, lr=1.59529e-05, gnorm=4.611, clip=90, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=17828
2022-07-12 17:54:20 - progress_bar.py[line:274] - INFO: epoch 009:    461 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.1, ups=0.8, wpb=52.5, bsz=16, num_updates=10690, lr=1.59191e-05, gnorm=2.158, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17841
2022-07-12 17:54:33 - progress_bar.py[line:274] - INFO: epoch 009:    471 / 1283 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.2, ups=0.81, wpb=51.9, bsz=16, num_updates=10700, lr=1.58853e-05, gnorm=3.388, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17853
2022-07-12 17:54:45 - progress_bar.py[line:274] - INFO: epoch 009:    481 / 1283 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=43.7, ups=0.81, wpb=53.8, bsz=16, num_updates=10710, lr=1.58514e-05, gnorm=6.312, clip=90, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=17865
2022-07-12 17:54:57 - progress_bar.py[line:274] - INFO: epoch 009:    491 / 1283 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.4, ups=0.8, wpb=52.8, bsz=16, num_updates=10720, lr=1.58176e-05, gnorm=5.031, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17878
2022-07-12 17:55:10 - progress_bar.py[line:274] - INFO: epoch 009:    501 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.4, ups=0.81, wpb=52.5, bsz=16, num_updates=10730, lr=1.57838e-05, gnorm=4.608, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17890
2022-07-12 17:55:22 - progress_bar.py[line:274] - INFO: epoch 009:    511 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=55.3, nsentences=16, sample_size=55.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=45.1, ups=0.82, wpb=55.3, bsz=16, num_updates=10740, lr=1.57499e-05, gnorm=4.78, clip=100, loss_scale=128, train_wall=12, gb_free=2, ema_decay=0.9999, wall=17902
2022-07-12 17:55:35 - progress_bar.py[line:274] - INFO: epoch 009:    521 / 1283 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40, ups=0.8, wpb=49.8, bsz=16, num_updates=10750, lr=1.57161e-05, gnorm=4.838, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17915
2022-07-12 17:55:47 - progress_bar.py[line:274] - INFO: epoch 009:    531 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=44.1, ups=0.81, wpb=54.4, bsz=16, num_updates=10760, lr=1.56823e-05, gnorm=4.758, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17927
2022-07-12 17:55:59 - progress_bar.py[line:274] - INFO: epoch 009:    541 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43.1, ups=0.81, wpb=53.3, bsz=16, num_updates=10770, lr=1.56485e-05, gnorm=6.519, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17940
2022-07-12 17:56:12 - progress_bar.py[line:274] - INFO: epoch 009:    551 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43.3, ups=0.8, wpb=53.9, bsz=16, num_updates=10780, lr=1.56146e-05, gnorm=5.178, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17952
2022-07-12 17:56:24 - progress_bar.py[line:274] - INFO: epoch 009:    561 / 1283 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.6, ups=0.81, wpb=52.8, bsz=16, num_updates=10790, lr=1.55808e-05, gnorm=5.642, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=17964
2022-07-12 17:56:37 - progress_bar.py[line:274] - INFO: epoch 009:    571 / 1283 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=39.8, ups=0.79, wpb=50.1, bsz=16, num_updates=10800, lr=1.5547e-05, gnorm=4.829, clip=100, loss_scale=128, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=17977
2022-07-12 17:56:49 - progress_bar.py[line:274] - INFO: epoch 009:    581 / 1283 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.7, ups=0.81, wpb=52.6, bsz=16, num_updates=10810, lr=1.55132e-05, gnorm=2.803, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=17989
2022-07-12 17:57:02 - progress_bar.py[line:274] - INFO: epoch 009:    591 / 1283 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.7, ups=0.8, wpb=53.5, bsz=16, num_updates=10820, lr=1.54793e-05, gnorm=4.519, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18002
2022-07-12 17:57:14 - progress_bar.py[line:274] - INFO: epoch 009:    601 / 1283 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=41.2, ups=0.82, wpb=50.5, bsz=16, num_updates=10830, lr=1.54455e-05, gnorm=6.323, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18014
2022-07-12 17:57:26 - progress_bar.py[line:274] - INFO: epoch 009:    611 / 1283 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.5, ups=0.81, wpb=52.6, bsz=16, num_updates=10840, lr=1.54117e-05, gnorm=3.799, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18026
2022-07-12 17:57:39 - progress_bar.py[line:274] - INFO: epoch 009:    621 / 1283 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.3, ups=0.8, wpb=52.8, bsz=16, num_updates=10850, lr=1.53778e-05, gnorm=4.231, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18039
2022-07-12 17:57:51 - progress_bar.py[line:274] - INFO: epoch 009:    631 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.9, ups=0.81, wpb=51.5, bsz=16, num_updates=10860, lr=1.5344e-05, gnorm=6.149, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18051
2022-07-12 17:58:03 - progress_bar.py[line:274] - INFO: epoch 009:    641 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.7, ups=0.81, wpb=54, bsz=16, num_updates=10870, lr=1.53102e-05, gnorm=5.157, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=18064
2022-07-12 17:58:16 - progress_bar.py[line:274] - INFO: epoch 009:    651 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.5, ups=0.81, wpb=53.6, bsz=16, num_updates=10880, lr=1.52764e-05, gnorm=3.12, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18076
2022-07-12 17:58:28 - progress_bar.py[line:274] - INFO: epoch 009:    661 / 1283 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.7, ups=0.8, wpb=50.6, bsz=16, num_updates=10890, lr=1.52425e-05, gnorm=3.309, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18088
2022-07-12 17:58:41 - progress_bar.py[line:274] - INFO: epoch 009:    671 / 1283 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42.9, ups=0.81, wpb=53.3, bsz=16, num_updates=10900, lr=1.52087e-05, gnorm=6.634, clip=90, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18101
2022-07-12 17:58:53 - progress_bar.py[line:274] - INFO: epoch 009:    681 / 1283 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.4, ups=0.81, wpb=53.6, bsz=16, num_updates=10910, lr=1.51749e-05, gnorm=4.565, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18113
2022-07-12 17:59:05 - progress_bar.py[line:274] - INFO: epoch 009:    691 / 1283 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.5, ups=0.81, wpb=51.1, bsz=16, num_updates=10920, lr=1.51411e-05, gnorm=5.176, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18125
2022-07-12 17:59:18 - progress_bar.py[line:274] - INFO: epoch 009:    701 / 1283 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.1, ups=0.81, wpb=49.6, bsz=16, num_updates=10930, lr=1.51072e-05, gnorm=4.1, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18138
2022-07-12 17:59:30 - progress_bar.py[line:274] - INFO: epoch 009:    711 / 1283 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=51, nsentences=15.9, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.5, ups=0.81, wpb=51, bsz=15.9, num_updates=10940, lr=1.50734e-05, gnorm=4.104, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18150
2022-07-12 17:59:42 - progress_bar.py[line:274] - INFO: epoch 009:    721 / 1283 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44.1, ups=0.81, wpb=54.2, bsz=16, num_updates=10950, lr=1.50396e-05, gnorm=2.333, clip=90, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18162
2022-07-12 17:59:55 - progress_bar.py[line:274] - INFO: epoch 009:    731 / 1283 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.3, ups=0.8, wpb=52.7, bsz=16, num_updates=10960, lr=1.50058e-05, gnorm=2.576, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18175
2022-07-12 18:00:06 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 18:00:08 - progress_bar.py[line:274] - INFO: epoch 009:    742 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=38.7, ups=0.75, wpb=51.8, bsz=16, num_updates=10970, lr=1.49719e-05, gnorm=3.168, clip=100, loss_scale=64, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=18188
2022-07-12 18:00:20 - progress_bar.py[line:274] - INFO: epoch 009:    752 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.6, ups=0.81, wpb=53.7, bsz=16, num_updates=10980, lr=1.49381e-05, gnorm=2.961, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18200
2022-07-12 18:00:33 - progress_bar.py[line:274] - INFO: epoch 009:    762 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42, ups=0.81, wpb=51.9, bsz=16, num_updates=10990, lr=1.49043e-05, gnorm=3.591, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18213
2022-07-12 18:00:45 - progress_bar.py[line:274] - INFO: epoch 009:    772 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.8, ups=0.81, wpb=51.4, bsz=16, num_updates=11000, lr=1.48704e-05, gnorm=4.09, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=18225
2022-07-12 18:00:57 - progress_bar.py[line:274] - INFO: epoch 009:    782 / 1283 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.1, ups=0.81, wpb=53.4, bsz=16, num_updates=11010, lr=1.48366e-05, gnorm=2.966, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18238
2022-07-12 18:01:01 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 178.00 MiB (GPU 0; 10.73 GiB total capacity; 8.49 GiB already allocated; 95.19 MiB free; 9.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 18:01:01 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 69        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8407 MB |    9308 MB |    1306 TB |    1306 TB |
|       from large pool |    8262 MB |    9099 MB |    1295 TB |    1295 TB |
|       from small pool |     145 MB |     212 MB |      10 TB |      10 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8407 MB |    9308 MB |    1306 TB |    1306 TB |
|       from large pool |    8262 MB |    9099 MB |    1295 TB |    1295 TB |
|       from small pool |     145 MB |     212 MB |      10 TB |      10 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9522 MB |    9590 MB |   54544 MB |   45022 MB |
|       from large pool |    9372 MB |    9372 MB |   50812 MB |   41440 MB |
|       from small pool |     150 MB |     218 MB |    3732 MB |    3582 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     936 MB |    3038 MB |    1274 TB |    1274 TB |
|       from large pool |     931 MB |    3034 MB |    1262 TB |    1262 TB |
|       from small pool |       4 MB |      20 MB |      12 TB |      12 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  301449 K  |  301445 K  |
|       from large pool |     835    |     910    |   89848 K  |   89847 K  |
|       from small pool |    3218    |    3470    |  211601 K  |  211597 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  301449 K  |  301445 K  |
|       from large pool |     835    |     910    |   89848 K  |   89847 K  |
|       from small pool |    3218    |    3470    |  211601 K  |  211597 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     176    |     210    |    2325    |    2149    |
|       from large pool |     101    |     101    |     459    |     358    |
|       from small pool |      75    |     109    |    1866    |    1791    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      58    |     190    |  165280 K  |  165280 K  |
|       from large pool |      41    |      78    |   43798 K  |   43798 K  |
|       from small pool |      17    |     133    |  121481 K  |  121481 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 18:01:01 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 18:01:10 - progress_bar.py[line:274] - INFO: epoch 009:    793 / 1283 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.2, ups=0.78, wpb=53.9, bsz=16, num_updates=11020, lr=1.48028e-05, gnorm=2.928, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18250
2022-07-12 18:01:22 - progress_bar.py[line:274] - INFO: epoch 009:    803 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.8, ups=0.81, wpb=50.1, bsz=16, num_updates=11030, lr=1.4769e-05, gnorm=3.234, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18263
2022-07-12 18:01:35 - progress_bar.py[line:274] - INFO: epoch 009:    813 / 1283 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.5, ups=0.8, wpb=51.9, bsz=16, num_updates=11040, lr=1.47351e-05, gnorm=3.08, clip=90, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=18275
2022-07-12 18:01:47 - progress_bar.py[line:274] - INFO: epoch 009:    823 / 1283 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.3, ups=0.81, wpb=53.5, bsz=16, num_updates=11050, lr=1.47013e-05, gnorm=3.091, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=18287
2022-07-12 18:02:00 - progress_bar.py[line:274] - INFO: epoch 009:    833 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=39.8, ups=0.8, wpb=49.8, bsz=16, num_updates=11060, lr=1.46675e-05, gnorm=3.642, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18300
2022-07-12 18:02:12 - progress_bar.py[line:274] - INFO: epoch 009:    843 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.3, ups=0.81, wpb=51.2, bsz=16, num_updates=11070, lr=1.46337e-05, gnorm=3.155, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18312
2022-07-12 18:02:24 - progress_bar.py[line:274] - INFO: epoch 009:    853 / 1283 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.4, ups=0.81, wpb=52.1, bsz=16, num_updates=11080, lr=1.45998e-05, gnorm=2.525, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18325
2022-07-12 18:02:37 - progress_bar.py[line:274] - INFO: epoch 009:    863 / 1283 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.9, ups=0.81, wpb=50.7, bsz=16, num_updates=11090, lr=1.4566e-05, gnorm=3.478, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18337
2022-07-12 18:02:49 - progress_bar.py[line:274] - INFO: epoch 009:    873 / 1283 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.8, ups=0.8, wpb=50.9, bsz=16, num_updates=11100, lr=1.45322e-05, gnorm=4.621, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18350
2022-07-12 18:03:02 - progress_bar.py[line:274] - INFO: epoch 009:    883 / 1283 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.7, ups=0.81, wpb=52.4, bsz=16, num_updates=11110, lr=1.44983e-05, gnorm=2.526, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18362
2022-07-12 18:03:14 - progress_bar.py[line:274] - INFO: epoch 009:    893 / 1283 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42, ups=0.8, wpb=52.2, bsz=16, num_updates=11120, lr=1.44645e-05, gnorm=2.707, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18374
2022-07-12 18:03:26 - progress_bar.py[line:274] - INFO: epoch 009:    903 / 1283 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=56, nsentences=16, sample_size=56, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=45.1, ups=0.81, wpb=56, bsz=16, num_updates=11130, lr=1.44307e-05, gnorm=2.49, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18387
2022-07-12 18:03:39 - progress_bar.py[line:274] - INFO: epoch 009:    913 / 1283 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=39.9, ups=0.79, wpb=50.2, bsz=16, num_updates=11140, lr=1.43969e-05, gnorm=4.128, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=18399
2022-07-12 18:03:51 - progress_bar.py[line:274] - INFO: epoch 009:    923 / 1283 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.6, ups=0.82, wpb=50.9, bsz=16, num_updates=11150, lr=1.4363e-05, gnorm=2.855, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18411
2022-07-12 18:04:04 - progress_bar.py[line:274] - INFO: epoch 009:    933 / 1283 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.3, ups=0.81, wpb=51.2, bsz=16, num_updates=11160, lr=1.43292e-05, gnorm=2.872, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18424
2022-07-12 18:04:16 - progress_bar.py[line:274] - INFO: epoch 009:    943 / 1283 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.9, ups=0.8, wpb=53.4, bsz=16, num_updates=11170, lr=1.42954e-05, gnorm=2.119, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18436
2022-07-12 18:04:28 - progress_bar.py[line:274] - INFO: epoch 009:    953 / 1283 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.6, ups=0.81, wpb=52.4, bsz=16, num_updates=11180, lr=1.42616e-05, gnorm=2.109, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18449
2022-07-12 18:04:41 - progress_bar.py[line:274] - INFO: epoch 009:    963 / 1283 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.4, ups=0.81, wpb=52.5, bsz=16, num_updates=11190, lr=1.42277e-05, gnorm=2.692, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18461
2022-07-12 18:04:53 - progress_bar.py[line:274] - INFO: epoch 009:    973 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.1, ups=0.82, wpb=50.3, bsz=16, num_updates=11200, lr=1.41939e-05, gnorm=4.576, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18473
2022-07-12 18:05:05 - progress_bar.py[line:274] - INFO: epoch 009:    983 / 1283 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.4, ups=0.81, wpb=50.1, bsz=16, num_updates=11210, lr=1.41601e-05, gnorm=3.125, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18486
2022-07-12 18:05:18 - progress_bar.py[line:274] - INFO: epoch 009:    993 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43, ups=0.81, wpb=53.4, bsz=16, num_updates=11220, lr=1.41262e-05, gnorm=3.72, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18498
2022-07-12 18:05:30 - progress_bar.py[line:274] - INFO: epoch 009:   1003 / 1283 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.4, ups=0.8, wpb=50.3, bsz=16, num_updates=11230, lr=1.40924e-05, gnorm=3.104, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=18511
2022-07-12 18:05:43 - progress_bar.py[line:274] - INFO: epoch 009:   1013 / 1283 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=49.1, nsentences=16, sample_size=49.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=39.6, ups=0.81, wpb=49.1, bsz=16, num_updates=11240, lr=1.40586e-05, gnorm=4.253, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18523
2022-07-12 18:05:55 - progress_bar.py[line:274] - INFO: epoch 009:   1023 / 1283 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.7, ups=0.81, wpb=50.4, bsz=16, num_updates=11250, lr=1.40248e-05, gnorm=4.369, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18535
2022-07-12 18:06:07 - progress_bar.py[line:274] - INFO: epoch 009:   1033 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43, ups=0.81, wpb=53.2, bsz=16, num_updates=11260, lr=1.39909e-05, gnorm=3.056, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18548
2022-07-12 18:06:20 - progress_bar.py[line:274] - INFO: epoch 009:   1043 / 1283 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.3, ups=0.81, wpb=52, bsz=16, num_updates=11270, lr=1.39571e-05, gnorm=4.456, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18560
2022-07-12 18:06:32 - progress_bar.py[line:274] - INFO: epoch 009:   1053 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.5, ups=0.8, wpb=51.5, bsz=16, num_updates=11280, lr=1.39233e-05, gnorm=3.765, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18572
2022-07-12 18:06:45 - progress_bar.py[line:274] - INFO: epoch 009:   1063 / 1283 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.7, ups=0.81, wpb=54.1, bsz=16, num_updates=11290, lr=1.38895e-05, gnorm=3.108, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18585
2022-07-12 18:06:57 - progress_bar.py[line:274] - INFO: epoch 009:   1073 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.3, ups=0.8, wpb=51.3, bsz=16, num_updates=11300, lr=1.38556e-05, gnorm=4.159, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18597
2022-07-12 18:07:09 - progress_bar.py[line:274] - INFO: epoch 009:   1083 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.1, ups=0.81, wpb=53.2, bsz=16, num_updates=11310, lr=1.38218e-05, gnorm=4.12, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18610
2022-07-12 18:07:22 - progress_bar.py[line:274] - INFO: epoch 009:   1093 / 1283 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.7, ups=0.81, wpb=50.5, bsz=16, num_updates=11320, lr=1.3788e-05, gnorm=3.464, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18622
2022-07-12 18:07:34 - progress_bar.py[line:274] - INFO: epoch 009:   1103 / 1283 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.7, ups=0.8, wpb=50.6, bsz=16, num_updates=11330, lr=1.37541e-05, gnorm=4.033, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18634
2022-07-12 18:07:47 - progress_bar.py[line:274] - INFO: epoch 009:   1113 / 1283 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.1, ups=0.8, wpb=51.1, bsz=16, num_updates=11340, lr=1.37203e-05, gnorm=3.848, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18647
2022-07-12 18:07:59 - progress_bar.py[line:274] - INFO: epoch 009:   1123 / 1283 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.2, ups=0.8, wpb=53.7, bsz=16, num_updates=11350, lr=1.36865e-05, gnorm=2.804, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18659
2022-07-12 18:08:11 - progress_bar.py[line:274] - INFO: epoch 009:   1133 / 1283 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41, ups=0.81, wpb=50.5, bsz=16, num_updates=11360, lr=1.36527e-05, gnorm=4.838, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18672
2022-07-12 18:08:24 - progress_bar.py[line:274] - INFO: epoch 009:   1143 / 1283 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.7, ups=0.81, wpb=53, bsz=16, num_updates=11370, lr=1.36188e-05, gnorm=3.414, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18684
2022-07-12 18:08:36 - progress_bar.py[line:274] - INFO: epoch 009:   1153 / 1283 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=40.8, ups=0.8, wpb=51.1, bsz=16, num_updates=11380, lr=1.3585e-05, gnorm=4.845, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18697
2022-07-12 18:08:49 - progress_bar.py[line:274] - INFO: epoch 009:   1163 / 1283 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.4, ups=0.81, wpb=52.3, bsz=16, num_updates=11390, lr=1.35512e-05, gnorm=3.172, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18709
2022-07-12 18:08:51 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-12 18:09:02 - progress_bar.py[line:274] - INFO: epoch 009:   1174 / 1283 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=38.8, ups=0.74, wpb=52.2, bsz=16, num_updates=11400, lr=1.35174e-05, gnorm=2.433, clip=90, loss_scale=32, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=18722
2022-07-12 18:09:15 - progress_bar.py[line:274] - INFO: epoch 009:   1184 / 1283 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.2, ups=0.81, wpb=53.6, bsz=16, num_updates=11410, lr=1.34835e-05, gnorm=4.146, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18735
2022-07-12 18:09:27 - progress_bar.py[line:274] - INFO: epoch 009:   1194 / 1283 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.9, ups=0.81, wpb=50.7, bsz=16, num_updates=11420, lr=1.34497e-05, gnorm=5.061, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18747
2022-07-12 18:09:39 - progress_bar.py[line:274] - INFO: epoch 009:   1204 / 1283 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42, ups=0.81, wpb=51.8, bsz=16, num_updates=11430, lr=1.34159e-05, gnorm=5.351, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18759
2022-07-12 18:09:52 - progress_bar.py[line:274] - INFO: epoch 009:   1214 / 1283 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.1, ups=0.81, wpb=49.6, bsz=16, num_updates=11440, lr=1.3382e-05, gnorm=4.649, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=18772
2022-07-12 18:10:04 - progress_bar.py[line:274] - INFO: epoch 009:   1224 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.7, ups=0.81, wpb=51.5, bsz=16, num_updates=11450, lr=1.33482e-05, gnorm=2.333, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=18784
2022-07-12 18:10:16 - progress_bar.py[line:274] - INFO: epoch 009:   1234 / 1283 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.4, ups=0.81, wpb=51.1, bsz=16, num_updates=11460, lr=1.33144e-05, gnorm=3.449, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18797
2022-07-12 18:10:29 - progress_bar.py[line:274] - INFO: epoch 009:   1244 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.4, ups=0.8, wpb=50.2, bsz=16, num_updates=11470, lr=1.32806e-05, gnorm=4.4, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=18809
2022-07-12 18:10:41 - progress_bar.py[line:274] - INFO: epoch 009:   1254 / 1283 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.1, ups=0.81, wpb=53.4, bsz=16, num_updates=11480, lr=1.32467e-05, gnorm=3.771, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=18821
2022-07-12 18:10:54 - progress_bar.py[line:274] - INFO: epoch 009:   1264 / 1283 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.7, ups=0.8, wpb=52, bsz=16, num_updates=11490, lr=1.32129e-05, gnorm=3.152, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=18834
2022-07-12 18:11:06 - progress_bar.py[line:274] - INFO: epoch 009:   1274 / 1283 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=39.8, ups=0.8, wpb=49.8, bsz=16, num_updates=11500, lr=1.31791e-05, gnorm=3.607, clip=100, loss_scale=32, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=18846
2022-07-12 18:11:17 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 18:19:21 - progress_bar.py[line:282] - INFO: epoch 009 | valid on 'valid' subset | loss 1.399 | loss_v1 0 | loss_v2 0 | nll_loss 1.143 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.21 | vqa_score 0.3485 | wps 30.3 | wpb 13.6 | bsz 4 | num_updates 11509 | best_vqa_score 0.3485
2022-07-12 18:19:21 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 9 @ 11509 updates
2022-07-12 18:19:21 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints9.pt
2022-07-12 18:19:52 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints9.pt
2022-07-12 18:20:49 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints9.pt (epoch 9 @ 11509 updates, score 0.3485) (writing took 88.25248329492752 seconds)
2022-07-12 18:20:49 - train.py[line:323] - INFO: end of epoch 9 (average epoch stats below)
2022-07-12 18:20:49 - progress_bar.py[line:282] - INFO: epoch 009 | loss 0.558 | loss_v1 0 | loss_v2 0 | nll_loss 0.131 | ntokens 51.941 | nsentences 15.996 | sample_size 51.941 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.09 | wps 30.7 | ups 0.59 | wpb 51.9 | bsz 16 | num_updates 11509 | lr 1.31486e-05 | gnorm 3.936 | clip 97.7 | loss_scale 32 | train_wall 1580 | gb_free 2.3 | ema_decay 0.9999 | wall 19429
2022-07-12 18:20:49 - trainer.py[line:639] - INFO: loading train data for epoch 10
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 18:20:51 - trainer.py[line:703] - INFO: begin training epoch 10
2022-07-12 18:20:51 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 18:20:53 - progress_bar.py[line:274] - INFO: epoch 010:      1 / 1283 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=47.7, nsentences=15.6, sample_size=47.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=0.8, ups=0.02, wpb=47.7, bsz=15.6, num_updates=11510, lr=1.31453e-05, gnorm=3.149, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19433
2022-07-12 18:21:06 - progress_bar.py[line:274] - INFO: epoch 010:     11 / 1283 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.6, ups=0.81, wpb=51.4, bsz=16, num_updates=11520, lr=1.31114e-05, gnorm=7.222, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19446
2022-07-12 18:21:18 - progress_bar.py[line:274] - INFO: epoch 010:     21 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.6, ups=0.81, wpb=51.5, bsz=16, num_updates=11530, lr=1.30776e-05, gnorm=3.247, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19458
2022-07-12 18:21:30 - progress_bar.py[line:274] - INFO: epoch 010:     31 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.2, ups=0.82, wpb=51.6, bsz=16, num_updates=11540, lr=1.30438e-05, gnorm=3.234, clip=90, loss_scale=32, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=19470
2022-07-12 18:21:42 - progress_bar.py[line:274] - INFO: epoch 010:     41 / 1283 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=44.6, ups=0.82, wpb=54.6, bsz=16, num_updates=11550, lr=1.30099e-05, gnorm=2.96, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19483
2022-07-12 18:21:55 - progress_bar.py[line:274] - INFO: epoch 010:     51 / 1283 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.1, ups=0.82, wpb=52.6, bsz=16, num_updates=11560, lr=1.29761e-05, gnorm=3.462, clip=90, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=19495
2022-07-12 18:22:07 - progress_bar.py[line:274] - INFO: epoch 010:     61 / 1283 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40, ups=0.8, wpb=49.8, bsz=16, num_updates=11570, lr=1.29423e-05, gnorm=2.967, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19507
2022-07-12 18:22:19 - progress_bar.py[line:274] - INFO: epoch 010:     71 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.9, ups=0.81, wpb=52.7, bsz=16, num_updates=11580, lr=1.29085e-05, gnorm=3.324, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19520
2022-07-12 18:22:32 - progress_bar.py[line:274] - INFO: epoch 010:     81 / 1283 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=11590, lr=1.28746e-05, gnorm=3.343, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19532
2022-07-12 18:22:44 - progress_bar.py[line:274] - INFO: epoch 010:     91 / 1283 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43, ups=0.82, wpb=52.8, bsz=16, num_updates=11600, lr=1.28408e-05, gnorm=3.759, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19544
2022-07-12 18:22:56 - progress_bar.py[line:274] - INFO: epoch 010:    101 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.2, ups=0.8, wpb=51.4, bsz=16, num_updates=11610, lr=1.2807e-05, gnorm=3.664, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19557
2022-07-12 18:23:09 - progress_bar.py[line:274] - INFO: epoch 010:    111 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.6, ups=0.8, wpb=49.5, bsz=16, num_updates=11620, lr=1.27732e-05, gnorm=3.228, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=19569
2022-07-12 18:23:21 - progress_bar.py[line:274] - INFO: epoch 010:    121 / 1283 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.5, ups=0.8, wpb=53, bsz=16, num_updates=11630, lr=1.27393e-05, gnorm=2.356, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19582
2022-07-12 18:23:34 - progress_bar.py[line:274] - INFO: epoch 010:    131 / 1283 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=49.3, nsentences=16, sample_size=49.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40, ups=0.81, wpb=49.3, bsz=16, num_updates=11640, lr=1.27055e-05, gnorm=3.111, clip=100, loss_scale=32, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=19594
2022-07-12 18:23:46 - progress_bar.py[line:274] - INFO: epoch 010:    141 / 1283 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=44.8, ups=0.81, wpb=55.1, bsz=16, num_updates=11650, lr=1.26717e-05, gnorm=3.027, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19606
2022-07-12 18:23:58 - progress_bar.py[line:274] - INFO: epoch 010:    151 / 1283 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.8, ups=0.82, wpb=53.5, bsz=16, num_updates=11660, lr=1.26378e-05, gnorm=5.248, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19618
2022-07-12 18:24:11 - progress_bar.py[line:274] - INFO: epoch 010:    161 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.1, ups=0.8, wpb=53.5, bsz=16, num_updates=11670, lr=1.2604e-05, gnorm=4.282, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19631
2022-07-12 18:24:23 - progress_bar.py[line:274] - INFO: epoch 010:    171 / 1283 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.9, ups=0.81, wpb=51.6, bsz=16, num_updates=11680, lr=1.25702e-05, gnorm=2.279, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19643
2022-07-12 18:24:35 - progress_bar.py[line:274] - INFO: epoch 010:    181 / 1283 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.4, ups=0.8, wpb=50.3, bsz=16, num_updates=11690, lr=1.25364e-05, gnorm=3.102, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19656
2022-07-12 18:24:48 - progress_bar.py[line:274] - INFO: epoch 010:    191 / 1283 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.1, ups=0.81, wpb=51.8, bsz=16, num_updates=11700, lr=1.25025e-05, gnorm=3.714, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19668
2022-07-12 18:25:00 - progress_bar.py[line:274] - INFO: epoch 010:    201 / 1283 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42, ups=0.81, wpb=51.9, bsz=16, num_updates=11710, lr=1.24687e-05, gnorm=4.823, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19680
2022-07-12 18:25:12 - progress_bar.py[line:274] - INFO: epoch 010:    211 / 1283 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.8, ups=0.82, wpb=51.1, bsz=16, num_updates=11720, lr=1.24349e-05, gnorm=3.24, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19693
2022-07-12 18:25:25 - progress_bar.py[line:274] - INFO: epoch 010:    221 / 1283 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.9, ups=0.81, wpb=51.7, bsz=16, num_updates=11730, lr=1.24011e-05, gnorm=4.494, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19705
2022-07-12 18:25:37 - progress_bar.py[line:274] - INFO: epoch 010:    231 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43, ups=0.81, wpb=53.3, bsz=16, num_updates=11740, lr=1.23672e-05, gnorm=2.916, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=19717
2022-07-12 18:25:49 - progress_bar.py[line:274] - INFO: epoch 010:    241 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.2, ups=0.8, wpb=51.2, bsz=16, num_updates=11750, lr=1.23334e-05, gnorm=3.138, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19730
2022-07-12 18:26:02 - progress_bar.py[line:274] - INFO: epoch 010:    251 / 1283 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=11760, lr=1.22996e-05, gnorm=2.751, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19742
2022-07-12 18:26:14 - progress_bar.py[line:274] - INFO: epoch 010:    261 / 1283 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.4, ups=0.82, wpb=52.8, bsz=16, num_updates=11770, lr=1.22657e-05, gnorm=3.625, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19754
2022-07-12 18:26:26 - progress_bar.py[line:274] - INFO: epoch 010:    271 / 1283 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=48.2, nsentences=16, sample_size=48.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=38.9, ups=0.81, wpb=48.2, bsz=16, num_updates=11780, lr=1.22319e-05, gnorm=2.504, clip=80, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19767
2022-07-12 18:26:39 - progress_bar.py[line:274] - INFO: epoch 010:    281 / 1283 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.5, ups=0.79, wpb=53.6, bsz=16, num_updates=11790, lr=1.21981e-05, gnorm=3.364, clip=100, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=19779
2022-07-12 18:26:51 - progress_bar.py[line:274] - INFO: epoch 010:    291 / 1283 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.9, ups=0.81, wpb=52, bsz=16, num_updates=11800, lr=1.21643e-05, gnorm=2.95, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19792
2022-07-12 18:27:04 - progress_bar.py[line:274] - INFO: epoch 010:    301 / 1283 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42, ups=0.81, wpb=52.1, bsz=16, num_updates=11810, lr=1.21304e-05, gnorm=2.647, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19804
2022-07-12 18:27:16 - progress_bar.py[line:274] - INFO: epoch 010:    311 / 1283 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.6, ups=0.81, wpb=52.7, bsz=16, num_updates=11820, lr=1.20966e-05, gnorm=2.482, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19816
2022-07-12 18:27:29 - progress_bar.py[line:274] - INFO: epoch 010:    321 / 1283 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.8, ups=0.8, wpb=53.3, bsz=16, num_updates=11830, lr=1.20628e-05, gnorm=3.028, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19829
2022-07-12 18:27:41 - progress_bar.py[line:274] - INFO: epoch 010:    331 / 1283 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.3, ups=0.81, wpb=52.5, bsz=16, num_updates=11840, lr=1.2029e-05, gnorm=3.412, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19841
2022-07-12 18:27:53 - progress_bar.py[line:274] - INFO: epoch 010:    341 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.6, ups=0.8, wpb=51.8, bsz=16, num_updates=11850, lr=1.19951e-05, gnorm=4.461, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19854
2022-07-12 18:28:06 - progress_bar.py[line:274] - INFO: epoch 010:    351 / 1283 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=48.2, nsentences=16, sample_size=48.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.3, ups=0.82, wpb=48.2, bsz=16, num_updates=11860, lr=1.19613e-05, gnorm=3.046, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19866
2022-07-12 18:28:18 - progress_bar.py[line:274] - INFO: epoch 010:    361 / 1283 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43, ups=0.81, wpb=53, bsz=16, num_updates=11870, lr=1.19275e-05, gnorm=2.718, clip=100, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=19878
2022-07-12 18:28:30 - progress_bar.py[line:274] - INFO: epoch 010:    371 / 1283 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.1, ups=0.81, wpb=49.4, bsz=16, num_updates=11880, lr=1.18936e-05, gnorm=4.077, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19891
2022-07-12 18:28:43 - progress_bar.py[line:274] - INFO: epoch 010:    381 / 1283 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.5, ups=0.8, wpb=51.8, bsz=16, num_updates=11890, lr=1.18598e-05, gnorm=3.164, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=19903
2022-07-12 18:28:55 - progress_bar.py[line:274] - INFO: epoch 010:    391 / 1283 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=44.5, ups=0.81, wpb=55.1, bsz=16, num_updates=11900, lr=1.1826e-05, gnorm=3.959, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=19916
2022-07-12 18:29:08 - progress_bar.py[line:274] - INFO: epoch 010:    401 / 1283 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.5, ups=0.81, wpb=53.7, bsz=16, num_updates=11910, lr=1.17922e-05, gnorm=2.267, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19928
2022-07-12 18:29:20 - progress_bar.py[line:274] - INFO: epoch 010:    411 / 1283 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.3, ups=0.8, wpb=51.5, bsz=16, num_updates=11920, lr=1.17583e-05, gnorm=2.422, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19940
2022-07-12 18:29:32 - progress_bar.py[line:274] - INFO: epoch 010:    421 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.5, ups=0.81, wpb=50.2, bsz=16, num_updates=11930, lr=1.17245e-05, gnorm=2.815, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19953
2022-07-12 18:29:45 - progress_bar.py[line:274] - INFO: epoch 010:    431 / 1283 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.7, ups=0.81, wpb=51.6, bsz=16, num_updates=11940, lr=1.16907e-05, gnorm=4, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=19965
2022-07-12 18:29:57 - progress_bar.py[line:274] - INFO: epoch 010:    441 / 1283 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.1, ups=0.8, wpb=51.5, bsz=16, num_updates=11950, lr=1.16569e-05, gnorm=4.066, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=19978
2022-07-12 18:30:10 - progress_bar.py[line:274] - INFO: epoch 010:    451 / 1283 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=40.6, ups=0.81, wpb=50.3, bsz=16, num_updates=11960, lr=1.1623e-05, gnorm=4.906, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=19990
2022-07-12 18:30:22 - progress_bar.py[line:274] - INFO: epoch 010:    461 / 1283 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.1, ups=0.8, wpb=52.5, bsz=16, num_updates=11970, lr=1.15892e-05, gnorm=3.659, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20002
2022-07-12 18:30:35 - progress_bar.py[line:274] - INFO: epoch 010:    471 / 1283 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.2, ups=0.81, wpb=51.9, bsz=16, num_updates=11980, lr=1.15554e-05, gnorm=3.822, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20015
2022-07-12 18:30:47 - progress_bar.py[line:274] - INFO: epoch 010:    481 / 1283 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=44, ups=0.82, wpb=53.8, bsz=16, num_updates=11990, lr=1.15215e-05, gnorm=4.334, clip=100, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=20027
2022-07-12 18:30:59 - progress_bar.py[line:274] - INFO: epoch 010:    491 / 1283 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.3, ups=0.8, wpb=52.8, bsz=16, num_updates=12000, lr=1.14877e-05, gnorm=3.394, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20039
2022-07-12 18:31:11 - progress_bar.py[line:274] - INFO: epoch 010:    501 / 1283 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.7, ups=0.81, wpb=52.5, bsz=16, num_updates=12010, lr=1.14539e-05, gnorm=5.244, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20052
2022-07-12 18:31:24 - progress_bar.py[line:274] - INFO: epoch 010:    511 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=55.3, nsentences=16, sample_size=55.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=45.1, ups=0.82, wpb=55.3, bsz=16, num_updates=12020, lr=1.14201e-05, gnorm=3.094, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=20064
2022-07-12 18:31:36 - progress_bar.py[line:274] - INFO: epoch 010:    521 / 1283 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.9, ups=0.8, wpb=49.8, bsz=16, num_updates=12030, lr=1.13862e-05, gnorm=3.562, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20076
2022-07-12 18:31:49 - progress_bar.py[line:274] - INFO: epoch 010:    531 / 1283 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=44.1, ups=0.81, wpb=54.4, bsz=16, num_updates=12040, lr=1.13524e-05, gnorm=4.769, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20089
2022-07-12 18:32:01 - progress_bar.py[line:274] - INFO: epoch 010:    541 / 1283 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=42.6, ups=0.8, wpb=53.3, bsz=16, num_updates=12050, lr=1.13186e-05, gnorm=3.735, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20101
2022-07-12 18:32:14 - progress_bar.py[line:274] - INFO: epoch 010:    551 / 1283 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.2, ups=0.8, wpb=53.9, bsz=16, num_updates=12060, lr=1.12848e-05, gnorm=4.177, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20114
2022-07-12 18:32:26 - progress_bar.py[line:274] - INFO: epoch 010:    561 / 1283 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.5, ups=0.81, wpb=52.8, bsz=16, num_updates=12070, lr=1.12509e-05, gnorm=3.021, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20126
2022-07-12 18:32:38 - progress_bar.py[line:274] - INFO: epoch 010:    571 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.1, ups=0.8, wpb=50.1, bsz=16, num_updates=12080, lr=1.12171e-05, gnorm=4.443, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20139
2022-07-12 18:32:51 - progress_bar.py[line:274] - INFO: epoch 010:    581 / 1283 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.7, ups=0.81, wpb=52.6, bsz=16, num_updates=12090, lr=1.11833e-05, gnorm=2.178, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20151
2022-07-12 18:33:03 - progress_bar.py[line:274] - INFO: epoch 010:    591 / 1283 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.7, ups=0.8, wpb=53.5, bsz=16, num_updates=12100, lr=1.11494e-05, gnorm=4.294, clip=90, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=20164
2022-07-12 18:33:16 - progress_bar.py[line:274] - INFO: epoch 010:    601 / 1283 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.9, ups=0.81, wpb=50.5, bsz=16, num_updates=12110, lr=1.11156e-05, gnorm=3.037, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20176
2022-07-12 18:33:28 - progress_bar.py[line:274] - INFO: epoch 010:    611 / 1283 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.7, ups=0.81, wpb=52.6, bsz=16, num_updates=12120, lr=1.10818e-05, gnorm=3.555, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20188
2022-07-12 18:33:41 - progress_bar.py[line:274] - INFO: epoch 010:    621 / 1283 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42, ups=0.8, wpb=52.8, bsz=16, num_updates=12130, lr=1.1048e-05, gnorm=2.806, clip=90, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=20201
2022-07-12 18:33:53 - progress_bar.py[line:274] - INFO: epoch 010:    631 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.7, ups=0.81, wpb=51.5, bsz=16, num_updates=12140, lr=1.10141e-05, gnorm=3.931, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20213
2022-07-12 18:34:05 - progress_bar.py[line:274] - INFO: epoch 010:    641 / 1283 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.5, ups=0.81, wpb=54, bsz=16, num_updates=12150, lr=1.09803e-05, gnorm=3.706, clip=90, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=20226
2022-07-12 18:34:18 - progress_bar.py[line:274] - INFO: epoch 010:    651 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.5, ups=0.81, wpb=53.6, bsz=16, num_updates=12160, lr=1.09465e-05, gnorm=4.616, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20238
2022-07-12 18:34:30 - progress_bar.py[line:274] - INFO: epoch 010:    661 / 1283 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.1, ups=0.81, wpb=50.6, bsz=16, num_updates=12170, lr=1.09127e-05, gnorm=3.022, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20250
2022-07-12 18:34:42 - progress_bar.py[line:274] - INFO: epoch 010:    671 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.9, ups=0.8, wpb=53.3, bsz=16, num_updates=12180, lr=1.08788e-05, gnorm=5.286, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20263
2022-07-12 18:34:55 - progress_bar.py[line:274] - INFO: epoch 010:    681 / 1283 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.4, ups=0.81, wpb=53.6, bsz=16, num_updates=12190, lr=1.0845e-05, gnorm=4.797, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20275
2022-07-12 18:35:07 - progress_bar.py[line:274] - INFO: epoch 010:    691 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.5, ups=0.81, wpb=51.1, bsz=16, num_updates=12200, lr=1.08112e-05, gnorm=3.619, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20287
2022-07-12 18:35:19 - progress_bar.py[line:274] - INFO: epoch 010:    701 / 1283 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.3, ups=0.81, wpb=49.6, bsz=16, num_updates=12210, lr=1.07773e-05, gnorm=2.757, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20300
2022-07-12 18:35:32 - progress_bar.py[line:274] - INFO: epoch 010:    711 / 1283 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.5, ups=0.81, wpb=51.4, bsz=16, num_updates=12220, lr=1.07435e-05, gnorm=6.057, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20312
2022-07-12 18:35:44 - progress_bar.py[line:274] - INFO: epoch 010:    721 / 1283 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=44, ups=0.81, wpb=54.1, bsz=16, num_updates=12230, lr=1.07097e-05, gnorm=3.645, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20324
2022-07-12 18:35:56 - progress_bar.py[line:274] - INFO: epoch 010:    731 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43, ups=0.81, wpb=52.8, bsz=16, num_updates=12240, lr=1.06759e-05, gnorm=3.447, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20337
2022-07-12 18:36:09 - progress_bar.py[line:274] - INFO: epoch 010:    741 / 1283 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=42, ups=0.82, wpb=51.5, bsz=16, num_updates=12250, lr=1.0642e-05, gnorm=5.133, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=20349
2022-07-12 18:36:21 - progress_bar.py[line:274] - INFO: epoch 010:    751 / 1283 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44, ups=0.81, wpb=54.2, bsz=16, num_updates=12260, lr=1.06082e-05, gnorm=4.532, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20361
2022-07-12 18:36:33 - progress_bar.py[line:274] - INFO: epoch 010:    761 / 1283 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.3, ups=0.81, wpb=51.9, bsz=16, num_updates=12270, lr=1.05744e-05, gnorm=6.506, clip=90, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=20373
2022-07-12 18:36:46 - progress_bar.py[line:274] - INFO: epoch 010:    771 / 1283 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.4, ups=0.81, wpb=51.2, bsz=16, num_updates=12280, lr=1.05406e-05, gnorm=4.675, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20386
2022-07-12 18:36:58 - progress_bar.py[line:274] - INFO: epoch 010:    781 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.2, ups=0.8, wpb=52.7, bsz=16, num_updates=12290, lr=1.05067e-05, gnorm=4.899, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20398
2022-07-12 18:37:10 - progress_bar.py[line:274] - INFO: epoch 010:    791 / 1283 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=54.7, nsentences=16, sample_size=54.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=44.2, ups=0.81, wpb=54.7, bsz=16, num_updates=12300, lr=1.04729e-05, gnorm=3.298, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20411
2022-07-12 18:37:23 - progress_bar.py[line:274] - INFO: epoch 010:    801 / 1283 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.7, ups=0.8, wpb=50.6, bsz=16, num_updates=12310, lr=1.04391e-05, gnorm=4.674, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20423
2022-07-12 18:37:35 - progress_bar.py[line:274] - INFO: epoch 010:    811 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.6, ups=0.8, wpb=50.5, bsz=16, num_updates=12320, lr=1.04052e-05, gnorm=4.326, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20436
2022-07-12 18:37:48 - progress_bar.py[line:274] - INFO: epoch 010:    821 / 1283 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.3, ups=0.8, wpb=53.9, bsz=16, num_updates=12330, lr=1.03714e-05, gnorm=5.515, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20448
2022-07-12 18:38:00 - progress_bar.py[line:274] - INFO: epoch 010:    831 / 1283 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.4, ups=0.8, wpb=50.4, bsz=16, num_updates=12340, lr=1.03376e-05, gnorm=4.913, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20460
2022-07-12 18:38:13 - progress_bar.py[line:274] - INFO: epoch 010:    841 / 1283 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.9, ups=0.8, wpb=51.3, bsz=16, num_updates=12350, lr=1.03038e-05, gnorm=3.391, clip=100, loss_scale=64, train_wall=13, gb_free=1.9, ema_decay=0.9999, wall=20473
2022-07-12 18:38:25 - progress_bar.py[line:274] - INFO: epoch 010:    851 / 1283 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.5, ups=0.81, wpb=52.2, bsz=16, num_updates=12360, lr=1.02699e-05, gnorm=3.785, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20485
2022-07-12 18:38:37 - progress_bar.py[line:274] - INFO: epoch 010:    861 / 1283 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.6, ups=0.81, wpb=50, bsz=16, num_updates=12370, lr=1.02361e-05, gnorm=3.614, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20498
2022-07-12 18:38:50 - progress_bar.py[line:274] - INFO: epoch 010:    871 / 1283 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.6, ups=0.81, wpb=51.6, bsz=16, num_updates=12380, lr=1.02023e-05, gnorm=4.759, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20510
2022-07-12 18:39:02 - progress_bar.py[line:274] - INFO: epoch 010:    881 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.9, ups=0.81, wpb=51.7, bsz=16, num_updates=12390, lr=1.01685e-05, gnorm=4.692, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20522
2022-07-12 18:39:15 - progress_bar.py[line:274] - INFO: epoch 010:    891 / 1283 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.7, ups=0.8, wpb=52.4, bsz=16, num_updates=12400, lr=1.01346e-05, gnorm=6.07, clip=90, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=20535
2022-07-12 18:39:24 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 10.73 GiB total capacity; 8.19 GiB already allocated; 175.38 MiB free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 18:39:24 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8284 MB |    8997 MB |    1471 TB |    1471 TB |
|       from large pool |    8138 MB |    8803 MB |    1459 TB |    1459 TB |
|       from small pool |     145 MB |     195 MB |      12 TB |      12 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8284 MB |    8997 MB |    1471 TB |    1471 TB |
|       from large pool |    8138 MB |    8803 MB |    1459 TB |    1459 TB |
|       from small pool |     145 MB |     195 MB |      12 TB |      12 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9442 MB |    9500 MB |   57172 MB |   47730 MB |
|       from large pool |    9294 MB |    9294 MB |   53118 MB |   43824 MB |
|       from small pool |     148 MB |     206 MB |    4054 MB |    3906 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1157 MB |    3629 MB |    1437 TB |    1437 TB |
|       from large pool |    1155 MB |    3626 MB |    1423 TB |    1423 TB |
|       from small pool |       2 MB |      24 MB |      13 TB |      13 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4055    |    4378    |  339512 K  |  339508 K  |
|       from large pool |     835    |     904    |  101193 K  |  101192 K  |
|       from small pool |    3220    |    3488    |  238319 K  |  238315 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4055    |    4378    |  339512 K  |  339508 K  |
|       from large pool |     835    |     904    |  101193 K  |  101192 K  |
|       from small pool |    3220    |    3488    |  238319 K  |  238315 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     199    |    2498    |    2328    |
|       from large pool |      96    |      96    |     471    |     375    |
|       from small pool |      74    |     103    |    2027    |    1953    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |     181    |  185678 K  |  185678 K  |
|       from large pool |      33    |      70    |   49175 K  |   49175 K  |
|       from small pool |      14    |     124    |  136503 K  |  136503 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 18:39:24 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 18:39:28 - progress_bar.py[line:274] - INFO: epoch 010:    902 / 1283 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=54.9, nsentences=16, sample_size=54.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.9, ups=0.76, wpb=54.9, bsz=16, num_updates=12410, lr=1.01008e-05, gnorm=3.052, clip=90, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=20548
2022-07-12 18:39:40 - progress_bar.py[line:274] - INFO: epoch 010:    912 / 1283 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=39.5, ups=0.79, wpb=49.8, bsz=16, num_updates=12420, lr=1.0067e-05, gnorm=5.916, clip=100, loss_scale=128, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=20561
2022-07-12 18:39:53 - progress_bar.py[line:274] - INFO: epoch 010:    922 / 1283 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.2, ups=0.81, wpb=50.9, bsz=16, num_updates=12430, lr=1.00332e-05, gnorm=3.449, clip=100, loss_scale=128, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=20573
2022-07-12 18:40:05 - progress_bar.py[line:274] - INFO: epoch 010:    932 / 1283 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.4, ups=0.8, wpb=51.6, bsz=16, num_updates=12440, lr=9.99932e-06, gnorm=4.634, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20585
2022-07-12 18:40:17 - progress_bar.py[line:274] - INFO: epoch 010:    942 / 1283 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.6, ups=0.81, wpb=52.3, bsz=16, num_updates=12450, lr=9.9655e-06, gnorm=3.761, clip=90, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20598
2022-07-12 18:40:30 - progress_bar.py[line:274] - INFO: epoch 010:    952 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.8, ups=0.8, wpb=53.4, bsz=16, num_updates=12460, lr=9.93167e-06, gnorm=3.258, clip=90, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20610
2022-07-12 18:40:43 - progress_bar.py[line:274] - INFO: epoch 010:    962 / 1283 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.4, ups=0.8, wpb=52, bsz=16, num_updates=12470, lr=9.89784e-06, gnorm=5.208, clip=100, loss_scale=128, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=20623
2022-07-12 18:40:55 - progress_bar.py[line:274] - INFO: epoch 010:    972 / 1283 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.3, ups=0.81, wpb=50.8, bsz=16, num_updates=12480, lr=9.86401e-06, gnorm=4.485, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20635
2022-07-12 18:41:07 - progress_bar.py[line:274] - INFO: epoch 010:    982 / 1283 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40, ups=0.81, wpb=49.7, bsz=16, num_updates=12490, lr=9.83019e-06, gnorm=4.676, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20647
2022-07-12 18:41:20 - progress_bar.py[line:274] - INFO: epoch 010:    992 / 1283 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.9, ups=0.81, wpb=51.9, bsz=16, num_updates=12500, lr=9.79636e-06, gnorm=6.871, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20660
2022-07-12 18:41:32 - progress_bar.py[line:274] - INFO: epoch 010:   1002 / 1283 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.7, ups=0.81, wpb=51.7, bsz=16, num_updates=12510, lr=9.76253e-06, gnorm=4.752, clip=90, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20672
2022-07-12 18:41:44 - progress_bar.py[line:274] - INFO: epoch 010:   1012 / 1283 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=48.8, nsentences=16, sample_size=48.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=39.2, ups=0.8, wpb=48.8, bsz=16, num_updates=12520, lr=9.72871e-06, gnorm=4.692, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20685
2022-07-12 18:41:57 - progress_bar.py[line:274] - INFO: epoch 010:   1022 / 1283 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.2, ups=0.81, wpb=50.8, bsz=16, num_updates=12530, lr=9.69488e-06, gnorm=9.865, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20697
2022-07-12 18:42:09 - progress_bar.py[line:274] - INFO: epoch 010:   1032 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.7, ups=0.8, wpb=53.2, bsz=16, num_updates=12540, lr=9.66105e-06, gnorm=5.305, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20709
2022-07-12 18:42:22 - progress_bar.py[line:274] - INFO: epoch 010:   1042 / 1283 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41.6, ups=0.8, wpb=51.9, bsz=16, num_updates=12550, lr=9.62722e-06, gnorm=5.642, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20722
2022-07-12 18:42:34 - progress_bar.py[line:274] - INFO: epoch 010:   1052 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.5, ups=0.8, wpb=51.9, bsz=16, num_updates=12560, lr=9.5934e-06, gnorm=4.715, clip=90, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20734
2022-07-12 18:42:47 - progress_bar.py[line:274] - INFO: epoch 010:   1062 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=44.1, ups=0.81, wpb=54.6, bsz=16, num_updates=12570, lr=9.55957e-06, gnorm=5.337, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20747
2022-07-12 18:42:59 - progress_bar.py[line:274] - INFO: epoch 010:   1072 / 1283 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=40.5, ups=0.81, wpb=50.2, bsz=16, num_updates=12580, lr=9.52574e-06, gnorm=7.452, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20759
2022-07-12 18:43:11 - progress_bar.py[line:274] - INFO: epoch 010:   1082 / 1283 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=43.3, ups=0.81, wpb=53.8, bsz=16, num_updates=12590, lr=9.49192e-06, gnorm=5.989, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20772
2022-07-12 18:43:24 - progress_bar.py[line:274] - INFO: epoch 010:   1092 / 1283 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.2, ups=0.81, wpb=50.7, bsz=16, num_updates=12600, lr=9.45809e-06, gnorm=4.268, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20784
2022-07-12 18:43:36 - progress_bar.py[line:274] - INFO: epoch 010:   1102 / 1283 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.6, ups=0.8, wpb=50.6, bsz=16, num_updates=12610, lr=9.42426e-06, gnorm=5.576, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20796
2022-07-12 18:43:49 - progress_bar.py[line:274] - INFO: epoch 010:   1112 / 1283 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41, ups=0.8, wpb=51.2, bsz=16, num_updates=12620, lr=9.39043e-06, gnorm=4.841, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20809
2022-07-12 18:44:01 - progress_bar.py[line:274] - INFO: epoch 010:   1122 / 1283 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.5, ups=0.8, wpb=52.8, bsz=16, num_updates=12630, lr=9.35661e-06, gnorm=3.718, clip=90, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20821
2022-07-12 18:44:13 - progress_bar.py[line:274] - INFO: epoch 010:   1132 / 1283 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.8, ups=0.81, wpb=50.5, bsz=16, num_updates=12640, lr=9.32278e-06, gnorm=5.028, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20834
2022-07-12 18:44:26 - progress_bar.py[line:274] - INFO: epoch 010:   1142 / 1283 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.2, ups=0.81, wpb=53.1, bsz=16, num_updates=12650, lr=9.28895e-06, gnorm=4.281, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20846
2022-07-12 18:44:38 - progress_bar.py[line:274] - INFO: epoch 010:   1152 / 1283 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=41, ups=0.8, wpb=51.3, bsz=16, num_updates=12660, lr=9.25512e-06, gnorm=6.223, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20859
2022-07-12 18:44:51 - progress_bar.py[line:274] - INFO: epoch 010:   1162 / 1283 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=42.8, ups=0.81, wpb=52.8, bsz=16, num_updates=12670, lr=9.2213e-06, gnorm=6.586, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=20871
2022-07-12 18:45:03 - progress_bar.py[line:274] - INFO: epoch 010:   1172 / 1283 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=42.3, ups=0.81, wpb=52.1, bsz=16, num_updates=12680, lr=9.18747e-06, gnorm=3.914, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20883
2022-07-12 18:45:15 - progress_bar.py[line:274] - INFO: epoch 010:   1182 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.3, ups=0.81, wpb=52.4, bsz=16, num_updates=12690, lr=9.15364e-06, gnorm=6.195, clip=90, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20896
2022-07-12 18:45:28 - progress_bar.py[line:274] - INFO: epoch 010:   1192 / 1283 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.1, ups=0.81, wpb=52, bsz=16, num_updates=12700, lr=9.11982e-06, gnorm=3.959, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20908
2022-07-12 18:45:40 - progress_bar.py[line:274] - INFO: epoch 010:   1202 / 1283 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=41.3, ups=0.8, wpb=51.6, bsz=16, num_updates=12710, lr=9.08599e-06, gnorm=4.916, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20920
2022-07-12 18:45:53 - progress_bar.py[line:274] - INFO: epoch 010:   1212 / 1283 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.6, ups=0.8, wpb=50.7, bsz=16, num_updates=12720, lr=9.05216e-06, gnorm=4.816, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20933
2022-07-12 18:46:05 - progress_bar.py[line:274] - INFO: epoch 010:   1222 / 1283 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.6, ups=0.81, wpb=51.3, bsz=16, num_updates=12730, lr=9.01833e-06, gnorm=5.371, clip=100, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=20945
2022-07-12 18:46:18 - progress_bar.py[line:274] - INFO: epoch 010:   1232 / 1283 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.9, ups=0.8, wpb=51.2, bsz=16, num_updates=12740, lr=8.98451e-06, gnorm=3.845, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20958
2022-07-12 18:46:30 - progress_bar.py[line:274] - INFO: epoch 010:   1242 / 1283 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=49, nsentences=16, sample_size=49, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=39.7, ups=0.81, wpb=49, bsz=16, num_updates=12750, lr=8.95068e-06, gnorm=5.965, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20970
2022-07-12 18:46:42 - progress_bar.py[line:274] - INFO: epoch 010:   1252 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.7, ups=0.81, wpb=52.4, bsz=16, num_updates=12760, lr=8.91685e-06, gnorm=4.354, clip=100, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=20982
2022-07-12 18:46:45 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 18:46:56 - progress_bar.py[line:274] - INFO: epoch 010:   1263 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=38.8, ups=0.73, wpb=53, bsz=16, num_updates=12770, lr=8.88303e-06, gnorm=3.539, clip=90, loss_scale=64, train_wall=14, gb_free=2.4, ema_decay=0.9999, wall=20996
2022-07-12 18:47:02 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 10.73 GiB total capacity; 8.40 GiB already allocated; 120.81 MiB free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 18:47:02 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 76        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8326 MB |    9097 MB |    1510 TB |    1510 TB |
|       from large pool |    8180 MB |    8896 MB |    1497 TB |    1497 TB |
|       from small pool |     145 MB |     202 MB |      12 TB |      12 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8326 MB |    9097 MB |    1510 TB |    1510 TB |
|       from large pool |    8180 MB |    8896 MB |    1497 TB |    1497 TB |
|       from small pool |     145 MB |     202 MB |      12 TB |      12 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9496 MB |    9562 MB |   57984 MB |   48488 MB |
|       from large pool |    9348 MB |    9348 MB |   53852 MB |   44504 MB |
|       from small pool |     148 MB |     214 MB |    4132 MB |    3984 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     997 MB |    4130 MB |    1477 TB |    1477 TB |
|       from large pool |     995 MB |    4128 MB |    1463 TB |    1463 TB |
|       from small pool |       2 MB |      20 MB |      13 TB |      13 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  345636 K  |  345632 K  |
|       from large pool |     835    |     904    |  103805 K  |  103804 K  |
|       from small pool |    3218    |    3476    |  241830 K  |  241827 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  345636 K  |  345632 K  |
|       from large pool |     835    |     904    |  103805 K  |  103804 K  |
|       from small pool |    3218    |    3476    |  241830 K  |  241827 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     171    |     204    |    2542    |    2371    |
|       from large pool |      97    |      97    |     476    |     379    |
|       from small pool |      74    |     107    |    2066    |    1992    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |     212    |  188726 K  |  188726 K  |
|       from large pool |      38    |      64    |   50378 K  |   50378 K  |
|       from small pool |       9    |     151    |  138347 K  |  138347 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 18:47:02 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 18:47:09 - progress_bar.py[line:274] - INFO: epoch 010:   1274 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=49.4, nsentences=15.9, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=38.2, ups=0.77, wpb=49.4, bsz=15.9, num_updates=12780, lr=8.8492e-06, gnorm=4.202, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=21009
2022-07-12 18:47:20 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 18:55:24 - progress_bar.py[line:282] - INFO: epoch 010 | valid on 'valid' subset | loss 1.391 | loss_v1 0 | loss_v2 0 | nll_loss 1.135 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.2 | vqa_score 0.3549 | wps 30.3 | wpb 13.6 | bsz 4 | num_updates 12789 | best_vqa_score 0.3549
2022-07-12 18:55:24 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 10 @ 12789 updates
2022-07-12 18:55:24 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints10.pt
2022-07-12 18:55:57 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints10.pt
2022-07-12 18:57:00 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints10.pt (epoch 10 @ 12789 updates, score 0.3549) (writing took 95.28527643694542 seconds)
2022-07-12 18:57:00 - train.py[line:323] - INFO: end of epoch 10 (average epoch stats below)
2022-07-12 18:57:00 - progress_bar.py[line:282] - INFO: epoch 010 | loss 0.55 | loss_v1 0 | loss_v2 0 | nll_loss 0.125 | ntokens 51.928 | nsentences 15.996 | sample_size 51.928 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.09 | wps 30.6 | ups 0.59 | wpb 51.9 | bsz 16 | num_updates 12789 | lr 8.81875e-06 | gnorm 4.146 | clip 95.5 | loss_scale 64 | train_wall 1582 | gb_free 2.3 | ema_decay 0.9999 | wall 21600
2022-07-12 18:57:00 - trainer.py[line:639] - INFO: loading train data for epoch 11
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 18:57:02 - trainer.py[line:703] - INFO: begin training epoch 11
2022-07-12 18:57:02 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 18:57:04 - progress_bar.py[line:274] - INFO: epoch 011:      1 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=47.7, nsentences=15.6, sample_size=47.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=0.8, ups=0.02, wpb=47.7, bsz=15.6, num_updates=12790, lr=8.81537e-06, gnorm=2.463, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21604
2022-07-12 18:57:16 - progress_bar.py[line:274] - INFO: epoch 011:     11 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.9, ups=0.81, wpb=51.4, bsz=16, num_updates=12800, lr=8.78154e-06, gnorm=3.447, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21616
2022-07-12 18:57:28 - progress_bar.py[line:274] - INFO: epoch 011:     21 / 1283 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.7, ups=0.81, wpb=51.5, bsz=16, num_updates=12810, lr=8.74772e-06, gnorm=2.825, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=21629
2022-07-12 18:57:41 - progress_bar.py[line:274] - INFO: epoch 011:     31 / 1283 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.9, ups=0.81, wpb=51.6, bsz=16, num_updates=12820, lr=8.71389e-06, gnorm=3.835, clip=100, loss_scale=64, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=21641
2022-07-12 18:57:53 - progress_bar.py[line:274] - INFO: epoch 011:     41 / 1283 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=44.3, ups=0.81, wpb=54.6, bsz=16, num_updates=12830, lr=8.68006e-06, gnorm=2.516, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21653
2022-07-12 18:58:06 - progress_bar.py[line:274] - INFO: epoch 011:     51 / 1283 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.1, ups=0.8, wpb=52.6, bsz=16, num_updates=12840, lr=8.64624e-06, gnorm=1.796, clip=90, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=21666
2022-07-12 18:58:18 - progress_bar.py[line:274] - INFO: epoch 011:     61 / 1283 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=39.4, ups=0.79, wpb=49.8, bsz=16, num_updates=12850, lr=8.61241e-06, gnorm=2.293, clip=90, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=21678
2022-07-12 18:58:35 - progress_bar.py[line:274] - INFO: epoch 011:     71 / 1283 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=30.9, ups=0.59, wpb=52.7, bsz=16, num_updates=12860, lr=8.57858e-06, gnorm=3.309, clip=100, loss_scale=64, train_wall=15, gb_free=2.3, ema_decay=0.9999, wall=21695
2022-07-12 18:58:48 - progress_bar.py[line:274] - INFO: epoch 011:     81 / 1283 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42, ups=0.8, wpb=52.4, bsz=16, num_updates=12870, lr=8.54475e-06, gnorm=2.891, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21708
2022-07-12 18:59:00 - progress_bar.py[line:274] - INFO: epoch 011:     91 / 1283 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42, ups=0.8, wpb=52.8, bsz=16, num_updates=12880, lr=8.51093e-06, gnorm=2.922, clip=100, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=21720
2022-07-12 18:59:13 - progress_bar.py[line:274] - INFO: epoch 011:    101 / 1283 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.2, ups=0.8, wpb=51.4, bsz=16, num_updates=12890, lr=8.4771e-06, gnorm=1.558, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21733
2022-07-12 18:59:25 - progress_bar.py[line:274] - INFO: epoch 011:    111 / 1283 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=39.7, ups=0.8, wpb=49.5, bsz=16, num_updates=12900, lr=8.44327e-06, gnorm=2.777, clip=80, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=21745
2022-07-12 18:59:38 - progress_bar.py[line:274] - INFO: epoch 011:    121 / 1283 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.4, ups=0.8, wpb=53, bsz=16, num_updates=12910, lr=8.40944e-06, gnorm=2.38, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21758
2022-07-12 18:59:50 - progress_bar.py[line:274] - INFO: epoch 011:    131 / 1283 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=49.3, nsentences=16, sample_size=49.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.2, ups=0.81, wpb=49.3, bsz=16, num_updates=12920, lr=8.37562e-06, gnorm=3.579, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=21770
2022-07-12 19:00:02 - progress_bar.py[line:274] - INFO: epoch 011:    141 / 1283 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44.9, ups=0.82, wpb=55.1, bsz=16, num_updates=12930, lr=8.34179e-06, gnorm=1.929, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21782
2022-07-12 19:00:14 - progress_bar.py[line:274] - INFO: epoch 011:    151 / 1283 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.8, ups=0.82, wpb=53.5, bsz=16, num_updates=12940, lr=8.30796e-06, gnorm=2.753, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21795
2022-07-12 19:00:22 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-12 19:00:28 - progress_bar.py[line:274] - INFO: epoch 011:    162 / 1283 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=38.1, ups=0.74, wpb=51.6, bsz=16, num_updates=12950, lr=8.27414e-06, gnorm=3.887, clip=90, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=21808
2022-07-12 19:00:40 - progress_bar.py[line:274] - INFO: epoch 011:    172 / 1283 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.6, ups=0.82, wpb=52.2, bsz=16, num_updates=12960, lr=8.24031e-06, gnorm=2.827, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21820
2022-07-12 19:00:53 - progress_bar.py[line:274] - INFO: epoch 011:    182 / 1283 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.1, ups=0.81, wpb=50.7, bsz=16, num_updates=12970, lr=8.20648e-06, gnorm=3.929, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=21833
2022-07-12 19:01:05 - progress_bar.py[line:274] - INFO: epoch 011:    192 / 1283 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.5, ups=0.81, wpb=52.3, bsz=16, num_updates=12980, lr=8.17265e-06, gnorm=2.671, clip=80, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=21845
2022-07-12 19:01:17 - progress_bar.py[line:274] - INFO: epoch 011:    202 / 1283 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41, ups=0.81, wpb=50.9, bsz=16, num_updates=12990, lr=8.13883e-06, gnorm=3.888, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21858
2022-07-12 19:01:30 - progress_bar.py[line:274] - INFO: epoch 011:    212 / 1283 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.6, ups=0.81, wpb=51.3, bsz=16, num_updates=13000, lr=8.105e-06, gnorm=2.818, clip=90, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=21870
2022-07-12 19:01:42 - progress_bar.py[line:274] - INFO: epoch 011:    222 / 1283 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.7, ups=0.81, wpb=51.5, bsz=16, num_updates=13010, lr=8.07117e-06, gnorm=2.764, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21882
2022-07-12 19:01:54 - progress_bar.py[line:274] - INFO: epoch 011:    232 / 1283 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.3, ups=0.81, wpb=53.3, bsz=16, num_updates=13020, lr=8.03735e-06, gnorm=2.159, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=21895
2022-07-12 19:02:07 - progress_bar.py[line:274] - INFO: epoch 011:    242 / 1283 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.7, ups=0.8, wpb=51.8, bsz=16, num_updates=13030, lr=8.00352e-06, gnorm=3.086, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21907
2022-07-12 19:02:19 - progress_bar.py[line:274] - INFO: epoch 011:    252 / 1283 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.7, ups=0.82, wpb=52.2, bsz=16, num_updates=13040, lr=7.96969e-06, gnorm=2.389, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21919
2022-07-12 19:02:31 - progress_bar.py[line:274] - INFO: epoch 011:    262 / 1283 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.5, ups=0.82, wpb=53, bsz=16, num_updates=13050, lr=7.93586e-06, gnorm=3.268, clip=90, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=21931
2022-07-12 19:02:43 - progress_bar.py[line:274] - INFO: epoch 011:    272 / 1283 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=47.8, nsentences=16, sample_size=47.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=38.8, ups=0.81, wpb=47.8, bsz=16, num_updates=13060, lr=7.90204e-06, gnorm=2.152, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21944
2022-07-12 19:02:50 - trainer.py[line:1303] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 10.73 GiB total capacity; 8.39 GiB already allocated; 89.00 MiB free; 9.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-07-12 19:02:50 - trainer.py[line:1306] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 77        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8320 MB |    9087 MB |    1560 TB |    1560 TB |
|       from large pool |    8174 MB |    8887 MB |    1547 TB |    1547 TB |
|       from small pool |     145 MB |     202 MB |      13 TB |      13 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8320 MB |    9087 MB |    1560 TB |    1560 TB |
|       from large pool |    8174 MB |    8887 MB |    1547 TB |    1547 TB |
|       from small pool |     145 MB |     202 MB |      13 TB |      13 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    9528 MB |    9594 MB |   58784 MB |   49256 MB |
|       from large pool |    9380 MB |    9380 MB |   54576 MB |   45196 MB |
|       from small pool |     148 MB |     214 MB |    4208 MB |    4060 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1207 MB |    3391 MB |    1527 TB |    1527 TB |
|       from large pool |    1205 MB |    3389 MB |    1513 TB |    1513 TB |
|       from small pool |       2 MB |      16 MB |      14 TB |      14 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4053    |    4376    |  365409 K  |  365405 K  |
|       from large pool |     835    |     904    |  107339 K  |  107338 K  |
|       from small pool |    3218    |    3476    |  258070 K  |  258067 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4053    |    4376    |  365409 K  |  365405 K  |
|       from large pool |     835    |     904    |  107339 K  |  107338 K  |
|       from small pool |    3218    |    3476    |  258070 K  |  258067 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     205    |    2585    |    2413    |
|       from large pool |      98    |      98    |     481    |     383    |
|       from small pool |      74    |     107    |    2104    |    2030    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      50    |     211    |  200179 K  |  200178 K  |
|       from large pool |      41    |      80    |   52009 K  |   52009 K  |
|       from small pool |       9    |     155    |  148169 K  |  148169 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-07-12 19:02:50 - trainer.py[line:797] - WARNING: attempting to recover from OOM in forward/backward pass
2022-07-12 19:02:56 - progress_bar.py[line:274] - INFO: epoch 011:    283 / 1283 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.5, ups=0.77, wpb=54, bsz=16, num_updates=13070, lr=7.86821e-06, gnorm=3.695, clip=100, loss_scale=32, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=21957
2022-07-12 19:03:09 - progress_bar.py[line:274] - INFO: epoch 011:    293 / 1283 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.3, ups=0.81, wpb=52.4, bsz=16, num_updates=13080, lr=7.83438e-06, gnorm=2.122, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21969
2022-07-12 19:03:21 - progress_bar.py[line:274] - INFO: epoch 011:    303 / 1283 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.9, ups=0.81, wpb=52, bsz=16, num_updates=13090, lr=7.80055e-06, gnorm=2.891, clip=60, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=21981
2022-07-12 19:03:34 - progress_bar.py[line:274] - INFO: epoch 011:    313 / 1283 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.7, ups=0.81, wpb=52.5, bsz=16, num_updates=13100, lr=7.76673e-06, gnorm=1.744, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=21994
2022-07-12 19:03:46 - progress_bar.py[line:274] - INFO: epoch 011:    323 / 1283 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.8, ups=0.8, wpb=54.4, bsz=16, num_updates=13110, lr=7.7329e-06, gnorm=2.913, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22006
2022-07-12 19:03:58 - progress_bar.py[line:274] - INFO: epoch 011:    333 / 1283 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.1, ups=0.8, wpb=51.2, bsz=16, num_updates=13120, lr=7.69907e-06, gnorm=2.516, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22019
2022-07-12 19:04:11 - progress_bar.py[line:274] - INFO: epoch 011:    343 / 1283 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.9, ups=0.81, wpb=51.9, bsz=16, num_updates=13130, lr=7.66525e-06, gnorm=2.448, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22031
2022-07-12 19:04:23 - progress_bar.py[line:274] - INFO: epoch 011:    353 / 1283 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=48, nsentences=16, sample_size=48, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39, ups=0.81, wpb=48, bsz=16, num_updates=13140, lr=7.63142e-06, gnorm=2.456, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22043
2022-07-12 19:04:35 - progress_bar.py[line:274] - INFO: epoch 011:    363 / 1283 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=43, ups=0.81, wpb=52.8, bsz=16, num_updates=13150, lr=7.59759e-06, gnorm=1.657, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22056
2022-07-12 19:04:48 - progress_bar.py[line:274] - INFO: epoch 011:    373 / 1283 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.9, ups=0.81, wpb=50.7, bsz=16, num_updates=13160, lr=7.56376e-06, gnorm=2.696, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22068
2022-07-12 19:05:00 - progress_bar.py[line:274] - INFO: epoch 011:    383 / 1283 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.2, ups=0.81, wpb=51.2, bsz=16, num_updates=13170, lr=7.52994e-06, gnorm=2.481, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22080
2022-07-12 19:05:12 - progress_bar.py[line:274] - INFO: epoch 011:    393 / 1283 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44.5, ups=0.82, wpb=54.6, bsz=16, num_updates=13180, lr=7.49611e-06, gnorm=2.711, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22093
2022-07-12 19:05:25 - progress_bar.py[line:274] - INFO: epoch 011:    403 / 1283 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.5, ups=0.81, wpb=53.8, bsz=16, num_updates=13190, lr=7.46228e-06, gnorm=2.5, clip=80, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22105
2022-07-12 19:05:37 - progress_bar.py[line:274] - INFO: epoch 011:    413 / 1283 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.6, ups=0.8, wpb=51.8, bsz=16, num_updates=13200, lr=7.42846e-06, gnorm=2.747, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22118
2022-07-12 19:05:50 - progress_bar.py[line:274] - INFO: epoch 011:    423 / 1283 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.9, ups=0.8, wpb=49.7, bsz=16, num_updates=13210, lr=7.39463e-06, gnorm=3.155, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22130
2022-07-12 19:06:02 - progress_bar.py[line:274] - INFO: epoch 011:    433 / 1283 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.1, ups=0.81, wpb=51.8, bsz=16, num_updates=13220, lr=7.3608e-06, gnorm=1.981, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22142
2022-07-12 19:06:14 - progress_bar.py[line:274] - INFO: epoch 011:    443 / 1283 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.1, ups=0.81, wpb=52.1, bsz=16, num_updates=13230, lr=7.32697e-06, gnorm=2.995, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22155
2022-07-12 19:06:27 - progress_bar.py[line:274] - INFO: epoch 011:    453 / 1283 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.5, ups=0.81, wpb=50.1, bsz=16, num_updates=13240, lr=7.29315e-06, gnorm=2.516, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22167
2022-07-12 19:06:39 - progress_bar.py[line:274] - INFO: epoch 011:    463 / 1283 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43, ups=0.8, wpb=53.5, bsz=16, num_updates=13250, lr=7.25932e-06, gnorm=2.53, clip=80, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22180
2022-07-12 19:06:52 - progress_bar.py[line:274] - INFO: epoch 011:    473 / 1283 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.1, ups=0.82, wpb=50.3, bsz=16, num_updates=13260, lr=7.22549e-06, gnorm=3.862, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22192
2022-07-12 19:07:04 - progress_bar.py[line:274] - INFO: epoch 011:    483 / 1283 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.3, ups=0.81, wpb=53.7, bsz=16, num_updates=13270, lr=7.19166e-06, gnorm=3.121, clip=90, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=22204
2022-07-12 19:07:16 - progress_bar.py[line:274] - INFO: epoch 011:    493 / 1283 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.4, ups=0.81, wpb=53.9, bsz=16, num_updates=13280, lr=7.15784e-06, gnorm=3.566, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22217
2022-07-12 19:07:29 - progress_bar.py[line:274] - INFO: epoch 011:    503 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.3, ups=0.81, wpb=52, bsz=16, num_updates=13290, lr=7.12401e-06, gnorm=4.335, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22229
2022-07-12 19:07:41 - progress_bar.py[line:274] - INFO: epoch 011:    513 / 1283 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=55.2, nsentences=16, sample_size=55.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=45.4, ups=0.82, wpb=55.2, bsz=16, num_updates=13300, lr=7.09018e-06, gnorm=4.194, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22241
2022-07-12 19:07:53 - progress_bar.py[line:274] - INFO: epoch 011:    523 / 1283 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.5, ups=0.8, wpb=50.5, bsz=16, num_updates=13310, lr=7.05636e-06, gnorm=3.967, clip=80, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22253
2022-07-12 19:08:06 - progress_bar.py[line:274] - INFO: epoch 011:    533 / 1283 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=44.5, ups=0.81, wpb=54.6, bsz=16, num_updates=13320, lr=7.02253e-06, gnorm=2.771, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22266
2022-07-12 19:08:18 - progress_bar.py[line:274] - INFO: epoch 011:    543 / 1283 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.9, ups=0.8, wpb=53.3, bsz=16, num_updates=13330, lr=6.9887e-06, gnorm=3.87, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22278
2022-07-12 19:08:30 - progress_bar.py[line:274] - INFO: epoch 011:    553 / 1283 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.7, ups=0.8, wpb=53.4, bsz=16, num_updates=13340, lr=6.95487e-06, gnorm=2.626, clip=90, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=22291
2022-07-12 19:08:43 - progress_bar.py[line:274] - INFO: epoch 011:    563 / 1283 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.4, ups=0.81, wpb=51.4, bsz=16, num_updates=13350, lr=6.92105e-06, gnorm=3.795, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22303
2022-07-12 19:08:55 - progress_bar.py[line:274] - INFO: epoch 011:    573 / 1283 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.3, ups=0.8, wpb=51.4, bsz=16, num_updates=13360, lr=6.88722e-06, gnorm=1.985, clip=80, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22316
2022-07-12 19:09:08 - progress_bar.py[line:274] - INFO: epoch 011:    583 / 1283 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.5, ups=0.81, wpb=52.2, bsz=16, num_updates=13370, lr=6.85339e-06, gnorm=2.805, clip=100, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22328
2022-07-12 19:09:20 - progress_bar.py[line:274] - INFO: epoch 011:    593 / 1283 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.1, ups=0.8, wpb=53.8, bsz=16, num_updates=13380, lr=6.81957e-06, gnorm=3.147, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22340
2022-07-12 19:09:32 - progress_bar.py[line:274] - INFO: epoch 011:    603 / 1283 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.5, ups=0.82, wpb=50.8, bsz=16, num_updates=13390, lr=6.78574e-06, gnorm=3.528, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22353
2022-07-12 19:09:45 - progress_bar.py[line:274] - INFO: epoch 011:    613 / 1283 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.5, ups=0.81, wpb=51.5, bsz=16, num_updates=13400, lr=6.75191e-06, gnorm=4.05, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22365
2022-07-12 19:09:57 - progress_bar.py[line:274] - INFO: epoch 011:    623 / 1283 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.1, ups=0.8, wpb=53.7, bsz=16, num_updates=13410, lr=6.71808e-06, gnorm=2.196, clip=90, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=22377
2022-07-12 19:10:09 - progress_bar.py[line:274] - INFO: epoch 011:    633 / 1283 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.6, ups=0.81, wpb=51.1, bsz=16, num_updates=13420, lr=6.68426e-06, gnorm=3.638, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22390
2022-07-12 19:10:22 - progress_bar.py[line:274] - INFO: epoch 011:    643 / 1283 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=55, nsentences=16, sample_size=55, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44.3, ups=0.81, wpb=55, bsz=16, num_updates=13430, lr=6.65043e-06, gnorm=2.491, clip=70, loss_scale=32, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=22402
2022-07-12 19:10:34 - progress_bar.py[line:274] - INFO: epoch 011:    653 / 1283 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.7, ups=0.82, wpb=52.3, bsz=16, num_updates=13440, lr=6.6166e-06, gnorm=2.735, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22414
2022-07-12 19:10:46 - progress_bar.py[line:274] - INFO: epoch 011:    663 / 1283 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=13450, lr=6.58278e-06, gnorm=2.58, clip=80, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22427
2022-07-12 19:10:59 - progress_bar.py[line:274] - INFO: epoch 011:    673 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.4, ups=0.8, wpb=51.6, bsz=16, num_updates=13460, lr=6.54895e-06, gnorm=4.983, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22439
2022-07-12 19:11:11 - progress_bar.py[line:274] - INFO: epoch 011:    683 / 1283 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.8, ups=0.81, wpb=54, bsz=16, num_updates=13470, lr=6.51512e-06, gnorm=3.17, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22451
2022-07-12 19:11:24 - progress_bar.py[line:274] - INFO: epoch 011:    693 / 1283 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.1, ups=0.81, wpb=50.8, bsz=16, num_updates=13480, lr=6.48129e-06, gnorm=3.182, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22464
2022-07-12 19:11:36 - progress_bar.py[line:274] - INFO: epoch 011:    703 / 1283 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.8, ups=0.82, wpb=49.8, bsz=16, num_updates=13490, lr=6.44747e-06, gnorm=2.009, clip=90, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=22476
2022-07-12 19:11:48 - progress_bar.py[line:274] - INFO: epoch 011:    713 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.2, ups=0.81, wpb=51.9, bsz=16, num_updates=13500, lr=6.41364e-06, gnorm=2.568, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22488
2022-07-12 19:12:00 - progress_bar.py[line:274] - INFO: epoch 011:    723 / 1283 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=54.5, nsentences=16, sample_size=54.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44.4, ups=0.82, wpb=54.5, bsz=16, num_updates=13510, lr=6.37981e-06, gnorm=2.045, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22501
2022-07-12 19:12:13 - progress_bar.py[line:274] - INFO: epoch 011:    733 / 1283 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.7, ups=0.81, wpb=51.5, bsz=16, num_updates=13520, lr=6.34598e-06, gnorm=4.456, clip=100, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22513
2022-07-12 19:12:25 - progress_bar.py[line:274] - INFO: epoch 011:    743 / 1283 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42, ups=0.81, wpb=51.8, bsz=16, num_updates=13530, lr=6.31216e-06, gnorm=3.054, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22525
2022-07-12 19:12:37 - progress_bar.py[line:274] - INFO: epoch 011:    753 / 1283 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=43.2, ups=0.81, wpb=53.4, bsz=16, num_updates=13540, lr=6.27833e-06, gnorm=3.102, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22538
2022-07-12 19:12:50 - progress_bar.py[line:274] - INFO: epoch 011:    763 / 1283 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.8, ups=0.82, wpb=52.4, bsz=16, num_updates=13550, lr=6.2445e-06, gnorm=4.261, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22550
2022-07-12 19:13:02 - progress_bar.py[line:274] - INFO: epoch 011:    773 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.9, ups=0.81, wpb=51.6, bsz=16, num_updates=13560, lr=6.21068e-06, gnorm=4.41, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22562
2022-07-12 19:13:14 - progress_bar.py[line:274] - INFO: epoch 011:    783 / 1283 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=54.7, nsentences=16, sample_size=54.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44, ups=0.8, wpb=54.7, bsz=16, num_updates=13570, lr=6.17685e-06, gnorm=2.854, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22575
2022-07-12 19:13:27 - progress_bar.py[line:274] - INFO: epoch 011:    793 / 1283 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.4, ups=0.81, wpb=52.6, bsz=16, num_updates=13580, lr=6.14302e-06, gnorm=3.425, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22587
2022-07-12 19:13:39 - progress_bar.py[line:274] - INFO: epoch 011:    803 / 1283 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.2, ups=0.82, wpb=50.3, bsz=16, num_updates=13590, lr=6.10919e-06, gnorm=2.672, clip=90, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=22599
2022-07-12 19:13:52 - progress_bar.py[line:274] - INFO: epoch 011:    813 / 1283 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.1, ups=0.79, wpb=52.1, bsz=16, num_updates=13600, lr=6.07537e-06, gnorm=3.588, clip=100, loss_scale=64, train_wall=13, gb_free=2, ema_decay=0.9999, wall=22612
2022-07-12 19:14:04 - progress_bar.py[line:274] - INFO: epoch 011:    823 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.1, ups=0.81, wpb=53.2, bsz=16, num_updates=13610, lr=6.04154e-06, gnorm=4.085, clip=100, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=22624
2022-07-12 19:14:17 - progress_bar.py[line:274] - INFO: epoch 011:    833 / 1283 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.8, ups=0.8, wpb=49.7, bsz=16, num_updates=13620, lr=6.00771e-06, gnorm=3.724, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22637
2022-07-12 19:14:29 - progress_bar.py[line:274] - INFO: epoch 011:    843 / 1283 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.5, ups=0.81, wpb=51.2, bsz=16, num_updates=13630, lr=5.97389e-06, gnorm=2.719, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22649
2022-07-12 19:14:41 - progress_bar.py[line:274] - INFO: epoch 011:    853 / 1283 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=13640, lr=5.94006e-06, gnorm=1.521, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22661
2022-07-12 19:14:53 - progress_bar.py[line:274] - INFO: epoch 011:    863 / 1283 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.4, ups=0.82, wpb=50.5, bsz=16, num_updates=13650, lr=5.90623e-06, gnorm=3.551, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22674
2022-07-12 19:15:06 - progress_bar.py[line:274] - INFO: epoch 011:    873 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.9, ups=0.8, wpb=50.9, bsz=16, num_updates=13660, lr=5.8724e-06, gnorm=3.885, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22686
2022-07-12 19:15:18 - progress_bar.py[line:274] - INFO: epoch 011:    883 / 1283 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=13670, lr=5.83858e-06, gnorm=2.04, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22698
2022-07-12 19:15:31 - progress_bar.py[line:274] - INFO: epoch 011:    893 / 1283 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.8, ups=0.8, wpb=52.2, bsz=16, num_updates=13680, lr=5.80475e-06, gnorm=3.556, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22711
2022-07-12 19:15:43 - progress_bar.py[line:274] - INFO: epoch 011:    903 / 1283 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=55.9, nsentences=16, sample_size=55.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=45.6, ups=0.82, wpb=55.9, bsz=16, num_updates=13690, lr=5.77092e-06, gnorm=3.639, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22723
2022-07-12 19:15:56 - progress_bar.py[line:274] - INFO: epoch 011:    913 / 1283 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=39.9, ups=0.79, wpb=50.2, bsz=16, num_updates=13700, lr=5.73709e-06, gnorm=5.513, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=22736
2022-07-12 19:16:08 - progress_bar.py[line:274] - INFO: epoch 011:    923 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.3, ups=0.81, wpb=51.1, bsz=16, num_updates=13710, lr=5.70327e-06, gnorm=2.589, clip=100, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=22748
2022-07-12 19:16:20 - progress_bar.py[line:274] - INFO: epoch 011:    933 / 1283 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.9, ups=0.8, wpb=51, bsz=16, num_updates=13720, lr=5.66944e-06, gnorm=2.349, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22761
2022-07-12 19:16:33 - progress_bar.py[line:274] - INFO: epoch 011:    943 / 1283 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=43.1, ups=0.81, wpb=53.4, bsz=16, num_updates=13730, lr=5.63561e-06, gnorm=2.074, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22773
2022-07-12 19:16:45 - progress_bar.py[line:274] - INFO: epoch 011:    953 / 1283 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.3, ups=0.81, wpb=52.5, bsz=16, num_updates=13740, lr=5.60179e-06, gnorm=2.802, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22785
2022-07-12 19:16:58 - progress_bar.py[line:274] - INFO: epoch 011:    963 / 1283 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.9, ups=0.8, wpb=52.5, bsz=16, num_updates=13750, lr=5.56796e-06, gnorm=2.431, clip=100, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=22798
2022-07-12 19:17:10 - progress_bar.py[line:274] - INFO: epoch 011:    973 / 1283 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41, ups=0.81, wpb=50.3, bsz=16, num_updates=13760, lr=5.53413e-06, gnorm=3.43, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22810
2022-07-12 19:17:23 - progress_bar.py[line:274] - INFO: epoch 011:    983 / 1283 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=40, ups=0.8, wpb=50.1, bsz=16, num_updates=13770, lr=5.5003e-06, gnorm=5.855, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22823
2022-07-12 19:17:35 - progress_bar.py[line:274] - INFO: epoch 011:    993 / 1283 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.4, ups=0.81, wpb=53.4, bsz=16, num_updates=13780, lr=5.46648e-06, gnorm=3.384, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22835
2022-07-12 19:17:47 - progress_bar.py[line:274] - INFO: epoch 011:   1003 / 1283 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.4, ups=0.8, wpb=50.3, bsz=16, num_updates=13790, lr=5.43265e-06, gnorm=4.44, clip=90, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=22848
2022-07-12 19:18:00 - progress_bar.py[line:274] - INFO: epoch 011:   1013 / 1283 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=49.2, nsentences=16, sample_size=49.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.5, ups=0.8, wpb=49.2, bsz=16, num_updates=13800, lr=5.39882e-06, gnorm=4.606, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22860
2022-07-12 19:18:12 - progress_bar.py[line:274] - INFO: epoch 011:   1023 / 1283 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.1, ups=0.82, wpb=50.4, bsz=16, num_updates=13810, lr=5.365e-06, gnorm=4.323, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=22872
2022-07-12 19:18:25 - progress_bar.py[line:274] - INFO: epoch 011:   1033 / 1283 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.7, ups=0.8, wpb=53.2, bsz=16, num_updates=13820, lr=5.33117e-06, gnorm=3.362, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=22885
2022-07-12 19:18:37 - progress_bar.py[line:274] - INFO: epoch 011:   1043 / 1283 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.9, ups=0.81, wpb=52, bsz=16, num_updates=13830, lr=5.29734e-06, gnorm=4.948, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22897
2022-07-12 19:18:49 - progress_bar.py[line:274] - INFO: epoch 011:   1053 / 1283 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.3, ups=0.81, wpb=51.3, bsz=16, num_updates=13840, lr=5.26351e-06, gnorm=2.336, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22910
2022-07-12 19:19:02 - progress_bar.py[line:274] - INFO: epoch 011:   1063 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=44.2, ups=0.81, wpb=54.4, bsz=16, num_updates=13850, lr=5.22969e-06, gnorm=4.443, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22922
2022-07-12 19:19:14 - progress_bar.py[line:274] - INFO: epoch 011:   1073 / 1283 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=40.7, ups=0.8, wpb=51.1, bsz=16, num_updates=13860, lr=5.19586e-06, gnorm=5.221, clip=100, loss_scale=64, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=22934
2022-07-12 19:19:27 - progress_bar.py[line:274] - INFO: epoch 011:   1083 / 1283 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=43, ups=0.81, wpb=53.3, bsz=16, num_updates=13870, lr=5.16203e-06, gnorm=5.493, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22947
2022-07-12 19:19:39 - progress_bar.py[line:274] - INFO: epoch 011:   1093 / 1283 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.8, ups=0.81, wpb=50.3, bsz=16, num_updates=13880, lr=5.12821e-06, gnorm=2.554, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22959
2022-07-12 19:19:51 - progress_bar.py[line:274] - INFO: epoch 011:   1103 / 1283 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.8, ups=0.8, wpb=50.7, bsz=16, num_updates=13890, lr=5.09438e-06, gnorm=4.315, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22972
2022-07-12 19:20:04 - progress_bar.py[line:274] - INFO: epoch 011:   1113 / 1283 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.3, ups=0.81, wpb=51.2, bsz=16, num_updates=13900, lr=5.06055e-06, gnorm=3.588, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22984
2022-07-12 19:20:16 - progress_bar.py[line:274] - INFO: epoch 011:   1123 / 1283 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.3, ups=0.81, wpb=53.5, bsz=16, num_updates=13910, lr=5.02672e-06, gnorm=4.333, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=22996
2022-07-12 19:20:28 - progress_bar.py[line:274] - INFO: epoch 011:   1133 / 1283 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=40.9, ups=0.81, wpb=50.7, bsz=16, num_updates=13920, lr=4.9929e-06, gnorm=5.484, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23009
2022-07-12 19:20:41 - progress_bar.py[line:274] - INFO: epoch 011:   1143 / 1283 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=52.6, nsentences=15.9, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.7, ups=0.81, wpb=52.6, bsz=15.9, num_updates=13930, lr=4.95907e-06, gnorm=1.819, clip=70, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=23021
2022-07-12 19:20:53 - progress_bar.py[line:274] - INFO: epoch 011:   1153 / 1283 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41, ups=0.8, wpb=51.1, bsz=16, num_updates=13940, lr=4.92524e-06, gnorm=2.584, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23034
2022-07-12 19:21:06 - progress_bar.py[line:274] - INFO: epoch 011:   1163 / 1283 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.4, ups=0.81, wpb=52.3, bsz=16, num_updates=13950, lr=4.89141e-06, gnorm=5.057, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=23046
2022-07-12 19:21:08 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-07-12 19:21:19 - progress_bar.py[line:274] - INFO: epoch 011:   1174 / 1283 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=38.8, ups=0.74, wpb=52.2, bsz=16, num_updates=13960, lr=4.85759e-06, gnorm=1.277, clip=60, loss_scale=32, train_wall=13, gb_free=2.2, ema_decay=0.9999, wall=23059
2022-07-12 19:21:31 - progress_bar.py[line:274] - INFO: epoch 011:   1184 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.3, ups=0.81, wpb=53.6, bsz=16, num_updates=13970, lr=4.82376e-06, gnorm=3.047, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23072
2022-07-12 19:21:44 - progress_bar.py[line:274] - INFO: epoch 011:   1194 / 1283 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=50.7, nsentences=16, sample_size=50.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.9, ups=0.81, wpb=50.7, bsz=16, num_updates=13980, lr=4.78993e-06, gnorm=2.89, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23084
2022-07-12 19:21:56 - progress_bar.py[line:274] - INFO: epoch 011:   1204 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.8, ups=0.81, wpb=51.8, bsz=16, num_updates=13990, lr=4.75611e-06, gnorm=2.899, clip=80, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23096
2022-07-12 19:22:09 - progress_bar.py[line:274] - INFO: epoch 011:   1214 / 1283 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40, ups=0.81, wpb=49.6, bsz=16, num_updates=14000, lr=4.72228e-06, gnorm=3.706, clip=70, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23109
2022-07-12 19:22:21 - progress_bar.py[line:274] - INFO: epoch 011:   1224 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.8, ups=0.81, wpb=51.5, bsz=16, num_updates=14010, lr=4.68845e-06, gnorm=4.29, clip=100, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=23121
2022-07-12 19:22:33 - progress_bar.py[line:274] - INFO: epoch 011:   1234 / 1283 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.7, ups=0.82, wpb=51.1, bsz=16, num_updates=14020, lr=4.65462e-06, gnorm=1.408, clip=70, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23133
2022-07-12 19:22:46 - progress_bar.py[line:274] - INFO: epoch 011:   1244 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.8, ups=0.81, wpb=50.2, bsz=16, num_updates=14030, lr=4.6208e-06, gnorm=2.936, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23146
2022-07-12 19:22:58 - progress_bar.py[line:274] - INFO: epoch 011:   1254 / 1283 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=53.4, nsentences=16, sample_size=53.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.1, ups=0.81, wpb=53.4, bsz=16, num_updates=14040, lr=4.58697e-06, gnorm=2.394, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23158
2022-07-12 19:23:10 - progress_bar.py[line:274] - INFO: epoch 011:   1264 / 1283 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.7, ups=0.8, wpb=52, bsz=16, num_updates=14050, lr=4.55314e-06, gnorm=2.244, clip=80, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=23171
2022-07-12 19:23:23 - progress_bar.py[line:274] - INFO: epoch 011:   1274 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.9, ups=0.8, wpb=49.8, bsz=16, num_updates=14060, lr=4.51932e-06, gnorm=2.881, clip=90, loss_scale=32, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=23183
2022-07-12 19:23:34 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 19:31:39 - progress_bar.py[line:282] - INFO: epoch 011 | valid on 'valid' subset | loss 1.394 | loss_v1 0 | loss_v2 0 | nll_loss 1.142 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.21 | vqa_score 0.3584 | wps 30.3 | wpb 13.6 | bsz 4 | num_updates 14069 | best_vqa_score 0.3584
2022-07-12 19:31:39 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 11 @ 14069 updates
2022-07-12 19:31:39 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints11.pt
2022-07-12 19:32:13 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints11.pt
2022-07-12 19:33:21 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints11.pt (epoch 11 @ 14069 updates, score 0.3584) (writing took 101.89147568203043 seconds)
2022-07-12 19:33:21 - train.py[line:323] - INFO: end of epoch 11 (average epoch stats below)
2022-07-12 19:33:21 - progress_bar.py[line:282] - INFO: epoch 011 | loss 0.531 | loss_v1 0 | loss_v2 0 | nll_loss 0.108 | ntokens 51.937 | nsentences 15.996 | sample_size 51.937 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.08 | wps 30.5 | ups 0.59 | wpb 51.9 | bsz 16 | num_updates 14069 | lr 4.48887e-06 | gnorm 3.133 | clip 90.9 | loss_scale 32 | train_wall 1584 | gb_free 2.3 | ema_decay 0.9999 | wall 23781
2022-07-12 19:33:21 - trainer.py[line:639] - INFO: loading train data for epoch 12
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 19:33:23 - trainer.py[line:703] - INFO: begin training epoch 12
2022-07-12 19:33:23 - train.py[line:296] - INFO: Start iterating over samples
2022-07-12 19:33:25 - progress_bar.py[line:274] - INFO: epoch 012:      1 / 1283 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=47.7, nsentences=15.6, sample_size=47.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=0.8, ups=0.02, wpb=47.7, bsz=15.6, num_updates=14070, lr=4.48549e-06, gnorm=2.653, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23785
2022-07-12 19:33:37 - progress_bar.py[line:274] - INFO: epoch 012:     11 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42, ups=0.82, wpb=51.4, bsz=16, num_updates=14080, lr=4.45166e-06, gnorm=3.574, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23797
2022-07-12 19:33:49 - progress_bar.py[line:274] - INFO: epoch 012:     21 / 1283 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.8, ups=0.81, wpb=51.5, bsz=16, num_updates=14090, lr=4.41783e-06, gnorm=2.976, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23810
2022-07-12 19:34:02 - progress_bar.py[line:274] - INFO: epoch 012:     31 / 1283 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.3, ups=0.82, wpb=51.6, bsz=16, num_updates=14100, lr=4.38401e-06, gnorm=1.943, clip=100, loss_scale=32, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=23822
2022-07-12 19:34:14 - progress_bar.py[line:274] - INFO: epoch 012:     41 / 1283 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=54.6, nsentences=16, sample_size=54.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44.8, ups=0.82, wpb=54.6, bsz=16, num_updates=14110, lr=4.35018e-06, gnorm=2.269, clip=70, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23834
2022-07-12 19:34:26 - progress_bar.py[line:274] - INFO: epoch 012:     51 / 1283 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.1, ups=0.82, wpb=52.6, bsz=16, num_updates=14120, lr=4.31635e-06, gnorm=2.226, clip=70, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=23846
2022-07-12 19:34:38 - progress_bar.py[line:274] - INFO: epoch 012:     61 / 1283 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=39.7, ups=0.8, wpb=49.8, bsz=16, num_updates=14130, lr=4.28252e-06, gnorm=2.341, clip=70, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23859
2022-07-12 19:34:51 - progress_bar.py[line:274] - INFO: epoch 012:     71 / 1283 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43, ups=0.82, wpb=52.7, bsz=16, num_updates=14140, lr=4.2487e-06, gnorm=3.245, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23871
2022-07-12 19:35:03 - progress_bar.py[line:274] - INFO: epoch 012:     81 / 1283 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=14150, lr=4.21487e-06, gnorm=2.074, clip=40, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23883
2022-07-12 19:35:15 - progress_bar.py[line:274] - INFO: epoch 012:     91 / 1283 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.8, ups=0.81, wpb=52.8, bsz=16, num_updates=14160, lr=4.18104e-06, gnorm=6.62, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23896
2022-07-12 19:35:28 - progress_bar.py[line:274] - INFO: epoch 012:    101 / 1283 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.8, ups=0.81, wpb=51.4, bsz=16, num_updates=14170, lr=4.14722e-06, gnorm=1.706, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23908
2022-07-12 19:35:40 - progress_bar.py[line:274] - INFO: epoch 012:    111 / 1283 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=49.5, nsentences=16, sample_size=49.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39.8, ups=0.8, wpb=49.5, bsz=16, num_updates=14180, lr=4.11339e-06, gnorm=2.432, clip=100, loss_scale=32, train_wall=12, gb_free=2, ema_decay=0.9999, wall=23920
2022-07-12 19:35:53 - progress_bar.py[line:274] - INFO: epoch 012:    121 / 1283 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.7, ups=0.81, wpb=53, bsz=16, num_updates=14190, lr=4.07956e-06, gnorm=1.837, clip=60, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23933
2022-07-12 19:36:05 - progress_bar.py[line:274] - INFO: epoch 012:    131 / 1283 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=49.3, nsentences=16, sample_size=49.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.1, ups=0.81, wpb=49.3, bsz=16, num_updates=14200, lr=4.04573e-06, gnorm=2.392, clip=100, loss_scale=32, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=23945
2022-07-12 19:36:17 - progress_bar.py[line:274] - INFO: epoch 012:    141 / 1283 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44.9, ups=0.82, wpb=55.1, bsz=16, num_updates=14210, lr=4.01191e-06, gnorm=2.322, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23957
2022-07-12 19:36:29 - progress_bar.py[line:274] - INFO: epoch 012:    151 / 1283 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.5, ups=0.81, wpb=53.5, bsz=16, num_updates=14220, lr=3.97808e-06, gnorm=3.5, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23970
2022-07-12 19:36:42 - progress_bar.py[line:274] - INFO: epoch 012:    161 / 1283 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.2, ups=0.81, wpb=53.5, bsz=16, num_updates=14230, lr=3.94425e-06, gnorm=3.245, clip=80, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=23982
2022-07-12 19:36:54 - progress_bar.py[line:274] - INFO: epoch 012:    171 / 1283 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.9, ups=0.81, wpb=51.6, bsz=16, num_updates=14240, lr=3.91043e-06, gnorm=2.146, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=23994
2022-07-12 19:37:07 - progress_bar.py[line:274] - INFO: epoch 012:    181 / 1283 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.5, ups=0.81, wpb=50.3, bsz=16, num_updates=14250, lr=3.8766e-06, gnorm=1.975, clip=80, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24007
2022-07-12 19:37:19 - progress_bar.py[line:274] - INFO: epoch 012:    191 / 1283 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.9, ups=0.81, wpb=51.8, bsz=16, num_updates=14260, lr=3.84277e-06, gnorm=1.865, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24019
2022-07-12 19:37:31 - progress_bar.py[line:274] - INFO: epoch 012:    201 / 1283 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.8, ups=0.8, wpb=51.9, bsz=16, num_updates=14270, lr=3.80894e-06, gnorm=2.738, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24032
2022-07-12 19:37:44 - progress_bar.py[line:274] - INFO: epoch 012:    211 / 1283 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.6, ups=0.81, wpb=51.1, bsz=16, num_updates=14280, lr=3.77512e-06, gnorm=2.322, clip=70, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24044
2022-07-12 19:37:56 - progress_bar.py[line:274] - INFO: epoch 012:    221 / 1283 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.8, ups=0.81, wpb=51.7, bsz=16, num_updates=14290, lr=3.74129e-06, gnorm=2.495, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24056
2022-07-12 19:38:08 - progress_bar.py[line:274] - INFO: epoch 012:    231 / 1283 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43, ups=0.81, wpb=53.3, bsz=16, num_updates=14300, lr=3.70746e-06, gnorm=2.231, clip=70, loss_scale=32, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=24069
2022-07-12 19:38:21 - progress_bar.py[line:274] - INFO: epoch 012:    241 / 1283 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.8, ups=0.8, wpb=51.2, bsz=16, num_updates=14310, lr=3.67364e-06, gnorm=2.753, clip=90, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=24081
2022-07-12 19:38:33 - progress_bar.py[line:274] - INFO: epoch 012:    251 / 1283 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.5, ups=0.81, wpb=52.4, bsz=16, num_updates=14320, lr=3.63981e-06, gnorm=1.574, clip=80, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24094
2022-07-12 19:38:46 - progress_bar.py[line:274] - INFO: epoch 012:    261 / 1283 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.2, ups=0.82, wpb=52.8, bsz=16, num_updates=14330, lr=3.60598e-06, gnorm=1.794, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24106
2022-07-12 19:38:58 - progress_bar.py[line:274] - INFO: epoch 012:    271 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=48.2, nsentences=16, sample_size=48.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=39, ups=0.81, wpb=48.2, bsz=16, num_updates=14340, lr=3.57215e-06, gnorm=2.293, clip=80, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24118
2022-07-12 19:39:11 - progress_bar.py[line:274] - INFO: epoch 012:    281 / 1283 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.4, ups=0.79, wpb=53.6, bsz=16, num_updates=14350, lr=3.53833e-06, gnorm=2.057, clip=90, loss_scale=32, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=24131
2022-07-12 19:39:23 - progress_bar.py[line:274] - INFO: epoch 012:    291 / 1283 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.8, ups=0.8, wpb=52, bsz=16, num_updates=14360, lr=3.5045e-06, gnorm=2.595, clip=70, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24143
2022-07-12 19:39:35 - progress_bar.py[line:274] - INFO: epoch 012:    301 / 1283 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.1, ups=0.81, wpb=52.1, bsz=16, num_updates=14370, lr=3.47067e-06, gnorm=3.198, clip=90, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24156
2022-07-12 19:39:48 - progress_bar.py[line:274] - INFO: epoch 012:    311 / 1283 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.4, ups=0.8, wpb=52.7, bsz=16, num_updates=14380, lr=3.43684e-06, gnorm=2.605, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24168
2022-07-12 19:40:00 - progress_bar.py[line:274] - INFO: epoch 012:    321 / 1283 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=43, ups=0.81, wpb=53.3, bsz=16, num_updates=14390, lr=3.40302e-06, gnorm=1.693, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24180
2022-07-12 19:40:13 - progress_bar.py[line:274] - INFO: epoch 012:    331 / 1283 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42, ups=0.8, wpb=52.5, bsz=16, num_updates=14400, lr=3.36919e-06, gnorm=2.886, clip=80, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24193
2022-07-12 19:40:25 - progress_bar.py[line:274] - INFO: epoch 012:    341 / 1283 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.3, ups=0.8, wpb=51.8, bsz=16, num_updates=14410, lr=3.33536e-06, gnorm=2.732, clip=100, loss_scale=32, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24205
2022-07-12 19:40:38 - progress_bar.py[line:274] - INFO: epoch 012:    351 / 1283 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=48.2, nsentences=16, sample_size=48.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=39.2, ups=0.81, wpb=48.2, bsz=16, num_updates=14420, lr=3.30154e-06, gnorm=1.925, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24218
2022-07-12 19:40:50 - progress_bar.py[line:274] - INFO: epoch 012:    361 / 1283 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=53, nsentences=16, sample_size=53, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=43.1, ups=0.81, wpb=53, bsz=16, num_updates=14430, lr=3.26771e-06, gnorm=2.03, clip=90, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24230
2022-07-12 19:41:02 - progress_bar.py[line:274] - INFO: epoch 012:    371 / 1283 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=49.4, nsentences=16, sample_size=49.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40, ups=0.81, wpb=49.4, bsz=16, num_updates=14440, lr=3.23388e-06, gnorm=4.052, clip=100, loss_scale=32, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24242
2022-07-12 19:41:15 - progress_bar.py[line:274] - INFO: epoch 012:    381 / 1283 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=51.8, nsentences=16, sample_size=51.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.6, ups=0.8, wpb=51.8, bsz=16, num_updates=14450, lr=3.20005e-06, gnorm=1.822, clip=80, loss_scale=32, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=24255
2022-07-12 19:41:27 - progress_bar.py[line:274] - INFO: epoch 012:    391 / 1283 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=55.1, nsentences=16, sample_size=55.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=44.5, ups=0.81, wpb=55.1, bsz=16, num_updates=14460, lr=3.16623e-06, gnorm=2.444, clip=80, loss_scale=32, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=24267
2022-07-12 19:41:39 - progress_bar.py[line:274] - INFO: epoch 012:    401 / 1283 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=53.7, nsentences=16, sample_size=53.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.7, ups=0.81, wpb=53.7, bsz=16, num_updates=14470, lr=3.1324e-06, gnorm=3.33, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24280
2022-07-12 19:41:52 - progress_bar.py[line:274] - INFO: epoch 012:    411 / 1283 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.4, ups=0.8, wpb=51.5, bsz=16, num_updates=14480, lr=3.09857e-06, gnorm=1.714, clip=70, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24292
2022-07-12 19:42:04 - progress_bar.py[line:274] - INFO: epoch 012:    421 / 1283 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.1, ups=0.8, wpb=50.2, bsz=16, num_updates=14490, lr=3.06475e-06, gnorm=2.134, clip=70, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24304
2022-07-12 19:42:17 - progress_bar.py[line:274] - INFO: epoch 012:    431 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.8, ups=0.81, wpb=51.6, bsz=16, num_updates=14500, lr=3.03092e-06, gnorm=4.483, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24317
2022-07-12 19:42:29 - progress_bar.py[line:274] - INFO: epoch 012:    441 / 1283 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.5, ups=0.81, wpb=51.5, bsz=16, num_updates=14510, lr=2.99709e-06, gnorm=3.049, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24329
2022-07-12 19:42:41 - progress_bar.py[line:274] - INFO: epoch 012:    451 / 1283 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=50.3, nsentences=16, sample_size=50.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.8, ups=0.81, wpb=50.3, bsz=16, num_updates=14520, lr=2.96326e-06, gnorm=4.506, clip=80, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=24342
2022-07-12 19:42:54 - progress_bar.py[line:274] - INFO: epoch 012:    461 / 1283 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42, ups=0.8, wpb=52.5, bsz=16, num_updates=14530, lr=2.92944e-06, gnorm=2.768, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24354
2022-07-12 19:43:06 - progress_bar.py[line:274] - INFO: epoch 012:    471 / 1283 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=42.1, ups=0.81, wpb=51.9, bsz=16, num_updates=14540, lr=2.89561e-06, gnorm=1.496, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24366
2022-07-12 19:43:18 - progress_bar.py[line:274] - INFO: epoch 012:    481 / 1283 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=53.8, nsentences=16, sample_size=53.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43.9, ups=0.82, wpb=53.8, bsz=16, num_updates=14550, lr=2.86178e-06, gnorm=3.268, clip=90, loss_scale=64, train_wall=12, gb_free=2.1, ema_decay=0.9999, wall=24379
2022-07-12 19:43:31 - progress_bar.py[line:274] - INFO: epoch 012:    491 / 1283 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.1, ups=0.8, wpb=52.8, bsz=16, num_updates=14560, lr=2.82795e-06, gnorm=2.969, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24391
2022-07-12 19:43:43 - progress_bar.py[line:274] - INFO: epoch 012:    501 / 1283 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=52.5, nsentences=16, sample_size=52.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.7, ups=0.81, wpb=52.5, bsz=16, num_updates=14570, lr=2.79413e-06, gnorm=3.225, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24403
2022-07-12 19:43:55 - progress_bar.py[line:274] - INFO: epoch 012:    511 / 1283 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=55.3, nsentences=16, sample_size=55.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=45.2, ups=0.82, wpb=55.3, bsz=16, num_updates=14580, lr=2.7603e-06, gnorm=3.071, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=24416
2022-07-12 19:44:08 - progress_bar.py[line:274] - INFO: epoch 012:    521 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=49.8, nsentences=16, sample_size=49.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40, ups=0.8, wpb=49.8, bsz=16, num_updates=14590, lr=2.72647e-06, gnorm=2.637, clip=70, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24428
2022-07-12 19:44:20 - progress_bar.py[line:274] - INFO: epoch 012:    531 / 1283 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=54.4, nsentences=16, sample_size=54.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=44, ups=0.81, wpb=54.4, bsz=16, num_updates=14600, lr=2.69265e-06, gnorm=3.677, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24441
2022-07-12 19:44:33 - progress_bar.py[line:274] - INFO: epoch 012:    541 / 1283 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.9, ups=0.81, wpb=53.3, bsz=16, num_updates=14610, lr=2.65882e-06, gnorm=3.536, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24453
2022-07-12 19:44:45 - progress_bar.py[line:274] - INFO: epoch 012:    551 / 1283 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.2, ups=0.8, wpb=53.9, bsz=16, num_updates=14620, lr=2.62499e-06, gnorm=3.079, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24465
2022-07-12 19:44:58 - progress_bar.py[line:274] - INFO: epoch 012:    561 / 1283 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42, ups=0.8, wpb=52.8, bsz=16, num_updates=14630, lr=2.59116e-06, gnorm=3.21, clip=70, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=24478
2022-07-12 19:45:10 - progress_bar.py[line:274] - INFO: epoch 012:    571 / 1283 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.3, ups=0.8, wpb=50.1, bsz=16, num_updates=14640, lr=2.55734e-06, gnorm=2.161, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=24490
2022-07-12 19:45:23 - progress_bar.py[line:274] - INFO: epoch 012:    581 / 1283 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.6, ups=0.81, wpb=52.6, bsz=16, num_updates=14650, lr=2.52351e-06, gnorm=1.541, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24503
2022-07-12 19:45:35 - progress_bar.py[line:274] - INFO: epoch 012:    591 / 1283 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=53.5, nsentences=16, sample_size=53.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.7, ups=0.8, wpb=53.5, bsz=16, num_updates=14660, lr=2.48968e-06, gnorm=2.497, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24515
2022-07-12 19:45:47 - progress_bar.py[line:274] - INFO: epoch 012:    601 / 1283 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.9, ups=0.81, wpb=50.5, bsz=16, num_updates=14670, lr=2.45586e-06, gnorm=2.137, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24528
2022-07-12 19:46:00 - progress_bar.py[line:274] - INFO: epoch 012:    611 / 1283 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.5, ups=0.81, wpb=52.6, bsz=16, num_updates=14680, lr=2.42203e-06, gnorm=3.459, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=24540
2022-07-12 19:46:12 - progress_bar.py[line:274] - INFO: epoch 012:    621 / 1283 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.9, ups=0.79, wpb=52.8, bsz=16, num_updates=14690, lr=2.3882e-06, gnorm=2.339, clip=90, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=24553
2022-07-12 19:46:25 - progress_bar.py[line:274] - INFO: epoch 012:    631 / 1283 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.9, ups=0.81, wpb=51.5, bsz=16, num_updates=14700, lr=2.35437e-06, gnorm=3.26, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24565
2022-07-12 19:46:37 - progress_bar.py[line:274] - INFO: epoch 012:    641 / 1283 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=54, nsentences=16, sample_size=54, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.7, ups=0.81, wpb=54, bsz=16, num_updates=14710, lr=2.32055e-06, gnorm=2.379, clip=80, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=24577
2022-07-12 19:46:49 - progress_bar.py[line:274] - INFO: epoch 012:    651 / 1283 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=43.7, ups=0.81, wpb=53.6, bsz=16, num_updates=14720, lr=2.28672e-06, gnorm=2.183, clip=70, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24590
2022-07-12 19:47:02 - progress_bar.py[line:274] - INFO: epoch 012:    661 / 1283 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=40.9, ups=0.81, wpb=50.6, bsz=16, num_updates=14730, lr=2.25289e-06, gnorm=3.236, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24602
2022-07-12 19:47:14 - progress_bar.py[line:274] - INFO: epoch 012:    671 / 1283 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=53.3, nsentences=16, sample_size=53.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=43, ups=0.81, wpb=53.3, bsz=16, num_updates=14740, lr=2.21907e-06, gnorm=2.733, clip=80, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=24614
2022-07-12 19:47:26 - progress_bar.py[line:274] - INFO: epoch 012:    681 / 1283 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=43.3, ups=0.81, wpb=53.6, bsz=16, num_updates=14750, lr=2.18524e-06, gnorm=1.839, clip=70, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24627
2022-07-12 19:47:39 - progress_bar.py[line:274] - INFO: epoch 012:    691 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=41.1, ups=0.8, wpb=51.1, bsz=16, num_updates=14760, lr=2.15141e-06, gnorm=4.305, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24639
2022-07-12 19:47:51 - progress_bar.py[line:274] - INFO: epoch 012:    701 / 1283 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=49.6, nsentences=16, sample_size=49.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.1, ups=0.81, wpb=49.6, bsz=16, num_updates=14770, lr=2.11758e-06, gnorm=1.951, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24651
2022-07-12 19:48:04 - progress_bar.py[line:274] - INFO: epoch 012:    711 / 1283 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=51.4, nsentences=16, sample_size=51.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.8, ups=0.81, wpb=51.4, bsz=16, num_updates=14780, lr=2.08376e-06, gnorm=2.352, clip=60, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24664
2022-07-12 19:48:16 - progress_bar.py[line:274] - INFO: epoch 012:    721 / 1283 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=43.9, ups=0.81, wpb=54.1, bsz=16, num_updates=14790, lr=2.04993e-06, gnorm=2.348, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24676
2022-07-12 19:48:28 - progress_bar.py[line:274] - INFO: epoch 012:    731 / 1283 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=52.8, nsentences=16, sample_size=52.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.9, ups=0.81, wpb=52.8, bsz=16, num_updates=14800, lr=2.0161e-06, gnorm=3.322, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24688
2022-07-12 19:48:40 - progress_bar.py[line:274] - INFO: epoch 012:    741 / 1283 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=51.5, nsentences=16, sample_size=51.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42, ups=0.82, wpb=51.5, bsz=16, num_updates=14810, lr=1.98227e-06, gnorm=3.248, clip=90, loss_scale=64, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=24701
2022-07-12 19:48:53 - progress_bar.py[line:274] - INFO: epoch 012:    751 / 1283 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=43.8, ups=0.81, wpb=54.2, bsz=16, num_updates=14820, lr=1.94845e-06, gnorm=1.841, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24713
2022-07-12 19:49:05 - progress_bar.py[line:274] - INFO: epoch 012:    761 / 1283 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=51.9, nsentences=16, sample_size=51.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.2, ups=0.81, wpb=51.9, bsz=16, num_updates=14830, lr=1.91462e-06, gnorm=2.983, clip=100, loss_scale=64, train_wall=12, gb_free=2, ema_decay=0.9999, wall=24725
2022-07-12 19:49:17 - progress_bar.py[line:274] - INFO: epoch 012:    771 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=51.2, nsentences=16, sample_size=51.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.5, ups=0.81, wpb=51.2, bsz=16, num_updates=14840, lr=1.88079e-06, gnorm=3.387, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24738
2022-07-12 19:49:30 - progress_bar.py[line:274] - INFO: epoch 012:    781 / 1283 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=52.7, nsentences=16, sample_size=52.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.2, ups=0.8, wpb=52.7, bsz=16, num_updates=14850, lr=1.84697e-06, gnorm=3.464, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=24750
2022-07-12 19:49:42 - progress_bar.py[line:274] - INFO: epoch 012:    791 / 1283 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=54.7, nsentences=16, sample_size=54.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=44.4, ups=0.81, wpb=54.7, bsz=16, num_updates=14860, lr=1.81314e-06, gnorm=2.697, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24762
2022-07-12 19:49:55 - progress_bar.py[line:274] - INFO: epoch 012:    801 / 1283 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.1, ups=0.81, wpb=50.6, bsz=16, num_updates=14870, lr=1.77931e-06, gnorm=3.128, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24775
2022-07-12 19:50:07 - progress_bar.py[line:274] - INFO: epoch 012:    811 / 1283 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=50.5, nsentences=16, sample_size=50.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.6, ups=0.8, wpb=50.5, bsz=16, num_updates=14880, lr=1.74548e-06, gnorm=3.765, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=24787
2022-07-12 19:50:20 - progress_bar.py[line:274] - INFO: epoch 012:    821 / 1283 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=53.9, nsentences=16, sample_size=53.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=42.9, ups=0.8, wpb=53.9, bsz=16, num_updates=14890, lr=1.71166e-06, gnorm=2.863, clip=100, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=24800
2022-07-12 19:50:32 - progress_bar.py[line:274] - INFO: epoch 012:    831 / 1283 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.6, ups=0.81, wpb=50.4, bsz=16, num_updates=14900, lr=1.67783e-06, gnorm=3.924, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24812
2022-07-12 19:50:45 - progress_bar.py[line:274] - INFO: epoch 012:    841 / 1283 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41, ups=0.8, wpb=51.3, bsz=16, num_updates=14910, lr=1.644e-06, gnorm=2.955, clip=90, loss_scale=64, train_wall=12, gb_free=1.9, ema_decay=0.9999, wall=24825
2022-07-12 19:50:57 - progress_bar.py[line:274] - INFO: epoch 012:    851 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.4, ups=0.81, wpb=52.2, bsz=16, num_updates=14920, lr=1.61018e-06, gnorm=2.271, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24837
2022-07-12 19:51:09 - progress_bar.py[line:274] - INFO: epoch 012:    861 / 1283 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=50, nsentences=16, sample_size=50, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.6, ups=0.81, wpb=50, bsz=16, num_updates=14930, lr=1.57635e-06, gnorm=4.392, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=24849
2022-07-12 19:51:22 - progress_bar.py[line:274] - INFO: epoch 012:    871 / 1283 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.7, ups=0.81, wpb=51.6, bsz=16, num_updates=14940, lr=1.54252e-06, gnorm=3.336, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24862
2022-07-12 19:51:34 - progress_bar.py[line:274] - INFO: epoch 012:    881 / 1283 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=51.7, nsentences=16, sample_size=51.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.8, ups=0.81, wpb=51.7, bsz=16, num_updates=14950, lr=1.50869e-06, gnorm=2.985, clip=90, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24874
2022-07-12 19:51:46 - progress_bar.py[line:274] - INFO: epoch 012:    891 / 1283 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.9, ups=0.8, wpb=52.4, bsz=16, num_updates=14960, lr=1.47487e-06, gnorm=3.628, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24887
2022-07-12 19:51:59 - progress_bar.py[line:274] - INFO: epoch 012:    901 / 1283 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=56.9, nsentences=16, sample_size=56.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=46.1, ups=0.81, wpb=56.9, bsz=16, num_updates=14970, lr=1.44104e-06, gnorm=3.726, clip=80, loss_scale=64, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24899
2022-07-12 19:52:11 - progress_bar.py[line:274] - INFO: epoch 012:    911 / 1283 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=49.7, nsentences=16, sample_size=49.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=39.4, ups=0.79, wpb=49.7, bsz=16, num_updates=14980, lr=1.40721e-06, gnorm=4.357, clip=100, loss_scale=128, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=24912
2022-07-12 19:52:24 - progress_bar.py[line:274] - INFO: epoch 012:    921 / 1283 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=50.2, nsentences=16, sample_size=50.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.7, ups=0.81, wpb=50.2, bsz=16, num_updates=14990, lr=1.37338e-06, gnorm=3.026, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24924
2022-07-12 19:52:36 - progress_bar.py[line:274] - INFO: epoch 012:    931 / 1283 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.3, ups=0.8, wpb=51.6, bsz=16, num_updates=15000, lr=1.33956e-06, gnorm=2.464, clip=100, loss_scale=128, train_wall=12, gb_free=2, ema_decay=0.9999, wall=24936
2022-07-12 19:52:48 - progress_bar.py[line:274] - INFO: epoch 012:    941 / 1283 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.4, ups=0.81, wpb=52, bsz=16, num_updates=15010, lr=1.30573e-06, gnorm=2.365, clip=80, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24949
2022-07-12 19:53:01 - progress_bar.py[line:274] - INFO: epoch 012:    951 / 1283 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=53.6, nsentences=16, sample_size=53.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.5, ups=0.79, wpb=53.6, bsz=16, num_updates=15020, lr=1.2719e-06, gnorm=1.689, clip=80, loss_scale=128, train_wall=13, gb_free=2.1, ema_decay=0.9999, wall=24961
2022-07-12 19:53:14 - progress_bar.py[line:274] - INFO: epoch 012:    961 / 1283 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.4, ups=0.8, wpb=53.1, bsz=16, num_updates=15030, lr=1.23808e-06, gnorm=2.584, clip=90, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24974
2022-07-12 19:53:26 - progress_bar.py[line:274] - INFO: epoch 012:    971 / 1283 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=40.7, ups=0.81, wpb=50.4, bsz=16, num_updates=15040, lr=1.20425e-06, gnorm=2.706, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=24986
2022-07-12 19:53:38 - progress_bar.py[line:274] - INFO: epoch 012:    981 / 1283 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.2, ups=0.81, wpb=49.9, bsz=16, num_updates=15050, lr=1.17042e-06, gnorm=2.845, clip=70, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=24999
2022-07-12 19:53:51 - progress_bar.py[line:274] - INFO: epoch 012:    991 / 1283 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=40.9, ups=0.8, wpb=51, bsz=16, num_updates=15060, lr=1.13659e-06, gnorm=3.516, clip=80, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=25011
2022-07-12 19:54:03 - progress_bar.py[line:274] - INFO: epoch 012:   1001 / 1283 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=52.9, nsentences=16, sample_size=52.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.8, ups=0.81, wpb=52.9, bsz=16, num_updates=15070, lr=1.10277e-06, gnorm=2.186, clip=80, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25023
2022-07-12 19:54:16 - progress_bar.py[line:274] - INFO: epoch 012:   1011 / 1283 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=48.6, nsentences=16, sample_size=48.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=39, ups=0.8, wpb=48.6, bsz=16, num_updates=15080, lr=1.06894e-06, gnorm=3.245, clip=80, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=25036
2022-07-12 19:54:28 - progress_bar.py[line:274] - INFO: epoch 012:   1021 / 1283 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=50.4, nsentences=16, sample_size=50.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=41.1, ups=0.82, wpb=50.4, bsz=16, num_updates=15090, lr=1.03511e-06, gnorm=4.715, clip=100, loss_scale=128, train_wall=12, gb_free=2.3, ema_decay=0.9999, wall=25048
2022-07-12 19:54:40 - progress_bar.py[line:274] - INFO: epoch 012:   1031 / 1283 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.8, ups=0.8, wpb=52.3, bsz=16, num_updates=15100, lr=1.00129e-06, gnorm=3.582, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25061
2022-07-12 19:54:53 - progress_bar.py[line:274] - INFO: epoch 012:   1041 / 1283 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=52.6, nsentences=16, sample_size=52.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42, ups=0.8, wpb=52.6, bsz=16, num_updates=15110, lr=9.67458e-07, gnorm=3.018, clip=80, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25073
2022-07-12 19:55:05 - progress_bar.py[line:274] - INFO: epoch 012:   1051 / 1283 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=52, nsentences=16, sample_size=52, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.6, ups=0.8, wpb=52, bsz=16, num_updates=15120, lr=9.33631e-07, gnorm=2.864, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25086
2022-07-12 19:55:18 - progress_bar.py[line:274] - INFO: epoch 012:   1061 / 1283 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=54.3, nsentences=16, sample_size=54.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=43.8, ups=0.81, wpb=54.3, bsz=16, num_updates=15130, lr=8.99804e-07, gnorm=2.835, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25098
2022-07-12 19:55:30 - progress_bar.py[line:274] - INFO: epoch 012:   1071 / 1283 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=50.1, nsentences=16, sample_size=50.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=40.7, ups=0.81, wpb=50.1, bsz=16, num_updates=15140, lr=8.65977e-07, gnorm=4.096, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25110
2022-07-12 19:55:43 - progress_bar.py[line:274] - INFO: epoch 012:   1081 / 1283 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=54.1, nsentences=16, sample_size=54.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=43.1, ups=0.8, wpb=54.1, bsz=16, num_updates=15150, lr=8.32149e-07, gnorm=4.518, clip=80, loss_scale=128, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=25123
2022-07-12 19:55:55 - progress_bar.py[line:274] - INFO: epoch 012:   1091 / 1283 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=51.3, nsentences=15.9, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.6, ups=0.81, wpb=51.3, bsz=15.9, num_updates=15160, lr=7.98322e-07, gnorm=3.124, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25135
2022-07-12 19:56:08 - progress_bar.py[line:274] - INFO: epoch 012:   1101 / 1283 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=50.6, nsentences=16, sample_size=50.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.7, ups=0.8, wpb=50.6, bsz=16, num_updates=15170, lr=7.64495e-07, gnorm=1.977, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25148
2022-07-12 19:56:20 - progress_bar.py[line:274] - INFO: epoch 012:   1111 / 1283 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.2, ups=0.8, wpb=49.9, bsz=16, num_updates=15180, lr=7.30668e-07, gnorm=3.23, clip=90, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25160
2022-07-12 19:56:32 - progress_bar.py[line:274] - INFO: epoch 012:   1121 / 1283 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=52.4, nsentences=16, sample_size=52.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.8, ups=0.8, wpb=52.4, bsz=16, num_updates=15190, lr=6.96841e-07, gnorm=2.259, clip=70, loss_scale=128, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=25173
2022-07-12 19:56:45 - progress_bar.py[line:274] - INFO: epoch 012:   1131 / 1283 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=50.9, nsentences=16, sample_size=50.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.4, ups=0.81, wpb=50.9, bsz=16, num_updates=15200, lr=6.63013e-07, gnorm=3.313, clip=80, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25185
2022-07-12 19:56:57 - progress_bar.py[line:274] - INFO: epoch 012:   1141 / 1283 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=53.2, nsentences=16, sample_size=53.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=42.6, ups=0.8, wpb=53.2, bsz=16, num_updates=15210, lr=6.29186e-07, gnorm=1.583, clip=60, loss_scale=128, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25198
2022-07-12 19:57:10 - progress_bar.py[line:274] - INFO: epoch 012:   1151 / 1283 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=51.1, nsentences=16, sample_size=51.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=40.8, ups=0.8, wpb=51.1, bsz=16, num_updates=15220, lr=5.95359e-07, gnorm=2.075, clip=70, loss_scale=128, train_wall=12, gb_free=1.8, ema_decay=0.9999, wall=25210
2022-07-12 19:57:22 - progress_bar.py[line:274] - INFO: epoch 012:   1161 / 1283 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=53.1, nsentences=16, sample_size=53.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42.7, ups=0.8, wpb=53.1, bsz=16, num_updates=15230, lr=5.61532e-07, gnorm=3.407, clip=100, loss_scale=128, train_wall=12, gb_free=2.5, ema_decay=0.9999, wall=25222
2022-07-12 19:57:27 - trainer.py[line:922] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-07-12 19:57:36 - progress_bar.py[line:274] - INFO: epoch 012:   1172 / 1283 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=38.6, ups=0.74, wpb=52.3, bsz=16, num_updates=15240, lr=5.27704e-07, gnorm=1.51, clip=60, loss_scale=64, train_wall=13, gb_free=2.3, ema_decay=0.9999, wall=25236
2022-07-12 19:57:48 - progress_bar.py[line:274] - INFO: epoch 012:   1182 / 1283 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=52.2, nsentences=16, sample_size=52.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=42, ups=0.8, wpb=52.2, bsz=16, num_updates=15250, lr=4.93877e-07, gnorm=4.234, clip=90, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=25248
2022-07-12 19:58:01 - progress_bar.py[line:274] - INFO: epoch 012:   1192 / 1283 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=52.1, nsentences=16, sample_size=52.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=41.9, ups=0.8, wpb=52.1, bsz=16, num_updates=15260, lr=4.6005e-07, gnorm=2.071, clip=70, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25261
2022-07-12 19:58:13 - progress_bar.py[line:274] - INFO: epoch 012:   1202 / 1283 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=51.6, nsentences=16, sample_size=51.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=41.5, ups=0.8, wpb=51.6, bsz=16, num_updates=15270, lr=4.26223e-07, gnorm=3.078, clip=70, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=25273
2022-07-12 19:58:25 - progress_bar.py[line:274] - INFO: epoch 012:   1212 / 1283 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=50.8, nsentences=16, sample_size=50.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=40.9, ups=0.8, wpb=50.8, bsz=16, num_updates=15280, lr=3.92396e-07, gnorm=2.368, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25286
2022-07-12 19:58:38 - progress_bar.py[line:274] - INFO: epoch 012:   1222 / 1283 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=51.3, nsentences=16, sample_size=51.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.5, ups=0.81, wpb=51.3, bsz=16, num_updates=15290, lr=3.58568e-07, gnorm=4.262, clip=100, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=25298
2022-07-12 19:58:50 - progress_bar.py[line:274] - INFO: epoch 012:   1232 / 1283 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=51, nsentences=16, sample_size=51, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=41.8, ups=0.82, wpb=51, bsz=16, num_updates=15300, lr=3.24741e-07, gnorm=1.895, clip=80, loss_scale=64, train_wall=12, gb_free=2.2, ema_decay=0.9999, wall=25310
2022-07-12 19:59:02 - progress_bar.py[line:274] - INFO: epoch 012:   1242 / 1283 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=49.1, nsentences=16, sample_size=49.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=39.5, ups=0.81, wpb=49.1, bsz=16, num_updates=15310, lr=2.90914e-07, gnorm=2.3, clip=90, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25323
2022-07-12 19:59:15 - progress_bar.py[line:274] - INFO: epoch 012:   1252 / 1283 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=52.3, nsentences=16, sample_size=52.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.5, ups=0.81, wpb=52.3, bsz=16, num_updates=15320, lr=2.57087e-07, gnorm=3.095, clip=100, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25335
2022-07-12 19:59:27 - progress_bar.py[line:274] - INFO: epoch 012:   1262 / 1283 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=54.2, nsentences=16, sample_size=54.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=42.9, ups=0.79, wpb=54.2, bsz=16, num_updates=15330, lr=2.2326e-07, gnorm=3.097, clip=90, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=25348
2022-07-12 19:59:40 - progress_bar.py[line:274] - INFO: epoch 012:   1272 / 1283 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=49.9, nsentences=16, sample_size=49.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=39.9, ups=0.8, wpb=49.9, bsz=16, num_updates=15340, lr=1.89432e-07, gnorm=2.257, clip=80, loss_scale=64, train_wall=12, gb_free=2.4, ema_decay=0.9999, wall=25360
2022-07-12 19:59:52 - progress_bar.py[line:274] - INFO: epoch 012:   1282 / 1283 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=48.6, nsentences=16, sample_size=48.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=38.7, ups=0.8, wpb=48.6, bsz=16, num_updates=15350, lr=1.55605e-07, gnorm=3.099, clip=70, loss_scale=64, train_wall=13, gb_free=2.4, ema_decay=0.9999, wall=25373
2022-07-12 19:59:53 - train.py[line:436] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
2022-07-12 20:08:01 - progress_bar.py[line:282] - INFO: epoch 012 | valid on 'valid' subset | loss 1.399 | loss_v1 0 | loss_v2 0 | nll_loss 1.147 | ntokens 13.583 | nsentences 3.999 | sample_size 13.583 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.21 | vqa_score 0.3607 | wps 30.1 | wpb 13.6 | bsz 4 | num_updates 15351 | best_vqa_score 0.3607
2022-07-12 20:08:01 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoints for epoch 12 @ 15351 updates
2022-07-12 20:08:01 - trainer.py[line:431] - INFO: Saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints12.pt
2022-07-12 20:08:40 - trainer.py[line:441] - INFO: Finished saving checkpoints to ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints12.pt
2022-07-12 20:09:57 - checkpoint_utils.py[line:135] - INFO: Saved checkpoints ./vqa_checkpoints/12_0.04_5e-5_480/checkpoints12.pt (epoch 12 @ 15351 updates, score 0.3607) (writing took 116.18431103602052 seconds)
2022-07-12 20:09:57 - train.py[line:323] - INFO: end of epoch 12 (average epoch stats below)
2022-07-12 20:09:57 - progress_bar.py[line:282] - INFO: epoch 012 | loss 0.524 | loss_v1 0 | loss_v2 0 | nll_loss 0.101 | ntokens 51.941 | nsentences 15.996 | sample_size 51.941 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.07 | wps 30.3 | ups 0.58 | wpb 51.9 | bsz 16 | num_updates 15351 | lr 1.52222e-07 | gnorm 2.815 | clip 84.7 | loss_scale 64 | train_wall 1585 | gb_free 2.3 | ema_decay 0.9999 | wall 25977
2022-07-12 20:09:57 - trainer.py[line:639] - INFO: loading train data for epoch 13
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/vizwiz_data/vizwiz_train.tsv slice_id 0 row count 20523 total row count 20523
slice_id 0 seek offset 0
2022-07-12 20:09:59 - train.py[line:205] - INFO: done training in 25975.3 seconds
2022-07-12 20:09:59 - distributed_c10d.py[line:217] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-07-12 20:09:59 - distributed_c10d.py[line:252] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.
/data1/6willrut/OFA/venv/lib/python3.6/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
